<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-05-23 å›› 11:25 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Numerical Analysis</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="gouziwu" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Numerical Analysis</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgcc5d6d0">Chap1 Mathematical Preliminaries</a>
<ul>
<li><a href="#orgd012745">1.2 Roundoff Errors and Computer Arithmetic</a></li>
<li><a href="#orgaa9ae66">1.3 ALgorithms and Convergence</a></li>
</ul>
</li>
<li><a href="#org4a36b6a">Chap2 Solutions of equations in one variable</a>
<ul>
<li><a href="#org794c3fd">2.1 Bisection method</a></li>
<li><a href="#orgeefd951">2.2 Fixed-Point Iteration</a></li>
<li><a href="#org1366344">2.3 Newton's method</a></li>
<li><a href="#orgb13f6be">2.4 Error analysis for iterative methods</a></li>
</ul>
</li>
<li><a href="#org5ef65cc">Chap3 Interpolation and polynomial approximation</a>
<ul>
<li><a href="#org9c85ec7">3.1 Interpolation and the Lagrange polynomial</a></li>
<li><a href="#org0efd1cb">3.2 Divied differences</a></li>
<li><a href="#orgbd2daa4">Additional Newton Interpolation</a>
<ul>
<li><a href="#org3d34d5a">Simple idea</a></li>
<li><a href="#org6bc052a">Basis transformation</a></li>
</ul>
</li>
<li><a href="#org3872197">3.3 Hermite interpolation</a></li>
<li><a href="#orgae644b5">3.4 Cubic spline interpolation</a></li>
</ul>
</li>
<li><a href="#org71c364e">chap4 numerical differentiation and integration</a>
<ul>
<li><a href="#org9bd904f">4.1 numerical differentiation</a></li>
<li><a href="#org98bf015">4.3 elements of numerical integration</a></li>
<li><a href="#orgb5d3049">4.4 composite numerical integration</a></li>
<li><a href="#orgb74e262">4.5 Romberg integration</a></li>
<li><a href="#orgfaa786a">4.2 Richardson's Extrapolation</a></li>
<li><a href="#org5a70382">4.6 Adaptive quadrature methods</a></li>
<li><a href="#org2abc88f">4.7 Gaussian Quadrature</a></li>
</ul>
</li>
<li><a href="#org827ebaf">Chap6 Direct Methods for Solving Linear Systems</a>
<ul>
<li><a href="#org7dc78a1">6.1 Linear Systems of Equations</a></li>
<li><a href="#org4b6ac96">6.2 Pivoting Strategies</a></li>
<li><a href="#org8af5e7b">6.5 Matrix Factorization</a></li>
<li><a href="#orgbdabf26">6.6 Special Types of Matrices</a></li>
</ul>
</li>
<li><a href="#org5db1c10">Chap7 Iterative techiniques in Matrix algebra</a>
<ul>
<li><a href="#org478f639">7.1 Norms of vectors and matrices</a></li>
<li><a href="#org880da26">7.2 Eigenvalues and Eigenvectors</a></li>
<li><a href="#org9aa5047">7.3 Iterative techniques for solving linear systems</a></li>
<li><a href="#orgcf7aa9e">7.4 Error bounds and iterative refinement</a></li>
</ul>
</li>
<li><a href="#org9f371e2">Chap8 Approximation theory</a>
<ul>
<li><a href="#org71d62ed">8.1 Discrete least squares approximation</a></li>
<li><a href="#org3e87b92">8.2 orthogonal polynomials and least squares approximation</a></li>
<li><a href="#orgf0e8fe4">8.3 Chebyshev polynomials and economization of power series</a></li>
</ul>
</li>
<li><a href="#orgf3cb22b">chap9 Approximating Eigenvalues</a>
<ul>
<li><a href="#org6280c01">9.3 the power method</a></li>
</ul>
</li>
<li><a href="#org31926e2"><span class="todo TODO">TODO</span> hw <code>[0/15]</code></a></li>
</ul>
</div>
</div>

<div id="outline-container-orgcc5d6d0" class="outline-2">
<h2 id="orgcc5d6d0">Chap1 Mathematical Preliminaries</h2>
<div class="outline-text-2" id="text-orgcc5d6d0">
</div>
<div id="outline-container-orgd012745" class="outline-3">
<h3 id="orgd012745">1.2 Roundoff Errors and Computer Arithmetic</h3>
<div class="outline-text-3" id="text-orgd012745">
<p>
<b>Truncation Error</b> : the error involved in using a truncated, or finite, summation to
approximate the sum of an infinite series 
</p>

<p>
<b>Roundoff Error</b>: the error produced when performing real number calculations.
It occurs because the arithmetic performed in a machine involves numbers
with only a finite number of digits. 
</p>


<p>
Suppose \(y=\textcolor{blue}{0.d_1d_2\dots
   d_k}d_{k+1}d_{k+2}\dots\textcolor{blue}{\times 10^n{}}\), then
</p>

<p>
\(fl(y)=\begin{cases} 0.d_1d_2\dots d_k\times 10^n&\quad\text{chopping}\\
   chop(y+5\times 10^{n-(k+1)})=0.\delta_1\delta_2\dots \delta_k\times
   10^n&\quad\text{Rounding}\\\end{cases}\)
</p>


\begin{definition}
If $p*$ is an approximation to $p$, the \textcolor{red}{absolute error} is $|p-p*|$,
and the \textcolor{red}{relative error} is $\frac{|p-p*|}{|p|}$, provided that $p\neq 0$
\end{definition}

\begin{definition}
The number $p*$ is said to approximate $p$ to $t$
\textcolor{red}{significant digits} if $t$ is the largest nonnegative
integer for which $\frac{|p-p*|}{|p|}<5\times 10^{-t}$
\end{definition}

<dl class="org-dl">
<dt>chopping</dt><dd>\(|\frac{y-fl(y)}{y}|=|\frac{0.d_1d_2\dots d_kd_{k+1}\dots
                 \times 10^n-0.d_1d_2\dots d_k\times 10^n}{0.d_1d_2\dots
                 d_kd_{k+1}\times
                 10^n}|=|\frac{0.d_{k+1}\dots}{0.d_1d_2\dots}|\times 10^{-k}\le
                 \frac{1}{0.1}\times 10^{-k}=10^{-k+1}\)</dd>
<dt>rounding</dt><dd>\(|\frac{y-fl(y)}{y}|\le \frac{0.5}{0.1}\times 10^{-k}=0.5\times
                 10^{-k+1}\)</dd>
</dl>

<p>
<b>Finite digit arithmetic</b>
</p>

<ul class="org-ul">
<li>\(x\oplus y=fl(fl(x)+fl(y))\)</li>
<li>\(x\otimes y=fl(fl(x)\times fl(y))\)</li>
<li>\(x\ominus y=fl(fl(x)-fl(y))\)</li>
<li>\(x\odiv y=fl(fl(x)\div fl(y))\)</li>
</ul>
</div>
</div>

<div id="outline-container-orgaa9ae66" class="outline-3">
<h3 id="orgaa9ae66">1.3 ALgorithms and Convergence</h3>
<div class="outline-text-3" id="text-orgaa9ae66">
<p>
An algorithm that satisfies that small changes in the initial data produce
correspondingly small changes in the final results is called <b>stable</b>;
otherwise it is <b>unstable</b>. An algorithm is called <b>conditionally stable</b> if it
is stable only for certain choices of initial data. 
</p>

<p>
Suppose that Eâ‚€ &gt; 0 denotes an initial error and En represents the magnitude
of an error after n subsequent operations. If \(E_n\approx CnE_0\), where C is a
constant independent of n, then the growth of error is said to be <b>linear</b>. If
\(E_n\approx C^nE_0\), for some C &gt; 1, then the growth of error is called <b>exponential</b> 
</p>

<p>
Suppose \(\{\beta_n\}_{n=1}^\infty, \lim\limits_{n \to \infty}\beta_n=0,
   \{\alpha_n\}_{n=1}^\infty, \lim\limits_{n\to\infty}\alpha_n=\alpha\).
If a positive constant K exists with \(|\alpha_n-\alpha|\le K|\beta_n|\) for
large n, then \(\{\alpha_n\}_{n=1}^\infty\) converges to Î± with <b>rate, or</b>
<b>order, of convergence</b> \(O(\beta_n)\)
</p>

<p>
Suppose \(\lim\limits_{h\to 0}G(h)=0, \lim\limits_{h\to 0}F(h)=L\) and
\(|F(h)-L|\le K|G(h)|\) for sufficiently small h, then we write
\(F(h)=L+O(G(h))\)
</p>
</div>
</div>
</div>
<div id="outline-container-org4a36b6a" class="outline-2">
<h2 id="org4a36b6a">Chap2 Solutions of equations in one variable</h2>
<div class="outline-text-2" id="text-org4a36b6a">
</div>
<div id="outline-container-org794c3fd" class="outline-3">
<h3 id="org794c3fd">2.1 Bisection method</h3>
<div class="outline-text-3" id="text-org794c3fd">
\begin{theorem}{Intermediate Value Theorem}
If $f\in C[a,b]$, $K\in(f(a), f(b))$, then there exists a number $p\in(a,b)$
for which $f(p)=K$
\end{theorem}

\begin{theorem}
Suppose that $f\in C[a,b]$ and $f(a)\cdot f(b)<0$. The bisection method
generates a sequence $\{p_n\},n=0,1,\dots$ approximating a zero $p$ of $f$ with
\begin{equation*}
|p_n-p|\le\frac{b-a}{2^n}, \quad\text{when } n\ge 1
\end{equation*}
\end{theorem}
</div>
</div>
<div id="outline-container-orgeefd951" class="outline-3">
<h3 id="orgeefd951">2.2 Fixed-Point Iteration</h3>
<div class="outline-text-3" id="text-orgeefd951">
<p>
\(f(x)=0\xleftrightarrow{\text{equivalent}} x=f(x)+x=g(x)\)
</p>

\begin{theorem}{Fixed-Point Theorem}
Let $g\in C[a,b]$ be s.t. $g(x)\in[a,b]$ for all $x\in[a,b]$. Suppose that
$g'$ exists on $(a,b)$ and that a constant $0<k<1$ exists with $|g'(x)|\le k$
for all $x\in(a,b)$ (hence $g'$ can't converge to 1). Then for any number
$p_0$ in $[a,b]$, the sequence defined by $p_n=g(p_{n-1}), n\ge 1$ converges
to the unique point $p$ in $[a,b]$
\end{theorem}

\begin{corollary}
$|p_n-p|\le\frac{1}{1-k}|p_{n+1}-p_n|$ and
$|p_n-p|\le\frac{k^n}{1-k}|p_1-p_0|$
\end{corollary}
</div>
</div>
<div id="outline-container-org1366344" class="outline-3">
<h3 id="org1366344">2.3 Newton's method</h3>
<div class="outline-text-3" id="text-org1366344">
<p>
Linearize a nonlinear function using <b>Taylor's expansion</b>
</p>

<p>
Let \(p_0\in [a,b]\) be an approximation to \(p\) s.t. \(f'(p_0)\neq 0\), hence 
\(f(x)=f(p_0)+f'(p_0)(x-p_0)+\frac{f''(\xi_x)}{2!}(x-p_0)^2\), then
\(0=f(p)\approx f(p_0)+f'(p_0)(p-p0)\rightarrow p\approx
   p_0-\frac{f(p_0)}{f'(p_0)}\)
\(p_n=p_{n-1}-\frac{f(p_{n-1})}{f'(p_{n-1})},\quad\text{for} n\ge 1\)
</p>

\begin{theorem}
Let $f\in C^2[a,b]$. If $p\in[a,b]$ is s.t. $f(p)=0,f'(p)\neq0$, then there
exists a $\delta>0$ s.t. Newton's method generates a sequence $\{p_n\},
n\in\mathbb{N}\setminus\{0\}$ converging to $p$ for any initial approximation
$p\in[p-\delta,p+\delta]$.
\end{theorem}
</div>
</div>
<div id="outline-container-orgb13f6be" class="outline-3">
<h3 id="orgb13f6be">2.4 Error analysis for iterative methods</h3>
<div class="outline-text-3" id="text-orgb13f6be">
\begin{definition}
Suppose $\{p_n\}(n=0,1,\dots)$ is a sequence that converges to $p$ with
$p_n\neq p$ for all $n$. If positive constants $\alpha$ and $\lambda$ exist
with
\begin{equation*}
\lim\limits_{n\to\infty}\frac{|p_{n+1}-p|}{|p_n-p|^\alpha}=\lambda
\end{equation*}
then $\{p_n\}(n=0,1,\dots)$ \textcolor{red}{converges to p of order
$\alpha$, with asymptotic error constant $\lambda$}
\end{definition}

\begin{theorem}
Let $p$ be a fixed point of $g(x)$. If there exists some constant $\alpha\ge
2$ s.t. $g\in C^\alpha[p-\delta,p+\delta]$,
\textcolor{red}{$g'(p)=\dots=g^{\alpha-1}(p)=0$} and \textcolor{red}{$g^\alpha(p)\neq 0$}.
Then the iterations with $p_n=g(p_{n-1})$, $n\ge1$ is of \textcolor{red}{order $\alpha$}
\end{theorem}

\begin{equation*}
p_{n+1}=g(p_n)=g(p)+g'(p)(p_n-p)+\dots+\frac{g^\alpha(\xi_n)}{\alpha!}(p_n-p)^\alpha
\end{equation*}

\begin{theorem}
Let $g\in C[a,b]$ be s.t. $g(x)\in[a,b]$ for all $x\in[a,b]$. Suppose in
addition that $g'$ is continuous on $(a,b)$ and a positive constant $k<1$
exists with
\begin{equation*}
|g'(x)|\le k, \quad \text{for all } x\in(a,b)
\end{equation*}
If $g'(p)\neq0$, then for any number $p_0\neq p$ in $[a,b]$, the sequence
\begin{equation*}
p_n=g(p_{n-1}),\quad\text{for }n\ge 1
\end{equation*}
converges only linearly to the unique fixed point in $[a,b]$
\end{theorem}

\begin{proof}
\begin{align*}
\lim\limits_{n\to\infty}\frac{|p_{n+1}-p|}{|p_n-p|}&=
\lim\limits_{n\to\infty}\frac{|g(p_n)-p|}{|p_n-p|}\\
&=\lim\limits_{n\to\infty}\frac{|g'(\xi)(p_n-p)|}{|p_n-p|}\\
&=|g'(p)|
\end{align*}
\end{proof}

\begin{theorem}
Let $p$ be a solution of the equation $x=g(x)$. Suppose that $g'(p)=0$ and
g'' is continuous with $|g''(x)|<M$ on an open interval $I$ containing $p$.
Then there exists a $\delta>0$ s.t. for $p_0\in[p-\delta,p+\delta]$, the
sequence defined by $p_n=g(p_{n-1})$, when $n\ge 1$ converges at least
quadratically to $p$. Moreover, for sufficiently large values of $n$,
\begin{equation*}
|p_{n+1}-p|<\frac{M}{2}|p_n-p|^2
\end{equation*}
\end{theorem}

\begin{proof}
Choose $k\in(0,1),\delta>0$ s.t. $[p-\delta,p+\delta]\subseteq I$ and
$|g'(x)|<k$ and $g''$ is continuous.
\begin{equation*}
g(x)=g(p)+g'(p)(x-p)+\frac{g''(\xi)}{2}(x-p)^2
\end{equation*}
Hence $g(x)=p+\frac{g''(\xi)}{2}(x-p)^2$.
$p_{n+1}=g(p_n)=p+\frac{g''(\xi_n)}{2}(p_n-p)^2$. Thus
$p_{n+1}-p=\frac{g''(\xi_n)}{2}(p_n-p)^2$. We get
\begin{equation*}
\lim\limits_{n\to\infty}\frac{|p_{n+1}-p|}{|p_n-p|^2}=\frac{g''(p)}{2}
\end{equation*}
\end{proof}

\begin{definition}
A solution $p$ of $f(x) = 0$ is a \textcolor{red}{zero of multiplicity} $m$
of $f$ if for $x\neq p$, $f(x)=(x-p)^mq(x)$ where $\lim\limits_{x\to
p}q(x)\neq 0$
\end{definition}

\begin{theorem}
The function $f\in C^m[a,b]$ has a zero of multiplicity $m$ at $p$ in $(a,b)$
if and only if
\begin{equation*}
0=f(p)=f'(p)=\dots=f^{(m-1)}(p),\quad\text{but } f^{(m)}(p)\neq 0
\end{equation*}
\end{theorem}

<p>
To handle the problem of multiple roots of a function \(f\) is to define
\(\mu(x)=\frac{f(x)}{f'(x)}\).
</p>

<p>
If p is a zero of f of multiplicity m with \(f(x)=(x-p)^mq(x
   )\), then
</p>
\begin{align*}
\mu(x)&=\frac{(x-p)^mq(x)}{m(x-p)^{m-1}q(x)+(x-p)^mq'(x)}\\
&=(x-p)\frac{q(x)}{mq(x)+(x-p)q'(x)}
\end{align*}
<p>
And \(q(x)\neq 0\).
</p>

<p>
Now Newton's method:
</p>
\begin{align*}
g(x)&=x-\frac{\mu(x)}{\mu'(x)}\\
&=x-\frac{f(x)/f'(x)}{(f'(x)^2-f(x)f''(x))/f'(x)^2}\\
&=x-\frac{f(x)f'(x)}{f'(x)^2-f(x)f''(x)}
\end{align*}
</div>
</div>
</div>
<div id="outline-container-org5ef65cc" class="outline-2">
<h2 id="org5ef65cc">Chap3 Interpolation and polynomial approximation</h2>
<div class="outline-text-2" id="text-org5ef65cc">
</div>
<div id="outline-container-org9c85ec7" class="outline-3">
<h3 id="org9c85ec7">3.1 Interpolation and the Lagrange polynomial</h3>
<div class="outline-text-3" id="text-org9c85ec7">
<p>
\(P_n(x)=\displaystyle\sum_{i=0}^nL_{n,i}(x)y_i\). Find \(L_{n,i}(x)\) for
\(i=0,\dots,n\) s.t. \(L_{n,j}(x_j)=\delta_{ij}\). \(\delta_{ij}\) Kronecker delta.
Each \(L_{n,i}\) has n roots \(x_0,\dots,\hat{x_i},\dots,x_n\).
\(L_{n,j}(x)=C_i(x-x_0)\dots\hat{(x-x_i)}\dots(x-x_n)=C_i \displaystyle
   \prod_{\substack{j\neq i\\j=0}}^n(x-x_j)\).
\(L_{n,j}(x_i)=1\to C_i=\displaystyle\prod_{j\neq i}\frac{1}{x_i-x_j}\).
Hence \(L_{n,i}(x)=\displaystyle\prod_{\substack{j\neq i\\j=0}}^n
   \frac{x-x_j}{x_i-x_j}\)
</p>

\begin{theorem}
If $x_0,x_1,\dots,x_n$ are n+1 distinct numbers and $f$ is a function whose values
are given at these numbers, then the n-th Lagrange interpolating polynomial 
is unique
\end{theorem}


<p>
<b>Analyze the remainder</b>. Suppose \(a\le x_0<x_1<\dots<x_n\le b\) and \(f\in
   C^{n+1}[a,b]\). Consider \(R_n(x)=f(x)-P_n(x)\).
\(R_n(x)\) has at least n+1 roots =&gt;
\(R_n(x)=K(x)\displaystyle\prod_{i=0}^n(x-x_i)\).
For any \(x\neq x_i\). Define
\(g(t)=R_n(t)-K(x)\displaystyle\prod_{i=0}^n(t-x_i)\). \(g(x)\) has n+2 distinct
roots \(x_0\dots x_n x\). Hence \(g^{(n+1)}(\xi_x)=0,\xi_x\in(a,b)\).
\(f^{(n+1)}(\xi_x)-Pn^{(n+1)}(\xi_x)-K(x)(n+1)!=R_n^{(n+1)}(\xi_x)-K(x)(n+1)!\).
Thus
\(R_n(x)=\frac{f^{(n+1)}(\xi_x)}{(n+1)!}\displaystyle\prod_{i=0}^n(x-x_i)\).
</p>

\begin{definition}
Let $f$ be a function defined at $x_0,\dots,x_n$ and suppose $m_1,\dots,m_k$ are
k distinct integers with $0\le m_i\le n$ for each i. The Lagrange polynomial that
agrees with $f(x)$ at the k points $x_{m_1},\dots,x_{m_k}$ denoted by 
$P_{m_1,\dot,m_k}(x)$
\end{definition}

\begin{theorem}
Let $f$ be defined at $x_0,\dots,x_k$ and let $x_i$ and $x_j$ be two distinct numbers in
this set. Then
\begin{equation*}
P(x)=\frac{(x-x_j)P_{0,1,\dots,j-1,j+1,\dots,k(x)}-(x-x_i)P_{0,\dots,i-1,i+1,\dots,k(x)}}
{x_i-x_j}
\end{equation*}
describes the k-th Lagrange polynomial that interpolates $f$ at the k+1 points
$x_0,\dots,x_k$
\end{theorem}

<p>
<b>Neville's Method</b>
</p>
\begin{tabular}{c c c c c c}
$x_0$ & $P_0$ &           &             &            \\
$x_1$ & $P_1$ & $P_{0,1}$ &             &            \\
$x_2$ & $P_2$ & $P_{1,2}$ & $P_{0,1,2}$ &            \\
$x_3$ & $P_3$ & $P_{2,3}$ & $P_{1,2,3}$ & $P_{0,1,2,3}$\\
\end{tabular}
</div>
</div>
<div id="outline-container-org0efd1cb" class="outline-3">
<h3 id="org0efd1cb">3.2 Divied differences</h3>
<div class="outline-text-3" id="text-org0efd1cb">
<p>
\(f[x_i,x_j]=\frac{f(x_i)-f(x_j)}{x_i-x_j}(i\neq j, x_i\neq x_j)\).
\(f[x_i,x_j,x_k]=\frac{f[x_i,x_j]-f[x_j,x_k]}{x_i-x_k}\).
</p>
</div>
</div>
<div id="outline-container-orgbd2daa4" class="outline-3">
<h3 id="orgbd2daa4">Additional Newton Interpolation</h3>
<div class="outline-text-3" id="text-orgbd2daa4">
</div>
<div id="outline-container-org3d34d5a" class="outline-4">
<h4 id="org3d34d5a">Simple idea</h4>
<div class="outline-text-4" id="text-org3d34d5a">
<p>
Given \(x_0,\dots,x_n\)
</p>
\begin{enumerate}
\item Fitting $x_0$ first: $f(x)\approx f_0, f_0=f(x_0)$
\item Add one more point $x_1$, $f_1=f(x_1)$
\begin{equation*}
f(x) \approx f_0+\alpha_1(x-x_0),\alpha_1=\frac{f_1-f_0}{x_1-x_0}
\end{equation*}
\item More points $f(x)\approx f_0+\alpha_1(x-x_0)+\alpha_2(x-x_0)(x-x_1)$
\end{enumerate}

<p>
<b>The pattern and coefficients</b>.
\(f(x)=\displaystyle\sum_{i=0}^n\alpha_i
    \displaystyle\prod_{j=0}^{j<i}(x-x_j)
    =\displaystyle\sum_{i=0}^n\alpha_iN^{(i)}(x)\)
</p>

\begin{equation*}
\begin{pmatrix}
f_0\\
f_1\\
\vdots\\
f_n
\end{pmatrix}=
\begin{pmatrix}
N^{(0)}(x_0) & N^{(1)}(x_0) & \dots & N^{(n)}(x_0)\\
N^{(0)}(x_1) & N^{(1)}(x_1) & \dots & N^{(n)}(x_1)\\
\vdots & \vdots & \ddots&\vdots\\
N^{(0)}(x_n) & N^{(1)}(x_n) & \dots & N^{(n)}(x_n)\\
\end{pmatrix}
\begin{pmatrix}
\alpha_0\\
\alpha_1\\
\vdots\\
\alpha_n
\end{pmatrix}
\end{equation*}

<p>
\(N^{(i)}(x_k)=\begin{cases}
    0&k<i\\
    \prod_{j=0}^{j<i}(x_k-x_j)&k\ge i\\
    \end{cases}\) with \(N^{(0)}(x) = 1\).
Newton interpolation matrix is lower triangular.
Lagrange matrix is identity.
</p>
</div>
</div>
<div id="outline-container-org6bc052a" class="outline-4">
<h4 id="org6bc052a">Basis transformation</h4>
<div class="outline-text-4" id="text-org6bc052a">
\begin{equation*}
\begin{pmatrix}
1\\
(x-x_0)\\
(x-x_0)(x-x_1)\\
\vdots
\end{pmatrix}=(?)
\begin{pmatrix}
1\\
x\\
x^2\\
\vdots
\end{pmatrix}
\end{equation*}
<p>
Hence \((\Phi_B)^T=(T_A^B)^T(\Phi_A)^T\).
\(\Phi_B=\Phi_AT_A^B\)
</p>

\begin{align*}
(\Phi_A)(\alpha_A)=(f)&=(\Phi_B)(\alpha_B)\\
&=(\Phi_A)(T_A^B)(\alpha_B)\\
&\Rightarrow\\
(\alpha_A)&=(T_A^B)(\alpha_B)\\
(\alpha_B)&=(T_A^B)^{-1}(\alpha_A)\\
&=(T_B^A)(\alpha_A)
\end{align*}
</div>
</div>
</div>
<div id="outline-container-org3872197" class="outline-3">
<h3 id="org3872197">3.3 Hermite interpolation</h3>
<div class="outline-text-3" id="text-org3872197">
<p>
Find the <b>osculating polynomial</b> \(P(x)\) s.t. \(P(x_i)=f(x_i),
   P'(x_i)=f'(x_i),\dots,P^{(m_i)}(x_i)=f^{(m_i)}(x_i)\) for all \(i=0,1,\dots,n\).
</p>

<p>
Just the Taylor polynomial \(P(x)=f(x_0)+f'(x_0)(x-x_0)+\dots+
   \frac{f^{(m_0)}(x_0)}{m_0!}(x-x_0)^{m_0}\) with remainder 
\(R(x)=f(x)-\varphi(x)=\frac{f^{(m_0+1)}(\xi)}{(m_0+1)!}(x-x_0)^{(m_0+1)}\)
</p>

<p>
\(m_i = 1\) gives <b>Hermite polynomial</b>
</p>

\begin{example}
Suppose $x_0\neq x_1\neq x_2$. Given $f(x_0),f(x_1), f(x_2),
f'(x_1)$ find the polynomial $P(x)$ s.t. $P(x_i)=f(x_i),P'(x_1)=f'(x_1)$ and
analyze the errors.
\end{example}

\begin{proof}
$P_3(x)=\displaystyle\sum_{i=0}^2f(x_i)h_i(x)+f'(x_1)\hat{h}_1(x)$ where
$h_i(x_j)=\delta_{ij},h_i'(x_i)=0,\hat{h}_i(x_i)=0,\hat{h}_i'(x_1)=1$.
\begin{itemize}
\item $h_0(x)$. Has roots $x_1,x_2$ and $x_1$ is a multiple root.
      $h_0(x)=C_0(x-x_1)^2(x-x_2)$ and $h_0(x_0)=1\Longrightarrow C_0$
\item $\hat{h}_1(x)$ has root $x_0,x_1,x_2\Longrightarrow 
      \hat{h}_1(x)=C_1(x-x_0)(x-x_1)(x-x_2)$
\end{itemize}
\end{proof}

<p>
In general, given \(x_0,\dots,x_n;y_0,\dots,y_n\) and \(y_0',\dots,y_n'\). The
Hermite polynomial \(H_{2n+1}(x)\) satisfies \(H_{2n+1}(x_i)=y_i\) and
\(H'_{2n+1}(x_i)=y_i'\) 
</p>

<p>
<i>Solution</i>.
\(H_{2n+1}(x)=\displaystyle\sum_{i=0}^ny_ih_i(x)+\displaystyle\sum_{i=0}^ny_i'
   \hat{h}_i(x)\)
</p>
</div>
</div>
<div id="outline-container-orgae644b5" class="outline-3">
<h3 id="orgae644b5">3.4 Cubic spline interpolation</h3>
<div class="outline-text-3" id="text-orgae644b5">
<p>
<b>Piecewise linear interpolation</b>. Approximate \(f(x)\) by linear polynomials on
each subinterval \([x_i,x_{i+1}]\).
</p>

<p>
\(f\approx P_1(x)=\frac{x-x_{i+1}}{x_i-x_{i+1}}y_i+\frac{x-x_i}
   {x_{i+1}-x_i}y_{i+1} \quad\text{for} \;x\in[x_i,x_{i+1}]\) 
</p>

<p>
Let \(h=\max\abs{x_{i+1}-x_i}\). Then \(P_1^h(x)\xrightarrow{uniform} f(x)\) as
\(h\to 0\) 
However, this is no longer smooth.
</p>

<p>
<b>Hermite piecewise polynomials</b>. Given
\(x_0,\dots,x_n;y_0,\dots,y_n,y_0',\dots,y_n'\), construct the Hermite
polynomial of degree 3 with \(y\) and \(y'\) on the two endpoints of
\([x_i,x_{i+1}]\)
</p>

<p>
<b>Cubic Spline</b>.
</p>
\begin{definition}
Given a function $f$ define on $[a,b]$ and a set of nodes $a=x_0<x_1<\dots<x_n=b$,
\textcolor{red}{cubic spline interpolant} $S$ for $f$ is a function that satisfies
the following conditions
\begin{itemize}
\item $S(x)$ is a cubic polynomial, denoted by $S_i(x)$ on the subinterval
$[x_i,x_{i+1}]$ for each $i=0,\dots,n-1$
\item $S(x_i)=f(x_i)$ for each $i=0,\dots, n$
\item $S_{i+1}(x_{i+1})=S_i(x_{i+1})$
\item $S'_{i+1}(x_{i+1})=S'_i(x_{i+1})$
\item $S''_{i+1}(x_{i+1})=S''_i(x_{i+1})$
\end{itemize}
\end{definition}

<p>
\includegraphics[width=100mm]{CubicSpline}
</p>

<p>
<b>Method of Bending moment</b>. Let \(h_j=x_j-x_{j-1}\) and \(S(x)=S_j(x)\) for
\(x\in[x_{j-1}, x_j]\). Then \(S_j''\) is a polynomial of degree
\textcolor{red}{1}, which can be determined by the values of f on
\textcolor{red}{2} nodes .
</p>

<p>
Assume \(S_j''(x_{j-1})=M_{j-1},S_j''(x_j)=M_j\). Then for all
\(x\in[x_{j-1},x_j]\),
\(S_j''(x)=M_{j-1}\frac{x_j-x}{h_j}+M_j\frac{x-x_{j-1}}{h_j}\). Hence we get
</p>
\begin{align*}
&S_j'(x)=-M_{j-1}\frac{(x_j-x)^2}{2h_j}+M_j\frac{(x-x_{j-1})^2}{2h_j}+A_j\\
&S_j(x)=M_{j-1}\frac{(x_j-x)^3}{6h_j}+M_j\frac{(x-x_{j-1})^3}{6h_j}+A_jx+B_j
\end{align*}

<p>
Solve this by \(S_j(x_{j-1})=y_{j-1},S_j(x_j)=y_j\), we get
</p>
\begin{align*}
A_j&=\frac{y_j-y_{j-1}}{h_j}-\frac{M_j-M_{j-1}}{6}h_j\\
A_jx+B_j&=(y_{i-1}-\frac{M_{j-1}}{6}h_j^2)\frac{x_j-x}{h_j}+ 
(y_j-\frac{M_j}{6}h_j^2)\frac{x-x_{j-1}}{h_j}
\end{align*}

<p>
Now solve for \(M_j\): Since \(S'\) is continuous at \(x_j\)
</p>
 \begin{align*}
[x_{j-1},x_j]:S'_j(x)&=-M_{j-1}\frac{(x_j-x)^2}{2h_j}+M_j\frac{(x-x_{j-1})^2}{2h_j}
                       +f[x_{j-1},x_j]-\frac{M_j-M_{j-1}}{6}h_j\\
[x_j,x_{j+1}]:S'_{j+1}(x)&=-M_j\frac{(x_{j+1}-x)^2}{2h_{j+1}}+M_{j+1}
\frac{(x-x_j)^2}{2h_{j+1}}+f[x_j,x_{j+1}]-\frac{M_{j+1}-M_j}{6}h_{j+1}\\
\end{align*}
<p>
From \(S'_j(x_j)=S'_{j+1}(x_j)\), let \(\lambda_j=\frac{h_{j+1}}{h_j+h_{j+1}},
   \mu_j=1-\lambda_j,g_j=\frac{6}{h_j+h_{j+1}}(f[x_j,x_{j+1}]-f[x_{j-1},x_j])\)
we get
</p>
\begin{equation*}
\mu_jM_{j-1}+2M_j+\lambda_jM_{j+1}=g_j\quad\text{for } \;1\le j\le n-1
\end{equation*}
\begin{equation*}
\begin{pmatrix}
\mu_1 & 2 & \lambda_1 &&\\
& \ddots &\ddots &\ddots &\\
&&\mu_{n-1}&2&\lambda_{n-1}
\end{pmatrix}
\begin{pmatrix}
M_0\\
\vdots\\
\vdots\\
M_n\\
\end{pmatrix}=
\begin{pmatrix}
g_1\\
\vdots\\
g_{n-1}
\end{pmatrix}
\end{equation*}

<p>
And  \(S'(a)=y_0',S'(b)=y_n'\)
</p>

<p>
If \(S''(a)=y_0''=M_0,S''(b)=y_n''=M_n\), then \(\lambda_0=0,g_0=2y_0'',\mu_n=0
   g_n=2y_n''\).
</p>

<p>
The case when \(M_0=M_n=0\) is called a <b>free boundary</b>, the spline is called
<b>natural spline</b>
</p>
</div>
</div>
</div>
<div id="outline-container-org71c364e" class="outline-2">
<h2 id="org71c364e">chap4 numerical differentiation and integration</h2>
<div class="outline-text-2" id="text-org71c364e">
</div>
<div id="outline-container-org9bd904f" class="outline-3">
<h3 id="org9bd904f">4.1 numerical differentiation</h3>
<div class="outline-text-3" id="text-org9bd904f">
<p>
<b>Target</b>: Given \(x_0\), approximate \(f'(x_0)\)
</p>

\begin{equation*}
f'(x_0)=\lim\limits_{h\to0}\frac{f(x_0+h)-f(x_0)}{h}
\end{equation*}

<p>
Approximate \(f(x)\) by its lagrange polynomial with interpolating points \(x_0\)
and \(x_0+h\)
</p>

\begin{align*}
f(x)&=\frac{f(x_0)(x-x_0-h)}{x_0-x_0-h}+\frac{f(x_0+h)(x-x_0)}{x_0+h-x_0}\\
&+\frac{(x-x_0)(x-x_0-h)}{2}f''(\xi_x)\\
f'(x)&=\frac{f(x_0+h)-f(x_0)}{h}+\frac{2(x-x_0)-h}{2}f''(\xi_x)\\
&+\frac{(x-x_0)(x-x_0-h)}{2}\frac{d}{dx}[f''(\xi_x)]\\
f'(x_0)&=\frac{f(x_0+h)-f(x_0)}{h}-\frac{h}{2}f''(\xi)
\end{align*}

<p>
Approximate \(f(x)\) by its Lagrange polynomial with interpolating points
\(\{x_0,x_1,\dots,x_n\}\)
</p>

\begin{align*}
f(x)&=\displaystyle\sum_{k=0}^nf(x_k)L_k(x)+\frac{(x-x_0)\dots(x-x_n)}{(n+1)!}
f^{(n+1)}(\xi_x)\\
f'(x_j)&=\displaystyle\sum_{k=0}^nf(x_k)L_k'(x_j)+\frac{f^{(n+1)}(\xi_j)}{(n+1)!}
\displaystyle\prod_{\substack{k=0\\k\neq j}}^n(x_j-x_k)
\end{align*}
</div>
</div>
<div id="outline-container-org98bf015" class="outline-3">
<h3 id="org98bf015">4.3 elements of numerical integration</h3>
<div class="outline-text-3" id="text-org98bf015">
<p>
<b>Target</b>: approximate \(I=\int_a^bf(x)dx\)
</p>

<p>
Integrate the <b>Lagrange interpolating polynomial</b> of \(f(x)\) instead
</p>

<p>
Select a set of distinct nodes \(a\le x_0<x_1<\dots<x_n\le b\) from \([a,b]\).
The Lagrange polynomial is \(P_n(x)=\displaystyle\sum_{k=0}^nf(x_k)L_k(x)\)
</p>

\begin{equation*}
\int_a^bf(x)dx\approx \displaystyle\sum_{k=0}^nf(x_k)
\overbrace{\int_a^b L_k(x)dx}^{A_k}
\end{equation*}

<p>
Error
</p>
\begin{align*}
R[f]&=\int_a^bf(x)dx-\displaystyle\sum_{k=0}^nA_kf(x_k)\\
&=\int_a^b[f(x)-P_n(x)]dx=\int_a^bR_n(x)dx\\
&=\int_a^b\frac{f^{(n+1)}(\xi_x)}{(n+1)!}\displaystyle\prod_{i=0}^n(x-x_i)dx
\end{align*}

\begin{definition}
The \textcolor{red}{degree of accuracy}, or \textcolor{red}{precision} of a quadrature
formula is the largest positive integer \textcolor{red}{$n$}   s.t. 
the formula is \textcolor{red}{exact}
for $x^k$ for each $k=0,1,\dots,n$
\end{definition}

<p>
Example. Consider the linear interpolation on \([a,b]\), we have 
</p>
\begin{equation*}
P_1(x)=\frac{x-b}{a-b}f(a)+\frac{x-a}{b-a}f(b)
\end{equation*}
<p>
\(A_1=A_2=\frac{b-a}{2}, \int_a^bf(x)dx\approx\frac{b-a}{2}[f(a)+f(b)]\). This
is \textcolor{red}{trapezoidal rule}.
</p>

<p>
Consider \(x^k\)
</p>
\begin{align*}
1:\quad &\int_a^b1dx=b-a \textcolor{red}{=} \frac{b-a}{2}[1+1]\\
x:\quad &\int_a^bxdx=b-a \textcolor{red}{=}\frac{b-a}{2}[a+b]\\
x^2:\quad &\int_a^bx^2dx=b-a \textcolor{red}{\neq}  \frac{b-a}{2}[a^2+b^2]\\
\end{align*}

<p>
For equally spaced nodes: \(x_i=a+ih,h=\frac{b-a}{n}, i=0,1,\dots,n\)
</p>

\begin{align*}
A_i&=\int_{x_0}^{x_n}\displaystyle\prod_{j\neq i}\frac{x-x_j}{x_i-x_j}dx\\
&=\int_0^n\displaystyle\prod_{i\neq j}\frac{(t-j)h}{(i-j)h}\times hdt\quad x=a+th\\
&=\frac{(b-a)(-1)^{n-i}}{n\;i!(n-i)!}\int_0^n\displaystyle\prod_{i\neq j}(t-j)dt
\end{align*}

<p>
\(\frac{(-1)^{n-i}}{n\;i!(n-i)!}\int_0^n\displaystyle\prod_{i\neq j}(t-j)dt\)
is the <b>Cotes coefficients</b>
</p>
</div>
</div>

<div id="outline-container-orgb5d3049" class="outline-3">
<h3 id="orgb5d3049">4.4 composite numerical integration</h3>
<div class="outline-text-3" id="text-orgb5d3049">
<p>
Due to the oscillatory nature of high-degree polynomials, <b>piecewise</b>
interpolation is applied to approximate \(f(x)\). A piecewise approach that
uses the low-order Newton-Cotes formulae
</p>


<p>
Composite Trapezoidal rule: \(h=\frac{b-a}{n}, x_k=a+kh\).
</p>

<p>
Apply Trapezoidal Rule on each \([x_{k-1}, x_k]\)
</p>
\begin{tikzpicture}
\filldraw [gray] (0,0) circle [radius=2pt]
(-1.5,0) circle [radius=2pt]
(0,0.5) circle [radius=2pt]
(-0.75,0) circle [radius=2pt]
(-0.75,0.5) circle [radius=2pt]
(0.75,0.5) circle [radius=2pt]
(1.5,0) circle [radius=2pt]
(0.75,0) circle [radius=2pt];
\draw (-1.5,0) -- (1.5,0);
\end{tikzpicture}

\begin{equation*}
\int_{x_{k-1}}^{x_k}f(x)d(x)\approx \frac{x_k-x_{k-1}}{2}[f(x_{k-1})-f(x_k)]
\end{equation*}
\begin{equation*}
\int_a^bf(x)dx\approx \displaystyle\sum_{k=1}^n\frac{h}{2}[f(x_{k-1})+f(x_k)]=
\frac{h}{2}\left[f(a)+2 \displaystyle\sum_{k=1}^{n-1}f(x_k)+f(b)\right]=
\textcolor{red}{T_n} 
\end{equation*}
\begin{equation*}
R[f]=\displaystyle\sum_{k=1}^n\left[-\frac{h^3}{12}f''(\xi_k)\right]=-\frac{h^2}
{12}(b-a)\frac{\displaystyle\sum_{k=1}^nf''(\xi_k)}{n}=
-\frac{h^2}{12}(b-a)f''(\xi),\xi\in(a,b)
\end{equation*}


<p>
<b>Composite simpson's rule</b>
</p>
\begin{equation*}
\int_{x_k}^{x_{k+1}}f(x)dx\approx\frac{h}{6}\left[f(x_k)+4f(x_{k+1/2})+f(x_{k+1})\right]
\end{equation*}
<p>
In fact, it's just a mean value \((f(x_k)+4f(x_{k+1/2})+f(x_{k+1})) / 6\)
</p>
</div>
</div>

<div id="outline-container-orgb74e262" class="outline-3">
<h3 id="orgb74e262">4.5 Romberg integration</h3>
<div class="outline-text-3" id="text-orgb74e262">
\begin{align*}
R_n[f]&=-\frac{h^2}{12}(b-a)f''(\xi)\\
R_{2n}[f]&=-\frac{h^2/4}{12}(b-a)f''(\xi')\approx \frac{1}{4}R_n[f]\\
\end{align*}
<p>
Hence we have
</p>
\begin{equation*}
\frac{I-T_{2n}}{I-T_n}\approx\frac{1}{4}
\end{equation*}
<p>
and \(I\approx \frac{4}{3}T_{2n}-\frac{1}{3}T_n=\textcolor{red}{S_n}\)
.\(\frac{4^2S_{2n}-S_n}{4^2-1}=C_n\), \(\frac{4^3C_{2n}-S_n}{4^3-1}=R_n\), the
<b>Romberg sequence</b>
</p>
</div>
</div>

<div id="outline-container-orgfaa786a" class="outline-3">
<h3 id="orgfaa786a">4.2 Richardson's Extrapolation</h3>
<div class="outline-text-3" id="text-orgfaa786a">
<p>
generate high-accuracy results while using low-order formulae
</p>

<p>
For some \(h\neq 0\), suppose we have \(T_0(h)\) that approximates an unknown
\(I\), and
</p>
\begin{align*}
T_0(h)-I&=\alpha_1 h+\alpha_2h+\dots\\
T_0(h/2)-I&=\alpha_1(h/2)+\alpha_2(h/2)^2+\dots\\
\end{align*}
<p>
Hence can improve accuracy by substituting
</p>
</div>
</div>

<div id="outline-container-org5a70382" class="outline-3">
<h3 id="org5a70382">4.6 Adaptive quadrature methods</h3>
<div class="outline-text-3" id="text-org5a70382">
<p>
Predict the amount of functional variation and adapt the step size to the
varing requirement
</p>

<p>
using the composite integration
</p>
<ul class="org-ul">
<li>recursively halve the step size</li>
<li>waste large number of computations</li>
<li>only need to halve the interval with large error</li>
<li>THIS is <b>adaptive</b></li>
</ul>


<p>
A simple strategy to bound the total error by \(\epsilon\) of
</p>
\begin{equation*}
\int_a^bf(x)dx
\end{equation*}
<p>
In an interval with length \(h\), the error is smaller than
\(h\frac{\epsilon}{b-a}\)
</p>


\begin{equation*}
\epsilon(f,a,b)=\int_a^bf(x)dx-S(a,b)=\frac{h^5}{90}f^{(4)}(\xi)
\end{equation*}
</div>
</div>

<div id="outline-container-org2abc88f" class="outline-3">
<h3 id="org2abc88f">4.7 Gaussian Quadrature</h3>
</div>
</div>
<div id="outline-container-org827ebaf" class="outline-2">
<h2 id="org827ebaf">Chap6 Direct Methods for Solving Linear Systems</h2>
<div class="outline-text-2" id="text-org827ebaf">
</div>
<div id="outline-container-org7dc78a1" class="outline-3">
<h3 id="org7dc78a1">6.1 Linear Systems of Equations</h3>
<div class="outline-text-3" id="text-org7dc78a1">
<p>
<b>Gaussian elimination with backward substitution</b>
</p>
</div>
</div>
<div id="outline-container-org4b6ac96" class="outline-3">
<h3 id="org4b6ac96">6.2 Pivoting Strategies</h3>
<div class="outline-text-3" id="text-org4b6ac96">
<p>
<b>Problem</b>: small pivot element may cause trouble
</p>

<p>
<b>Paritial Pivoting</b>: Determine the smallest pâ‰¥k s.t.
\(|a_{pk}^{(k)}|=\displaystyle\max_{k\le j\le n}|a_{ik}^{(k)}|\) and
interchange the pth and the kth rows
</p>

<p>
<b>Scaled Partial Pivoting</b>:
</p>
<ol class="org-ol">
<li>Define a scale factor \(s_i\) for each row as \(s_i=\displaystyle\max_{1\le
      j\le n}|a_{ij}|\)</li>
<li>Determine the smallest \(p\ge k\) s.t.
\(\frac{|a_{pk}^{(k)}}{s_p}=\displaystyle\max_{k\le i\le
      n}\frac{|a_{ik}^{(k)}|}{s_i}\)
and interchange the pth and the kth rows</li>
</ol>


<p>
<b>Complete Pivoting</b>: Search all the entries \(a_{ij}\) to find the entry with
the largest magnitude
</p>
</div>
</div>
<div id="outline-container-org8af5e7b" class="outline-3">
<h3 id="org8af5e7b">6.5 Matrix Factorization</h3>
<div class="outline-text-3" id="text-org8af5e7b">
<p>
\(m_{ik}=a_{ik}/a_{kk}\)
</p>
\begin{equation*}
L_k=
\begin{pmatrix}
1 &            &            &               &  \\
  & \ddots     &            &\mbox{\Huge 0} &  \\
  &            & 1          &               &  \\
  &            & -m_{k+1,k} &               &  \\
  &            & \vdots     & \ddots        &  \\
  &            & -m_{n,k}   &               & 1\\
\end{pmatrix}
\end{equation*}  


<p>
Hence 
</p>

\begin{equation*}
L_1^{-1}L_2^{-1}\dots L_{n-1}^{-1}=
\begin{pmatrix}
1&&&\mbox{\Huge 0}\\
&1&&\\
&&\ddots&\\
\text{\Huge $m_{i,j}$}&&&1\\
\end{pmatrix}
\end{equation*}

\begin{equation*}
U=
\begin{pmatrix}
a_{11}&a_{12}&\dots&a_{1n}\\
&a_{22}&\dots&a_{2n}\\
&&\dots&\vdots\\
&&&a_{nn}\\
\end{pmatrix}
\end{equation*}

<p>
\(A=LU\)
</p>
</div>
</div>
<div id="outline-container-orgbdabf26" class="outline-3">
<h3 id="orgbdabf26">6.6 Special Types of Matrices</h3>
<div class="outline-text-3" id="text-orgbdabf26">
<p>
<b>Strictly Diagonally Dominant Matrix</b>.
\(|a_{ii}|>\displaystyle\sum_{\substack{j=1,\\j\neq i}}^n|a_{ij}| \quad
   \text{for each } i=1,\dots,n\)
</p>

\begin{theorem}
A strictly diagonally dominant matrix A is \textcolor{red}{nonsingular}. Moreover,
Gaussian elimination can be performed \textcolor{red}{without} row or column
\textcolor{red}{interchanges}, and the computations will be \textcolor{red}{stable}
w.r.t. the growth of roundoff errors
\end{theorem}

<p>
<b>Choleski's Method for Positive Definite Matrix</b>:
</p>
\begin{definition}
A matrix A is \textcolor{red}{positive definite} if ti's symmetric and if    
$ \mathbf{x}^T \mathbf{A} \mathbf{x}>0$ for every n-dimensional vector $ \mathbf{x}\neq 0$
\end{definition}

\begin{lemma}
A is positive definite
\begin{enumerate}
\item $A^{-1}$ is positive definite as well, and $a_{ii}>0$
\item $\sum|a_{ij}|\le\max|a_{kk}|$; $(a_{ij})^2<a_{ii}a_{jj}$ for each i â‰  j
\item Each of /A's leading principal submatrices $A_k$/ has a positive determinant
\end{enumerate}
\end{lemma}

\begin{equation*}
U =
\begin{pmatrix}
&u_{ij}\\
&&\\
\end{pmatrix}=
\begin{pmatrix}
u_{11}&&\\
&\ddots&\\
&&u_{nn}\\
\end{pmatrix}
\begin{pmatrix}
1&&u_{ij}/u_{ii}\\
&1&\\
&&1\\
\end{pmatrix}=D\tilde{U}
\end{equation*}
<p>
A is symmetric, hence 
</p>
\begin{equation*}
L=\tilde{U}^t, A=LDL^t
\end{equation*}
<p>
Let 
</p>
\begin{equation*}
D^{1/2}=
\begin{pmatrix}
\sqrt{u_{11}}&&\\
&\ddots&\\
&&\sqrt{u_{nn}}\\
\end{pmatrix}, \tilde{L}=LD^{1/2/}, A=\tilde{L}\tilde{L}^t
\end{equation*}

<p>
<b>Crout Reduction for tridiagonal Linear System</b>
</p>

\begin{equation*}
\begin{pmatrix}
b_1 & c_1    &        &        &\\
a_2 & b_2    & c_2    &        &\\
    & \ddots & \ddots & \ddots &\\
    &        & a_{n-1}& b_{n-1}& c_{n-1} \\
    &        &        & a_n    & b_n\\
\end{pmatrix}
\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_{n-1}\\
x_n
\end{pmatrix}=
\begin{pmatrix}
f_1\\
f_2\\
\vdots\\
f_{n-1}\\
f_n
\end{pmatrix}
\end{equation*}

\begin{equation*}
A=
\begin{pmatrix}
\alpha_1 &&&\\
\gamma_2 & \ddots &&\\
         & \ddots & \ddots   &\\
         &        & \gamma_n & \alpha_n\\
\end{pmatrix}
\begin{pmatrix}
1 & \beta_1 &&\\
  & \ddots & \ddots &\\
  &        & \ddots & \beta_{n-1}\\
  &        &        & 1\\
\end{pmatrix}
\end{equation*}  
</div>
</div>
</div>
<div id="outline-container-org5db1c10" class="outline-2">
<h2 id="org5db1c10">Chap7 Iterative techiniques in Matrix algebra</h2>
<div class="outline-text-2" id="text-org5db1c10">
</div>
<div id="outline-container-org478f639" class="outline-3">
<h3 id="org478f639">7.1 Norms of vectors and matrices</h3>
<div class="outline-text-3" id="text-org478f639">
\begin{definition}
A \textcolor{red}{vector norm} on $R^n$ is a function $||\cdot||: \mathbb{R}^n\to \mathbb{R}$
with following properties for all $ \mathbf{x,y}\in \mathbb{R}^n, \alpha\in C$
\begin{enumerate}
\item $|| \mathbf{x}||\le 0$; $|| \mathbf{x}||=0\Longleftrightarrow \mathbf{x}= \mathbf{0}$
\item $||\alpha \mathbf{x}||=|\alpha|\cdot|| \mathbf{x}||$
\item $|| \mathbf{x}+ \mathbf{y}||\le|| \mathbf{x}||+|| \mathbf{y}||$
\end{enumerate}
\end{definition}

<p>
\(|| \mathbf{x}||_1=\displaystyle\sum_{i=1}^n|x_i|\).
\(||\mathbf{x}_p||=(\displaystyle\sum_{i=1}^n|x_i|^p)^{1/p}\)
</p>

\begin{definition}
A sequence $\{\mathbf{x}^{(k)}\}_{k=1}^\infty$ of vectors in $R^n$ 
\textcolor{red}{converge to} $\mathbf{x}$ w.r.t the norm $||\cdot||$ if
given any $\epsilon>0$ there exists an integer $N(\epsilon)$ s.t.
$||\mathbf{x}^{(k)}-\mathbf{x}||<\epsilon$ for all $k\ge N(\epsilon)$
\end{definition}

\begin{theorem}
The sequence of vectors $\{\mathbf{x}^{(k)}\}$ converges to $ \mathbf{x}\in R^n$
w.r.t. $||\cdot||$ if and only if $ \lim\limits_{k\to\infty}\mathbf{x}^{(k)}_i=x_i$
for each $i=1,2,\dots,n$
\end{theorem}

\begin{definition}
If there exist positive constants $C_1,C_2$ s.t. $C_1||\mathbf{x}||_B\le||\mathbf{x}||_A
\le C_2||\mathbf{x}|_B|$. Then $||\cdot||_A,||\cdot||_B$ are \textcolor{red}{equivalent} 
\end{definition}

\begin{theorem}
All the vector norm in $R^n$ are equivalent
\end{theorem}


\begin{definition}
A \textcolor{red}{matrix norm} on the set of $n\times n$:
\begin{enumerate}
\item $||\mathbf{A}||\ge0;||\mathbf{A}||=0\Longleftrightarrow \mathbf{A}=\mathbf{0}$
\item $||\alpha \mathbf{A}||=|\alpha|\cdot||\mathbf{A}||$
\item $||\mathbf{A}+\mathbf{B}||\le||\mathbf{A}||+||\mathbf{B}||$
\item $||\mathbf{AB}||\le||\mathbf{A}||\cdot||\mathbf{B}||$
\end{enumerate}
\end{definition}

<p>
<b>Frobenius Norm</b>: \(||\mathbf{A}||_F=\sqrt{\displaystyle\sum_{i=1}^n
   \displaystyle\sum_{j=1}^n|a_{ij}|^2}\)
</p>

<p>
<b>Natural Norm</b>: \(||\mathbf{A}||_p=\displaystyle\max_{\mathbf{x}\neq
   \mathbf{0}}\frac{||\mathbf{Ax}||_p}{||\mathbf{x}||_p}=\displaystyle\max_{\mathbf{z}\neq
   \mathbf{0}}||\mathbf{A}\frac{\mathbf{z}}{||\mathbf{z}||}||=\displaystyle\max_{||\mathbf{x}||_p=1}||\mathbf{Ax}||_p\)
</p>

<p>
\(||\mathbf{A}||_\infty=\displaystyle\max_{1\le i\le n}\displaystyle\sum_{j=1}^n|a_{ij}|\),
\(||\mathbf{A}||_1=\displaystyle\max_{1\le j\le n}\displaystyle\sum_{i=1}^n|a_{ij}|\),
\(||\mathbf{A}||_2=\sqrt{\lambda_\text{max}(\mathbf{A}^T \mathbf{A})}\)
</p>
</div>
</div>
<div id="outline-container-org880da26" class="outline-3">
<h3 id="org880da26">7.2 Eigenvalues and Eigenvectors</h3>
<div class="outline-text-3" id="text-org880da26">
<p>
<b>spectral radius</b>.
</p>
\begin{definition}
The \textcolor{red}{spectral radius $\rho(A)$} of a matrix A is defined as
$\rho(A)=\max|\lambda|$ where $\lambda$ is an eigenvalue of A
\end{definition}

\begin{theorem}
If A is an $n\times n$ matrix, then $\rho(A)\le||A||$ for any natural norm
\end{theorem}

\begin{proof}
$|\lambda|\cdot||\bl{x}||=||\lambda\bl{x}||=||A\bl{x}||\le||A||\cdot||\bl{x}||$
\end{proof}

\begin{definition}
We call an $n\times n$ matrix A \textcolor{red}{convergent} if for all $i,j=1,\dots,n$
$\lim\limits_{k\to\infty}(A^k)_{ij}=0$
\end{definition}
</div>
</div>
<div id="outline-container-org9aa5047" class="outline-3">
<h3 id="org9aa5047">7.3 Iterative techniques for solving linear systems</h3>
<div class="outline-text-3" id="text-org9aa5047">
<p>
<b>Jacobi iterative method</b>.
</p>
\begin{equation*}
\begin{cases}
a_{11}x_1+a_{12}x_2+\dots+a_{1n}x_n=b_1\\
a_{21}x_1+a_{22}x_2+\dots+a_{2n}x_n=b_2\\
\dots\\
a_{n1}x_1+a_{n2}x_2+\dots+a_{nn}x_n=b_n\\
\end{cases}\Longrightarrow
\begin{cases}
x_1=\frac{1}{a_{11}}(-a_{12}x_2-\dots-a_{1n}x_n+b_1)\\
x_2=\frac{1}{a_{22}}(-a_{21}x_1-\dots-a_{2n}x_n+b_2)\\
\dots\\
x_1=\frac{1}{a_{nn}}(-a_{n2}x_1-\dots-a_{nn-1}x_{n-1}+b_n)\\
\end{cases}
\end{equation*}
<p>
In matrix form, 
</p>
\begin{equation*}
A=
\begin{pmatrix}
D&-U&-U\\
-L&D&-U\\
-L&-L&D
\end{pmatrix}
\end{equation*}
\begin{align*}
A\bl{x}=\bl{b}&\Leftrightarrow(D-L-U)\bl{x}=\bl{b}\\
&\Leftrightarrow D\bl{x}=(L+U)\bl{x}+\bl{b}\\
&\Leftrightarrow \bl{x}=\underbrace{D^{-1}(L+U)}_{T_j}\bl{x}+\underbrace{D^{-1}}_{\bl{c}_j}\bl{b}
\end{align*}.
\(T_j\) is Jacobi iterative matrix. \(\bl{x}^{(k)}=T_j\bl{x}^{(k-1)}+\bl{c}_j\)


*Gauss-Seidel iterative method*
\begin{align*}
&\bl{x}^{(k)}=D^{-1}(L\bl{x}^{(k)}+U\bl{x}^{(k-1)})+D^{-1}\bl{b}\\
\Leftrightarrow&(D-L)\bl{x}^{(k)}=U\bl{x}^{(k-1)}+\bl{b}\\
\Leftrightarrow&\bl{x}^{(k)}=\underbrace{(D-L)^{-1}U\bl{x^{(k-1)}}}_{T_g}
+\underbrace{(D-L)^{-1}\bl{b}}_{\bl{c}_g}
\end{align*}


<p>
<b>convergence of iterative methods</b>
</p>
\begin{theorem}
the following are equivalent:
\begin{enumerate}
\item A is a convergent matrix
\item $\lim\limits_{n\to\infty}||A^n|| = 0$ for some natural norm
\item $\lim\limits_{n\to\infty}||A^n||=0$ for all natural norms
\item $\rho(A)<1$
\item $\lim\limits_{n\to\infty}A^n\bl{x}=\bl{0}$ for every $\bl{x}$
\end{enumerate}
\end{theorem}

<p>
\(\bl{e}^{(k)}=\bl{x}^{(k)}-\bl{x}^*=(T\bl{x}^{(k-1)}+\bl{c})-(T\bl{x}^*+\bl{c})
   =T(\bl{x}^{(k-1)}-\bl{x}^*)=T\bl{e}^{(k-1)}\Rightarrow\bl{e}^{(k)}=T^k\bl{e}^{(0)}\).
\(||\bl{e}^{(k)}\le||T||\cdot||\bl{e}^{(k-1)}||\le\dots\le||T||^k\cdot||bl{e}^{(0)}||\)
</p>

\begin{theorem}
For any $\bl{x}^{(0)}\in R^n$, the sequence $\{\bl{x}^{(k)}\}_{k=0}^\infty$
defined by $\bl{x}^{(k)}=T\bl{x}^{(k-1)}+\bl{c}$ for each k, converges to the
unique solution of $\bl{x}=T\bl{x}+\bl{c}$ if and only if $\rho(T)<1$
\end{theorem}
<p>
\(\rho(T)<1\Longrightarrow(I-T)^{-1}=\displaystyle\sum_{j=0}^\infty T^j\)
</p>

\begin{theorem}
If $\norm{T}<1$ for any natural matrix norm and $\bl{c}$ is a given vector, then the
sequence $\{\bl{x}^{(k)}\}_{k=0}^\infty$ defined by $\bl{x}^{(k)}=T\bl{x}^{(k-1)}+\bl{c}$
converges for any $\bl{x}^{(0)}\in R^n$ to a vector $\bl{x}$. And the following
error bounds hold
\begin{enumerate}
\item $\norm{\bl{x}-\bl{x}^{(k)}}\le\norm{T}^k\norm{\bl{x}-\bl{x}^{(0)}}$
\item $\norm{\bl{x}-\bl{x}^{(k)}}\le\frac{\norm{T}^k}{1-\norm{T}}\norm{\bl{x}^{(1)}
-\bl{x}^{(0)}}$
\end{enumerate}
\end{theorem}

\begin{theorem}
If A is a strictly diagonally dominant, then for any choice of $\bl{x}^{(0)}$, both the
Jacobi and Gauss-Seidel methods give sequences $\{\bl{x}^{(k)}\}_{k=0}^\infty$
that converges to the unique solution
\end{theorem}

<p>
<b>relaxation methods</b>.
\(x_i^{(k)}=\frac{1}{a_{ii}}(b_i-\displaystyle\sum_{j=1}^{i-1}a_{ij}x_i^{(k)}-
   \displaystyle\sum_{j=i+1}^na_{ij}x_j^{(k-1)})=x_i^{(k-1)}+\frac{r_i^{(k)}}{a_{ii}}\)
and relaxation method is
\(x_i^{(k)}=x_i^{(k-1)}+\omega\frac{r_i^{(k)}}{a_{ii}}\)
</p>

\begin{theorem}{(kahan)}
If $a_{ii}\neq 0$ for each i. Then $\rho(T_\omega)\ge\abs{\omega-1}$.
\end{theorem}
<p>
This implies the SOR method can converge only if \(0<\omega<2\)
</p>

\begin{theorem}{(Ostrowski-Reich)}
If A is positive definite and $0<\omega<2$, the SOR converges
\end{theorem}

\begin{theorem}
If A is positive definite and tridiagonal, then $\rho(T_g)=(\rho(T_j))^2<1$, and
the optimal choice of $\omega$ for the SOR method is
$\omega=\frac{2}{1+\sqrt{1-(\rho(T_j))^2}}$. With this choice of $\omega$, we
have $\rho(T_\omega)=\omega-1$
\end{theorem}
</div>
</div>

<div id="outline-container-orgcf7aa9e" class="outline-3">
<h3 id="orgcf7aa9e">7.4 Error bounds and iterative refinement</h3>
<div class="outline-text-3" id="text-orgcf7aa9e">
<p>
Assume that A is accurate and \(\bl{b}\) has the error \(\delta \bl{b}\),
then \(\bl{A}(\bl{x}+\delta \bl{x})=\bl{b}+\delta \bl{b}\)
</p>

\begin{theorem}
Suppose $\tilde{\bl{x}}$ is an approximation to the solution of $ \bl{Ax=b}$
A is nonsingular matrix. Then for any natural norm,
\begin{equation*}
||\bl{x-\tilde{x}}||\le||\bl{r}||\cdot||A^{-1}||
\end{equation*}
and if $ \bl{x\neq 0, b\neq 0}$,
\begin{equation*}
\frac{||\delta\bl{x}||}{||\bl{x}||}\le||\bl{A}
||\cdot||\bl{A}^{-1}||\cdot \frac{||\delta\bl{b}||}{||\bl{b}||}
\end{equation*}
\end{theorem}

\begin{proof}
$\bl{r=b-A\tilde{x}}=A\bl{x}-A\tilde{\bl{x}}$ and A is nonsingular. Hence 
$\bl{x-\tilde{x}}=A^{-1}\bl{r}$. Since $\frac{||A^{-1}\bl{r}||}{||\bl{r}||}\le||A^{-1}||$
, $||\bl{x-\tilde{x}}||=||A^{-1}\bl{x}||\le||A^-1||\cdot||\bl{r}||$. Also
$||\bl{b}||\le||A||\cdot||\bl{x}||$. So $1/||\bl{x}||\le||A||/||\bl{b}||$
\end{proof}

\begin{theorem}
If a matrix B satisfies $||B||<1$ for some natural norm, then
\begin{enumerate}
\item $I\pm B$ is nonsingular
\item $||(I\pm B)^{-1}||\le \frac{1}{1-||B||}$
\end{enumerate}
\end{theorem}

<p>
Assume \(\bl{b}\) is accurate, A has the error \(\delta A\), then
\((A+\delta A)(\bl{x}+\delta\bl{x})=\bl{b}\). Hence
\(\frac{||\delta\bl{x}||}{||\bl{x}||}\le \frac{||A^{-1}||\cdot||\delta
   A||}{1-||A^{-1}||\cdot||\delta A||}=\frac{||A||\cdot||A^{-1}||}{1
   -||A||\cdot||A^{-1}||\cdot \frac{||\delta A||}{||A||}}\)
</p>

<p>
<b>condition number K(A)</b> is \(||A||\cdot||A^{-1}||\)
</p>

\begin{theorem}
Suppose A is nonsingular and $||\delta A||\le \frac{1}{||A^{-1}||}$. The solution
$\bl{x}+\delta\bl{x}$ to $(A+\delta A)(\bl{x}+\delta\bl{x})$ approximates the solution
$\bl{x}$ of $A\bl{x}=\bl{b}$ with the error estimate
\begin{equation*}
\frac{||\delta\bl{x}||}{||\bl{x}||}\le \frac{K(A)}{1-K(A)||\delta A||/||A||}
(\frac{||\delta A||}{||A||}+ \frac{||\delta\bl{b}||}{||\bl{b}||})
\end{equation*}
\end{theorem}

<p>
note:
</p>
<ol class="org-ol">
<li>If A is symmetric, then \(K(A)_2= \frac{\max|\lambda|}{\min|\lambda|}\)</li>
<li>\(K(A)_p\ge1\) for all natural norm</li>
<li>\(K(\alpha A)_=K(A)\) for any \(\alpha\in R\)</li>
<li>\(K(A)_2=1\) if A is orthogonal</li>
<li>\(K(RA)_2=K(AR)_2=K(A)_2\) for all orthogonal matrix R_</li>
</ol>


<p>
<b>iterative refinement</b>:
</p>
\begin{theorem}
Suppose $\bl{x}^*$ is an approximation to the solution of $A\bl{x}=\bl{b}$, A is
nonsingular matrix and $\bl{r}=\bl{b}-A\bl{x}$. Then for any natural norm,
$||\bl{x-x^*}\le||\bl{r}||\cdot||A^{-1}||$, and if $\bl{x,b}\neq\bl{0}$
\begin{equation*}
\frac{||\bl{x}-\bl{x}^*||}{||\bl{x}||}\le K(A)\frac{||\bl{r}||}{||\bl{b}||}
\end{equation*}
\end{theorem}

<p>
<b>refinement</b>
</p>
<ol class="org-ol">
<li>\(A\bl{x}=\bl{b}\) =&gt; approximation \(\bl{x}_1\)</li>
<li>\(\bl{r}_1=\bl{b}-A\bl{x}_1\)</li>
<li>\(A\bl{d}_1=\bl{r}_1\) =&gt; \(\bl{d}_1\)</li>
<li>\(\bl{x}_2=\bl{x}_1+\bl{d}_1\)</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org9f371e2" class="outline-2">
<h2 id="org9f371e2">Chap8 Approximation theory</h2>
<div class="outline-text-2" id="text-org9f371e2">
<p>
Given \(x_1\dots x_m\) and \(y_1\dots y_m\) find a <b>simpler</b> function \(P(x)\approx
  f(x)\)
</p>
</div>
<div id="outline-container-org71d62ed" class="outline-3">
<h3 id="org71d62ed">8.1 Discrete least squares approximation</h3>
<div class="outline-text-3" id="text-org71d62ed">
<p>
Determine the polynomial \(P_n(x)=a_0+a_1x+\dots+a_nx^n\) to approximate the
data \(\{(x_i,y_i)\mid i=1,2,\dots,m\}\) s.t. the least squares error
\(E_2=\displaystyle\sum_{i=1}^m(P_n(x_i)-y_i)^2\) is minimized. Here \(n\ll m\)
</p>

<p>
\(E_2(a_0,\dots,a_n)=\displaystyle\sum_{i=1}^m(a_0+a_1x_i+\dots+a_nx^n_i-y_i)^2\)
</p>

<p>
For \(E_2\) to be minimized it's necessary that \(\frac{\partial E_2}{\partial
   a_k}=0\)
</p>

\begin{align*}
0&=\frac{\partial E_2}{\partial a_k}=
2 \displaystyle\sum_{i=1}^m(P_n(x_i)-y_i)\frac{\partial P_N(x_i)}{\partial a_k}\\
&=2 \displaystyle\sum_{i=1}^m(\displaystyle\sum_{j=0}^na_jx_i^j-y_i)x_i^k\\
&=2(\displaystyle\sum_{j=0}^na_j(\displaystyle\sum_{i=1}^mx_i^{j+k})-
\displaystyle\sum_{i=1}^my_ix_i^k)
\end{align*}

<p>
Let \(b_k=\displaystyle\sum_{i=1}^m x_i^k,
   c_k=\displaystyle\sum_{i=1}^my_ix_i^k\), then
</p>
\begin{equation*}
\begin{pmatrix}
b_{0+0} & \dots & b_{0+n}\\
\vdots & \vdots&\vdots\\
b_{n+0} & \dots & b_{n+n}\\
\end{pmatrix}
\begin{pmatrix}
a_0\\
\vdots\\
a_n
\end{pmatrix}=
\begin{pmatrix}
c_0\\
\vdots
c_n\\
\end{pmatrix}
\end{equation*}
</div>
</div>
<div id="outline-container-org3e87b92" class="outline-3">
<h3 id="org3e87b92">8.2 orthogonal polynomials and least squares approximation</h3>
<div class="outline-text-3" id="text-org3e87b92">
\begin{theorem}
If $\varphi_j(x)$ is a polynomial of degree $j$ for each $j=0,\dots,n$, then 
$\{\varphi_0(x),\dots,\varphi_n(x)\}$ is \textcolor{red}{linearly independent} on
any interval $[a,b]$
\end{theorem}

\begin{theorem}
Let $\prod_n$ be the set of all polynomials of degree at most n. If
$\{\varphi_0(x),\dots,\varphi_n(x)\}$ is a collection of linearly independent
polynomials in $\prod_n$ then any polynomials in $\prod_n$ can be written
uniquely as a linear combination of $\{\varphi_0(x),\dots,\varphi_n(x)\}$
\end{theorem}

\begin{definition}
For a general linear independent set of functions $\{\varphi_0(x),\dots,\varphi_n(x)\}$,
a linear combination of $\{\varphi_0(x),\dots,\varphi_n(x)\}$.
$P(x)=\displaystyle\sum_{j=0}^n\alpha_j\varphi_j(x)$ is called a
\textcolor{red}{generalized polynomial} 
\end{definition}


<p>
Weight function
</p>
\begin{align*}
&E=\sum w_i[P(x_i)-y_i]^2\\
&E=\int_a^bw(x)[P(x)-f(x)]^2dx
\end{align*}
\begin{equation*}
\sum w_i\norm{P(x)-f(x)}_2^2=\sum w_i\bl{e}^T\bl{e}=\bl{e}^TW\bl{e}
\end{equation*}
<p>
where
#+ATTR<sub>LATEX</sub> :mode math :environment pmatrix :math-preffix W=
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">w<sub>1</sub></td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&hellip;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">w<sub>n</sub>_</td>
</tr>
</tbody>
</table>


<p>
The <b>general least squares approximation problem</b>. \(E\) is minimized
</p>


<p>
<b>Inner product</b> and <b>norm</b>
</p>
\begin{equation*}
(f,g)=
\begin{cases}
\displaystyle\sum_{i=1}^m w_if(x_i)g(x_i)\\
\int_a^bw(x)f(x)g(x)dx
\end{cases}
\end{equation*}
<p>
It can be shown that \((f,g)\) is an <b>inner proudct</b> and \(\norm{f}=\sqrt{(f,f)}\)
is a <b>norm</b>
</p>


<p>
Hence, The general least squares approximation problem is to find a
generalized polynomial \(P(x)\) such that \(E=(P-y,P-y)=\norm{P-y}^2\) is
minimized. 
</p>


<p>
Let \(P(x)=a_0\phi_0(x)+\dots+a_n\phi_n(x)\). 
\(\frac{\partial E}{\partial a_k}=0\Longrightarrow
   \displaystyle\sum_{j=0}^n(\phi_k,\phi_j)a_j=(\phi_k,f)\).
</p>
\begin{equation*}
\begin{pmatrix}
&&\\
&b_{ij}=(\phi_i,\phi_j)&\\
&&
\end{pmatrix}
\begin{pmatrix}
a_0\\
\vdots\\
a_n
\end{pmatrix}
=
\begin{pmatrix}
(\phi_0,f)\\
\dots\\
(\phi_n,f)
\end{pmatrix}=\vec{c}
\end{equation*}


<p>
Example. When approximating \(f(x)\in C[0,1]\) with \(\phi_j(x)=x^j\) and
\(w(x)=1\), then
</p>
\begin{equation*}
(\phi_i,\phi_j)=\int_0^1x^ix^jdx=\frac{1}{i+j+1}
\end{equation*}
<p>
Hilbert matrix.
</p>


<p>
Improvement: Find a general linear independent set of functions s.t. any pair
is <b>orthogonal</b>, then the matrix will be diagonal. And
</p>
\begin{equation*}
a_k=\frac{(\phi_k,f)}{(\phi_k,\phi_k)}
\end{equation*}


<p>
<b>Construction</b>
</p>
\begin{theorem}
the set of polynomial functions defined in the following way is orthogonal on [a,b]
w.r.t. weight function $w$
\begin{align*}
\phi_0(x)&=1\\
\phi_1(x)&=x-B_1\\
\phi_k(x)&=(x-B_k)\phi_{k-1}(x)-C_k\phi_{k-2}(x)\\
B_k&=\frac{(x\phi_{k-1},\phi_{k-1})}{(\phi_{k-1},\phi_{k-1})}\\
C_k&=\frac{(x\phi_{k-1},\phi_{k-2})}{(\phi_{k-2},\phi_{k-2})}
\end{align*}
\end{theorem}


<p>
Example. Approximate
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-left">x</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">4</td>
</tr>

<tr>
<td class="org-left">y</td>
<td class="org-right">4</td>
<td class="org-right">10</td>
<td class="org-right">18</td>
<td class="org-right">26</td>
</tr>
</tbody>
</table>

<p>
with \(y=a_0+a_1x+a_2x^2, w=1\)
</p>


<p>
Solution. \(y=a_0\phi_0(x)+a_1\phi_1(x)+a_2\phi_2(x)\). \(\phi_0(x)=1\)
</p>
</div>
</div>
<div id="outline-container-orgf0e8fe4" class="outline-3">
<h3 id="orgf0e8fe4">8.3 Chebyshev polynomials and economization of power series</h3>
<div class="outline-text-3" id="text-orgf0e8fe4">
<p>
Minimize \(\norm{P-y}_\infty\), \textcolor{red}{minimax problem} 
</p>

<ol class="org-ol">
<li><p>
Find a polynomial \(P_n(x)\) of degree n s.t. \(\norm{P_n-f}_\infty\) is
minimized
</p>

\begin{definition}
If $P(x_0)-f(x_0)=\pm\norm{P-f}_\infty, \textcolor{red}{x_0}$ is called a
$(\pm)$ \textcolor{red}{deviation point}
\end{definition}

<p>
We can estimate the features of the polynomial
</p>
<ol class="org-ol">
<li>If \(f\in C[a,b]\) and f is \textcolor{red}{not} a polynomial of degree
n, then there exists a \textcolor{red}{unique} polynomial \(P_n(x)\) s.t. 
\(\norm{P_n-f}_\infty\) is minimized</li>
<li>\(P_n(x)\) exists, and must have both + and - deviation points</li>
<li>\begin{theorem}{Chebyshev Theorem }
$P_n(x)$ minimizes $\norm{P_n-f}\Longleftrightarrow P_n(x)$ has at least
\textcolor{red}{n+2} alternating + and - deviation points w.r.t. $f$.
That is, there exists a set of points $a\le t_1<\dots<t_{n+2}\le b$
s.t. 
\begin{equation*}
P_n(t_k)-f(t_k)=\pm(-1)^k\norm{P_n-f}_\infty
\end{equation*}
\end{theorem}

<p>
The set \(\{t_k\}\) is called the \textcolor{red}{Chebyshev altenating
sequence} 
</p></li>
</ol></li>

<li><p>
Determine the interpolating points \(\{x_0,\dots,x_n\}\) s.t. \(P_n(x)\)
minimizes the remainder
</p>

\begin{equation*}
\abs{P_n(x)-f(x)}=
\abs{R_n(x)}=\abs{\frac{f^{(n+1)}(\xi_x)}{(n+1)!}\displaystyle\prod_{i=0}^n(x-x_i)}
\end{equation*}

<p>
2.1 Find \(\{x_1,\dots,x_n\}\) s.t. \(\norm{\omega_n}_\infty\) is minimized on
\([-1,1]\), where \(\omega_n(x)=\displaystyle\prod_{i=1}^n(x-x_i)\).
</p>

<p>
Since \(\omega_n(x)=x^n-P_{n-1}(x)\), the problem becomes to
</p></li>
<li>Find a polynomial \(P_{n-1}(x) s.t. \norm{x^n-P_{n-1}(x)}_\infty\) is
minimized on \([-1,1]\)</li>
</ol>


<p>
<b>Chebyshev polynomials</b>. Consider the \(n+1\) extreme values of \(\cos(n\theta)\)
on \([0,\pi]\).
</p>

<p>
Let \(x=\cos(\theta)\), then \(x\in[-1,1]\), \(T_n(x)=\cos(n\theta)=
   cos(n\cdot \arccos x)\) is called the \textcolor{red}{Chebyshev polynomial}.
</p>

<p>
Properties:
</p>
<ol class="org-ol">
<li>\(t_k=\cos(\frac{k}{n}\pi), k=0,\dots,n,
      T_n(t_k)=(-1)^k\norm{T_n(x)}_\infty\)</li>
<li>\(T_n(x)\) has \(n\) roots \(x_k=\cos(\frac{2k-1}{2n}\pi), k=1,\dots,n\)</li>
<li><p>
\(T_n\) has recurrence relation
</p>
\begin{equation*}
T_0(x)=1,T_1(x)=x,T_{n+1}(x)=2xT_n(x)-T_{n-1}(x)
\end{equation*}</li>
<li><p>
\(\{T_0(x),T_1(x),\dots\}\) are orthogonal on \([-1,1]\) w.r.t. weight
function \(w(x)=1/\sqrt{1-x^2}\)
</p>
\begin{equation*}
(T_n,T_m)=\int_{-1}^1\frac{T_n(x)T_m(x)}{\sqrt{1-x^2}}dx=
\begin{cases}
0 & n\neq m\\
\pi & n=m=0\\
\pi/2&n=m\neq 0\\
\end{cases}
\end{equation*}</li>
</ol>


<p>
\(w_n(x)=x^n-P_{n-1}(x)=T_n(x)/2^{n-1}\). Let \(\Wt{\prod}\) ={monic polynomials
of degree n}. 
</p>
\begin{equation*}
\min_{w_n\in\wt{\prod}}\norm{w_n}_\infty=\norm{\frac{1}
{2^{n-1}}T_n(x)}_\infty=\frac{1}{2^{n-1}}
\end{equation*}
\begin{equation*}
\abs{P_n(x)-f(x)}=
\abs{R_n(x)}=\abs{\frac{f^{(n+1)}(\xi_x)}{(n+1)!}\displaystyle\prod_{i=0}^n(x-x_i)}
\end{equation*}
<p>
Take the \(n+1\) roots of \(T_{n+1}(x)\) as the interpolating points, then the
interpolating polynomial \(P_n(x)\) assumes the minimum upper bound of the
absolute error \(\frac{M}{2^n(n+1)!}\)
</p>


<p>
<b>Economization of power series</b>. Given \(P_n(x)\approx f(x)\), economization of
pppppppower series is to reduce the degree of polynomial with a <b>minimal loss of
accuracy</b> 
</p>


<p>
Consider approximating an arbitrary n-th degree polynomial
</p>
\begin{equation*}
P_n(x)=a_nx^n+a_{n-1}x^{n-1}+\dots+a_1x+a_0
\end{equation*}
<p>
with a polynomial \(P_{n-1}(x)\) by removing an n-th degree polynomial \(Q_n(x)\)
that has the coefficient \(a_n\) for \(x_n\). Then
</p>
\begin{equation*}
\max_{[-1,1]}\abs{f(x)-P_{n-1}(x)}\le\max_{[-1,1]}\abs{f(x)-P_n(x)}+
\max_{[-1,1]}\abs{Q_n(x)}
\end{equation*}
<p>
To minimize the loss of accuracy, \(Q_n(x)=a_n\frac{T_n(x)}{2^{n-1}}\)
</p>


<p>
Example. The 4-th order Taylor polynomial for \(f(x)=e^x\) on \([-1,1]\) is
</p>

\begin{equation*}
P_4=1+x+\frac{x^2}{2}+\frac{x^3}{5}+\frac{x^4}{24}
\end{equation*}
<p>
.The upper bound of truncation error is 
\(\abs{R_4(x)}\le\frac{e}{5!}\abs{x^5}\approx0.023\)
</p>

<p>
solution. \(T_4=8x^4-8x^2+1, Q_4\)
</p>
</div>
</div>
</div>
<div id="outline-container-orgf3cb22b" class="outline-2">
<h2 id="orgf3cb22b">chap9 Approximating Eigenvalues</h2>
<div class="outline-text-2" id="text-orgf3cb22b">
</div>
<div id="outline-container-org6280c01" class="outline-3">
<h3 id="org6280c01">9.3 the power method</h3>
<div class="outline-text-3" id="text-org6280c01">
<p>
<b>the original method</b>
Assumptions: A is an \(n\times n\) matrix with eigenvalues satisfying
\(|\lambda_1|>|\lambda_2|\ge\dots\ge|\lambda_n|\ge 0\)
</p>

\begin{align*}
&\bl{x}^{(0)}=\displaystyle\sum_{j=1}^{n}\beta_j\bl{v}_j,\quad\beta_1\neq 0\\
&\bl{x}^{(1)}=A\bl{x}^{(0)}=\displaystyle\sum_{j=1}^n\beta_j\lambda_j\bl{v}_j\\
&\bl{x}^{(2)}=A\bl{x}^{(1)}=\displaystyle\sum_{j=1}^n\beta_j\lambda_j^2\bl{v}_j\\
&\dots\\
&\bl{x}^{(k)}\approx\lambda_1^k\beta_1\bl{v}_1, \quad \lambda_1\approx
\frac{\bl{x}^{(k)}_i}{\bl{x}^{(k-1)}_i}
\end{align*}

<p>
<b>Normalization</b>. Suppose \(||\bl{x}||_\infty=1\). Let
\(||\bl{x}^{(k)}||_\infty=|x_{p_k}^{(k)}|\).Then \(\bl{u}^{(k-1)}=
   \frac{\bl{x}^{(k-1)}}{|x_{p_{k-1}}^{(k-1)}|}\) and
\(\bl{x}^{(k)}=A\bl{u}^{(k-1)}\).
Then \(\bl{u}^{(k)}= \frac{\bl{x}^{(k)}}{|x_{p_k}^{(k)}|}\to \bl{v}_1\).
\(\lambda_1\approx
   \frac{\bl{x}_i^{(k)}}{\bl{u}_i^{(k-1)}}=\bl{x}_{p_{k-1}}^{(k)}\)
</p>

<p>
Note:
</p>
<ol class="org-ol">
<li>the method works for <b>multiple</b> eigenvalues
\(\lambda_1=\lambda_2=\dots=\lambda_r\)</li>
<li>the method fails to converge if \(\lambda_1=-\lambda_2\)</li>
<li>Aitken's \(\Delta^2\) can be used</li>
</ol>


<p>
<b>Rate of convergence</b>. \(\bl{x}^{(k)}=A\bl{x}^{(k-1)}=\lambda_1^k
   \displaystyle\sum_{j=1}^n\beta_j(\frac{\lambda_j}{\lambda_1})^k\bl{v}_j\).
Make \(|\lambda_2/\lambda_1|\) as small as possible.
Assume \(\lambda_1>\lambda_2\ge\dots\ge\lambda_n, |\lambda_2|>|\lambda_n|\).
Let \(B=A-pI\), then \(|\lambda I-A|=|\lambda I-(B+pI)|=|(\lambda-p)I-B|\).
Hence \(\lambda_A-p=\lambda_B\). Since  \(\frac{|\lambda_2-p|}{|\lambda_1-p|}<
   \frac{|\lambda_2|}{|\lambda_1|}\) . The iteration is fast
</p>


<p>
<b>Inverse power method</b>. If A has
\(|\lambda_1|\ge|\lambda_2|\ge\dots>|\lambda_n|\), then \(A^{-1}\) has
\(|\frac{1}{\lambda_n}|>| \frac{1}{\lambda_{n-1}}|\ge\dots\ge|
   \frac{1}{\lambda_1}|\) 
</p>
</div>
</div>
</div>

<div id="outline-container-org31926e2" class="outline-2">
<h2 id="org31926e2"><span class="todo TODO">TODO</span> hw <code>[0/15]</code></h2>
<div class="outline-text-2" id="text-org31926e2">
<p>
<code>C-u C-c C-c</code>
</p>
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> NA01-CH1-A</li>
<li class="off"><code>[&#xa0;]</code> NA02-CH2-A</li>
<li class="off"><code>[&#xa0;]</code> NA03-CH6-AB</li>
<li class="off"><code>[&#xa0;]</code> NA04-CH6-A</li>
<li class="off"><code>[&#xa0;]</code> NA04-CH7-A</li>
<li class="off"><code>[&#xa0;]</code> NA05-CH7-A</li>
<li class="off"><code>[&#xa0;]</code> NA06-CH3-A</li>
<li class="off"><code>[&#xa0;]</code> NA06-CH7-A
conditional number
hilber matrix</li>
<li class="off"><code>[&#xa0;]</code> NA06 CH9 -A</li>
<li class="off"><code>[&#xa0;]</code> NA07-CH3-AB</li>
<li class="off"><code>[&#xa0;]</code> NA08-CH3-A</li>
<li class="off"><code>[&#xa0;]</code> NA08-CH8-A
least squares polynomial</li>

<li class="off"><code>[&#xa0;]</code> NA09-CH8-A
least squares polynomial orthogonal</li>
<li class="off"><code>[&#xa0;]</code> NA10-CH4-A
numerical differentiation</li>
<li class="off"><code>[&#xa0;]</code> NA10-CH8-A</li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: gouziwu</p>
<p class="date">Created: 2019-05-23 å›› 11:25</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
