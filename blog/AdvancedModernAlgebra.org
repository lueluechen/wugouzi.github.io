#+TITLE: Advanced Modern Algebra
#+AUTHOR: Joseph J. Rotman

#+OPTIONS: tex:imagemagick
#+OPTIONS: toc:2
#+LATEX_HEADER: \input{preamble.tex}
#+LATEX_HEADER: \setcounter{secnumdepth}{2}
#+LATEX_HEADER: \setcounter{tocdepth}{2}
#+EXPORT_FILE_NAME: ../latex/AdvancedModernAlgebra/AdvancedModernAlgebra.tex
#+LATEX_HEADER: \DeclareMathOperator{\Frac}{Frac}


* Things Past
** Some Number Theory
   *Least Integer Axiom* (*Well-ordering Principle*). There is a smallest integer in
   every nonempty subset $C$ of $\N$
** Roots of Unity
   #+ATTR_LATEX: :options [Polar Decomposition]
   #+BEGIN_proposition
   Every complex number $z$ has a factorization
   \begin{equation*}
   z=r(\cos\theta+i\sin\theta)
   \end{equation*}
   where $r=\abs{z}\ge0$ and $0\le\theta\le 2\pi$
   #+END_proposition

   #+ATTR_LATEX: :options [Addition Theorem]
   #+BEGIN_proposition
   If $z=\cos\theta+i\sin\theta$ and $w=\cos\psi+i\sin\psi$, then
   \begin{equation*}
   zw=\cos(\theta+\psi)+i\sin(\theta+\psi)
   \end{equation*}
   #+END_proposition
   
   #+ATTR_LATEX: :options [De Moivre]
   #+BEGIN_theorem
   $\forall x\in\R,n\in\N$
   \begin{equation*}
   \cos(nx)+i\sin(nx)=(\cos x+i\sin x)^n
   \end{equation*}
   #+END_theorem

   #+ATTR_LATEX: :options [Euler]
   #+BEGIN_theorem
   $e^{ix}=\cos x+i\sin x$
   #+END_theorem

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $n\in\N\ge 1$ , an *nth root of unity* is a complex number $\xi$ with
   $\xi^n=1$

   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Every nth root of unity is equal to
   \begin{equation*}
   e^{2\pi ik/n}=\cos(\frac{2\pi k}{n})+i\sin(\frac{2\pi k}{n})
   \end{equation*}
   for $k=0,1,\dots,n-1$
   #+END_corollary

   \begin{equation*}
   x^n-1=\displaystyle\prod_{\xi^n=1}(x-\xi)
   \end{equation*}

   If $\xi$ is an nth root of unity and if $n$ is the smallest, then $\xi$ is a
   *primitive* \(n\)\tf{th root of unity}

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $d\in\N^+$ , then the $d$th *cyclotomic polynomial* is 
   \begin{equation*}
   \Phi_d(x)=\displaystyle\prod(x-\xi)
   \end{equation*}
   where $\xi$ ranges over all the /primitive dth/ roots of unity
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   For every integer $n\ge 1$
   \begin{equation*}
   x^n-1=\displaystyle\prod_{d|n}\Phi_d(x)
   \end{equation*}
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Define *Euler \phi-function* as the degree of the nth cyclotomic
   polynomial
   \begin{equation*}
   \phi(n)=\deg(\Phi_n(x))
   \end{equation*}
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $n\ge1$ is an integer, then $\phi(n)$ is the number of integers $k$ with
   $1\le k\le n$ and $(k,n)=1$
   #+END_proposition

   #+BEGIN_proof
   Suffice to prove $e^{2\pi ik/n}$ is a primitive nth root of unity if and only
   if $k$ and $n$ are relatively prime
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   For every integer $n\ge 1$, we have
   \begin{equation*}
   n=\displaystyle\sum_{d|n}\phi(d)
   \end{equation*}
   #+END_corollary
** Some Set Theory
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop1.47
   1. If \(f:X\to Y\) and \(g:Y\to X\) are functions s.t. \(g\circ f=1_X\), then
      $f$ is injective and $g$ is surjective
   2. A function $f:X\to Y$ has an inverse \(g:Y\to X\) if and only if $f$ is a bijection
   #+END_proposition
* Group \rom{1}
** Permutations
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A *permutation* of a set $X$ is a bijection from $X$ to itself.
   #+END_definition


   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   The family of all the permutations of a set $X$, denoted by $S_X$ is called
   the \textbf{symmetric group} on $X$. When $X=\lb 1,2,\dots,n\rb$, $S_X$ is
   usually denoted by $X_n$ and is called the \textbf{symmetric group on } $n$
   \textbf{letters} 
   #+END_definition
   
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Let $i_1,i_2,\dots,i_r$ be distinct integers in $\lb 1,2,\dots,n\rb$. If
   $\alpha\in S_n$ fixes the other integers and if
   \begin{equation*}
   \alpha(i_1)=i_2,\alpha(i_2)=i_3,\dots,\alpha(i_{r-1})=i_r,\alpha(i_r)=i_1
   \end{equation*}
   then \alpha is called an textbf{r-cycle}. \alpha is a cycle of
   *length* $r$ and denoted by
   \begin{equation*}
   \alpha=(i_1\; i_2\;\dots\; i_r)
   \end{equation*}
   #+END_definition

   2-cycles are also called the *transpositions*.

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Two permutations $\alpha,\beta\in S_n$ are \textbf{disjoint} if every $i$
   moved by one is fixed by the other.
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   Disjoint permutations $\alpha,\beta\in S_n$ commute
   #+END_lemma
   
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Every permutation $\alpha\in S_n$ is either a cycle or a product of disjoint cycles.
   #+END_proposition

   #+BEGIN_proof
   Induction on the number $k$ of points moved by $\alpha$
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A *complete factorization* of a permutation $\alpha$ is a
   factorization of $\alpha$ into disjoint cycles that contains exactly one
   1-cycle $(i)$ for every $i$ fixed by $\alpha$
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let $\alpha\in S_n$ and let $\alpha=\beta_1\dots\beta_t$ be a complete
   factorization into disjoint cycles. This factorization is unique except for
   the order in which the cycles occur
   #+END_theorem

   #+BEGIN_proof
   for all $i$, if $\beta_t(i)\neq i$, then $\beta_t^k(i)\neq\beta_t^{k-1}(i)$
   for any $k\ge 1$
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   If $\gamma,\alpha\in S_n$, then $\alpha\gamma\alpha^{-1}$ has the same cycle
   structure as $\gamma$. In more detail, if the complete factorization of
   $\gamma$ is
   \begin{equation*}
   \gamma=\beta_1\beta_2\dots(i_1\; i_2\;\dots)\dots\beta_t
   \end{equation*}
   then $\alpha\gamma\alpha^{-1}$ is permutation that is obtained from $\gamma$
   by applying $\alpha$ to the symbols in the cycles of $\gamma$
   #+END_lemma

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   label:example2.8
   Suppose
   \begin{gather*}
   \beta=(1\;2\;3)(4)(5)\\
   \gamma=(5\;2\;4)(1)(3)
   \end{gather*}
   then we can easily find the $\alpha$
   \begin{equation*}
   \alpha=
   \begin{pmatrix}
   1&2&3&4&5\\
   5&2&4&1&3
   \end{pmatrix}
   \end{equation*}
   and so $\alpha=(1\;5\;3\;4)$. Now $\alpha\in S_5$ and $\gamma=(\alpha 1\;\alpha
   2\;\alpha 3)$
   #+END_examplle
   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Permutations $\gamma$ and $\sigma$ in $S_n$ has the same cycle structure if
   and only if there exists $\alpha\in S_n$ with $\sigma=\alpha\gamma\alpha^{-1}$
   #+END_theorem


   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $n\ge 2$ then every $\alpha\in S_n$ is a product of tranpositions
   #+END_proposition
   #+BEGIN_proof
   $(1\;2\;\dots\; r)=(1\; r)(1\; r-1)\dots(1\; 2)$
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   The *15-puzzle* has a *starting position* that is a $4\times 4$ array of the
   numbers between 1 and 15 and a symbol #, which we interpret as "blank". For
   example, consider the following starting position

   #+ATTR_LATEX: :align |c|c|c|c|
   |----+----+----+----|
   |  3 | 15 |  4 |  8 |
   |----+----+----+----|
   | 10 | 11 |  1 |  9 |
   |----+----+----+----|
   |  2 |  5 | 13 | 12 |
   |----+----+----+----|
   |  6 |  7 | 14 |  # |
   |----+----+----+----|

   A *simple move* interchanges the blank with a symbol adjacent to it. We win the
   game if after a sequence of simple moves, the starting position is
   transformed into the standard array $1,2,\dots,15,\#$. 

   To analyze this game, note that the given array is really a permutation
   $\alpha\in S_{16}$. For example, the given starting position is
   #+ATTR_LATEX: :mode math :environment pmatrix
   | 1 |  2 | 3 | 4 |  5 |  6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |
   | 3 | 15 | 4 | 8 | 10 | 11 | 1 | 9 | 2 |  5 | 13 | 12 |  6 |  7 | 14 | 16 |

   To win the game, we need special transpositions $\tau_1,\dots,\tau_m$ sot
   that
   \begin{equation*}
   \tau_m\dots\tau_1\alpha=(1)
   \end{equation*}
   #+END_examplle

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A permutation $\alpha\in S_n$ is *even* if it can be factored into a
   product of an even number of transpositions. Otherwise *odd*
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $\alpha\in S_n$ and $\alpha=\beta_1\dots\beta_t$ is a complete
   factorization, then \textbf{signum} $\alpha$ is defined by
   \begin{equation*}
   \sgn(\alpha)=(-1)^{n-t}
   \end{equation*}
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   For all $\alpha,\beta\in S_n$
   \begin{equation*}
   \sgn(\alpha\beta)=\sgn(\alpha)\sgn(\beta)
   \end{equation*}
   #+END_theorem

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   1. Let $\alpha\in S_n$; if $\sgn(\alpha)=1$ then $\alpha$ is even. otherwise
      odd
   2. A permutation $\alpha$ is odd if and only if it's a product of an odd
      number of transpositions
   #+END_theorem

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let $\alpha,\beta\in S_n$. If $\alpha$ and $\beta$ have the same parity, then
   $\alpha\beta$ is even while if $\alpha$ and $\beta$ have distinct parity,
   $\alpha\beta$ is odd
   #+END_corollary

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   An analysis of the 15-puzzle shows that if $\alpha\in S_{16}$ is the starting
   position, then the game can be won if and only if \alpha is an even permutation
   that fixes 16.

   The blank 16 starts in position 16. Each simple move takes 16 up, down, left
   or right. Thus the total number $m$ of moves is $u+d+l+r$. If 16 is to return
   home, each one of these must be undone. Thus the total number of moves is
   even: $m=2u+2r$. Hence $\alpha=\tau_1\dots\tau_m$ and so $\alpha$ is an even
   permutation. In example
   \begin{equation*}
   \alpha=(1\;3\;4\;8\;9\;2\;15\;14\;7)(5\;10)(6\;11\;13)(12)(16)
   \end{equation*}
   Now $\sgn(\alpha)=(-1)^{16-5}=-1$.
   #+END_examplle
** Groups
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A \textbf{binary operation} on a set $G$ is a function
   \begin{equation*}
   *:G\times G\to G
   \end{equation*}
   #+END_definition
   
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A \textbf{group} is a set $G$ equipped with a binary operation * s.t.
   1. the \textbf{associative law} holds
   2. \textbf{identity}
   3. every $x\in G$ has an \textbf{inverse}, there is a $x'\in G$  with 
      $x*x'=e=x'*x$
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A group $G$ is called \textbf{abelian} if it satisfies the
   \textbf{commutative law}
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   Let $G$ be a group
   1. The \textbf{cancellation laws} holds: if either $x*a=x*b$ or $a*x=b*x$, then
      $a=b$
   2. $e$ is unique
   3. Each $x\in G$ has a unique inverse
   4. $(x^{-1})^{-1}=x$
   #+END_lemma

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   An expression $a_1a_2\dots a_n$ \textbf{needs no parentheses} if all the ultimate
   products it yields are equal
   #+END_definition

   #+ATTR_LATEX: :options [Generalized Associativity]
   #+BEGIN_theorem
   If $G$ is a group and $a_1,a_2,\dots,a_n\in G$ then the expression
   $a_1a_2\dots a_n$ needs no parentheses
   #+END_theorem

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Let $G$ be a group and let $a\in G$. If $a^k=1$ for some $k>1$ then the
   smallest such exponent $k\ge 1$ is called the *order* or $a$; if no such
   power exists, then one says that $a$ has *infinite order*
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $G$ is a finite group, then every $x\in G$ has finite order
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A *motion* is a distance preserving bijection $\varphi:\R^2\to\R^2$. If
   \pi is a polygon in the plane, then its *symmetry group* $\Sigma(\pi)$
   consists of all the motions $\varphi$ for which $\varphi(\pi)=\pi$. The
   elements of $\Sigma(\pi)$ are called the *symmetries* of \pi
   #+END_definition

   Let $\pi_4$ be a square. Then the group $\Sigma(\pi_4)$ is called the
   *dihedral group* with 8 elements, denoted by $D_8$

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $\pi_n$ is a regular polygon with $n$ vertices $v_1,\dots,v_n$ and center
   $O$, then the symmetry group $\Sigma(\pi_n)$ is called the \tf{dihedral
   group} with $2n$ elements, and it's denoted by $D_{2n}$
   #+END_definition

   #+BEGIN_exercise
   label:ex2.26
   If $G$ is a group in which \(x^2=1\) for every \(x\in G\), prove that $G$
   must be abelian
   #+END_exercise

#+BEGIN_exercise
label:ex2.27
If $G$ is a group with an even number of elements, prove that the number of
elements in $G$ of order 2 is odd. In particular, $G$ must contain an element of
order 2.
#+END_exercise

#+BEGIN_proof
1 is an element of order 1.
#+END_proof
** Lagrange's Theorem
   #+ATTR_LATEX: :options []
   #+BEGIN_theorem

   #+END_theorem

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A subset $H$ of a group $G$ is a *subgroup* if
   1. $1\in H$
   2. if $x,y\in H$, then $xy\in H$
   3. if $x\in H$, then $x^{-1}\in H$
   #+END_definition

   If $H$ is a subgroup of $G$, we write $H\le G$. If $H$ is a proper subgroup,
   then we write $H<G$

   The four permutations
   \begin{equation*}
   \bV=\{(1),(1 2)(3 4),(1 3)(2 4),(1 4)(2 3)\}
   \end{equation*}
   form a group because $\bV\le S_4$

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   A subset $H$ of a group $G$ is a subgroup if and only if $H$ is nonempty and
   whenever $x,y\in H$, $xy^{-1}\in H$
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   A nonempty subset $H$ of a finite group $G$ is a subgroup if and only if $H$
   is closed; that is, if $a,b\in H$, then $ab\in H$
   #+END_proposition

   [[index:alternating group]]
   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   The subset $A_n$ of $S_n$, consisting of all the even permutations, is a
   subgroup called the *alternating group* on $n$ letters
   #+END_examplle

   [[index:cyclic group]]
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $G$ is a group and $a\in G$
   \begin{equation*}
   \langle a\rangle=\{a^n:n\in\Z\}=\{\text{all powers of } a\}
   \end{equation*}
   $\la a\ra$ is called the *cyclic subgroup* of $G$ *generated* by $a$. A
   group $G$ is called *cyclic* if there exists $a\in G$ s.t. $G=\la a\ra$,
   in which case $a$ is called the *generator*
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   The *integers mod $m$*, denoted by $\I_m$ is the family of all congruence
   classes mod $m$
   #+END_definition


   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let $m\ge 2$ be a fixed integer
   1. If $a\in \Z$, then $[a]=[r]$ for some $r$ with $0\le r<m$
   2. If $0\le r'<r<m$, then $[r']\neq[r]$
   3. $\I_m$ has exactly $m$ elements
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   1. If $G=\la a\ra$ is a cyclic group of order $n$, then $a^k$ is a generator
      of $G$ if and only if $(k,n)=1$
   2. If $G$ is a cyclic group of order $n$ and $\gen(G)=\{\text{all generators
      of } G\}$, then
      \begin{equation*}
      \abs{\gen(G)}=\phi(n)
      \end{equation*}
      where $\phi$ is the Euler \phi-function
   #+END_theorem
   #+BEGIN_proof
   1. there is $t\in\N$ s.t. $a^{kt}=a$ hence $a^{kt-1}=1$ and $n\mid kt-1$
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let $G$ be a finite group and let $a\in G$. Then the order of $a$ is
   $\abs{\la a\ra}$.
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $G$ is a finite group, then the number of elements in $G$, denoted by
   $\abs{G}$ is called the *order* of $G$
   #+END_definition


   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   The intersection $\bigcap_{i\in I}H_i$ of any family of subgroups of a group
   $G$ is again a subgroup of $G$
   #+END_proposition


   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   If $X$ is a subset of a group $G$, then there is a subgroup $\la X\ra$ of $G$
   containing $X$ tHhat is *smallest* in the sense that $\la X\ra\le H$ for
   every subgroup $H$ 
   of $G$ that contains $X$
   
   #+END_corollary


   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $X$ is a subset of a group $G$, then $\la X\ra$ is called the *subgroup*
   *generated by* $X$
   #+END_definition

   A *word* on $X$ is an element $g\in G$ of the form $g=x_1^{e_1}\dots
   x_n^{e_n}$ where $x_i\in X$ and $e_i=\pm 1$ for all $i$

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $X$ is a nonempty subset of a group $G$, then $\la X\ra$ is the set of all
   words on $X$
   #+END_proposition

   [[index:coset]]
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $H\le G$ and $a\in G$, then the *coset* $aH$ is the subset $aH$ of $G$,
   where
   \begin{equation*}
   aH=\{ah:h\in H\}
   \end{equation*}
   #+END_definition
   $aH$ *left coset*, $Ha$ *right coset*

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   $H\le G,a,b\in G$
   1. $aH=bH$ if and only if $b^{-1}a\in H$
   2. if $aH\cap bH\neq\emptyset$, then $aH=bH$
   3. $\abs{aH}=\abs{H}$ for all $a\in G$
   #+END_lemma
   #+BEGIN_proof
   define a relation $a\equiv b$ if $b^{-1}a\in H$
   #+END_proof


   #+ATTR_LATEX: :options [Lagrange's Theorem]
   #+BEGIN_theorem
   If $H$ is a subgroup of a finite group $G$, then $\abs{H}$ is a divisor of $\abs{G}$
   #+END_theorem

   #+BEGIN_proof
   Let $\{a_1H,a_2H,\dots,a_tH\}$ be the family of all the distinct cosets of
   $H$ in $G$. Then
   \begin{equation*}
   G=a_1H\cup a_2H\cup\dots\cup a_tH
   \end{equation*}
   hence
   \begin{equation*}
   \abs{G}=\abs{a_1H}+\dots+\abs{a_tH}
   \end{equation*}
   But $\abs{a_iH}=\abs{H}$ for all $i$. Hence $\abs{G}=t\abs{H}$
   #+END_proof

   [[index:index]]
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   The *index* of a subgroup $H$ in $G$ denoted by $[G:H]$, is the number of
   left cosets of $H$ in $G$
   #+END_definition

   Note that $\abs{G}=[G:H]\abs{H}$

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   If $G$ is a finite group and $a\in G$, then the order of $a$ is a divisor of
   $\abs{G}$ 
   #+END_corollary

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   If $G$ is a finite group, then $a^{\abs{G}}=1$ for all $a\in G$
   #+END_corollary

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   If $p$ is a prime, then every group $G$ of order $p$ is cyclic
   #+END_corollary

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   The set $U(\I_m)$, defined by
   \begin{equation*}
    U(\I_m)=\{[r]\in\I_m:(r,m)=1\}
   \end{equation*}
   is a multiplicative group of order $\phi(m)$. If $p$ is a prime, then
   $U(\I_p)=\I_p^{\times}$, the nonzero elements of $\I_p$.
   #+END_proposition
   
   #+BEGIN_proof
   $(r,m)=1=(r',m)$ implies $(rr',m)=1$. Hence $U(\I_m)$ is closed under
   multiplication. If $(x,m)=1$, then $rs+sm=1$. There fore $(r,m)=1$. Each of
   them have inverse.
   #+END_proof

   [[index:Fermat's Theorem]]
   #+ATTR_LATEX: :options [Fermat]
   #+BEGIN_corollary
   label:Fermat
   If $p$ is a prime and $a\in\Z$, then
   \begin{equation*}
   a^p\equiv a\mod p
   \end{equation*}
   #+END_corollary

   #+BEGIN_proof
   suffices to show $[a^p]=[a]$ in $\I_p$. If $[a]=[0]$, then $[a^p]=[a]^p=[0]$.
   Else, since $\abs{\I_p^\times}=p-1$, $[a]^{p-1}=[1]$
   #+END_proof


   #+ATTR_LATEX: :options [Euler]
   #+BEGIN_theorem
   If $(r,m)=1$, then
   \begin{equation*}
   r^{\phi(m)}\equiv 1\mod m
   \end{equation*}
   #+END_theorem
   #+BEGIN_proof
   Since $\abs{U(\I_m)}=\phi(m)$. Lagrange's theorem gives
   $[r]^{\phi(m)}=[1]$ for all $[r]\in U(\I_m)$.

   In fact we construct a group to prove this.
   #+END_proof

   #+ATTR_LATEX: :options [Wilson's Theorem]
   #+BEGIN_theorem
   An integer $p$ is a prime if and only if
   \begin{equation*}
   (p-1)!\equiv -1\mod p
   \end{equation*}
   #+END_theorem

   #+BEGIN_proof
   Assume that $p$ is a prime. If $a_1,\dots,a_n$ is a list of all the elements
   of finite abelian group, then product $a_1a_2\dots a_n$ is the same as the
   product of all elements $a$ with $a^2=1$. Since $p$ is prime, $\I_p^\times$
   has only one element of order 2, namely $[-1]$. It follows that the product
   of all the elements in $\I_p^\times$ namely $[(p-1)!]$ is equal to $[-1]$.

   Conversly assume that $m$ is composite: there are integers $a$ and $b$ with
   $m=ab$ and $1<a\le b<m$. If $a<b$ then $m=ab$ is a divisor of $(m-1)!$. If
   $a=b$, then $m=a^2$. if $a=2$, then $(a^2-1)!\equiv 2\mod 4$. If $2<a$, then
   $2a<a^2$ and so $a$ and $2a$ are factors of $(a^2-1)!$
   #+END_proof

   #+BEGIN_exercise
   label:ex2.36
   Let $G$ be a group of order 4. Prove that either $G$ is cyclic or \(x^2=1\)
   for every \(x\in G\). Conclude, using Exercise ref:ex2.26 that $G$ must be abelian.
   #+END_exercise

   #+BEGIN_proof
   
   #+END_proof
** Homomorphisms
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $(G,*)$ and $(H,\circ)$ are groups, then a function $f:G\to H$ is a
   *homomorphism* if
   \begin{equation*}
   f(x*y)=f(x)\circ f(y)
   \end{equation*}
   for all $x,y\in G$. If $f$ is also a bijection, then $f$ is called an
   *isomorphism*. $G$ and $H$ are called *isomorphic*, denoted by $G\cong H$
   #+END_definition
   
   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   Let $f:G\to H$ be a homomorphism
   1. $f(1)=1$
   2. $f(x^{-1})=f(x)^{-1}$
   3. $f(x^n)=f(x)^n$ for all $n\in\Z$
   #+END_lemma

   

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $f:G\to H$ is a homomorphism, define
   \begin{equation*}
   \ker f=\{x\in G:f(x)=1
   \end{equation*}
   and
   \begin{equation*}
   \im f=\{h\in H:h=f(x)\text{ for some } x\in G\
   \end{equation*}
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let $f:G\to H$ be a homomorphism
   1. $\ker f$ is a subgroup of $G$ and $\im f$ is a subgroup of $H$
   2. if $x\in\ker f$ and if $a\in G$, then $axa^{-1}\in\ker f$
   3. $f$ is an injection if and only if $\ker f=\{1\}$
   #+END_proposition

   #+BEGIN_proof
   3. [@3] $f(a)=f(b)\Leftrightarrow f(ab^{-1})=1$
   #+END_proof
   
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A subgroup $K$ of a group $G$ is called a *normal subgroup* if $k\in K$
   and $g\in G$ imply $gkg^{-1}\in K$, denoted by $K\triangleleft G$
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $G$ is a group and $a\in G$, then a *conjugate* of $a$ is any element
   in $G$ of the form
   \begin{equation*}
   gag^{-1}
   \end{equation*}
   where $g\in G$
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $G$ is a group and $g\in G$, define *conjugation* $\gamma_g:G\to G$ by
   \begin{equation*}
   \gamma_g(a)=gag^{-1}
   \end{equation*}
   for all $a\in G$
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   1. If $G$ is a group and $g\in G$, then conjugation $\gamma_g:G\to G$ is an
      isomorphism
   2. Conjugate elements have the same order
   #+END_proposition

   #+BEGIN_proof
   1. bijection: $\gamma_g\circ\gamma_{g^{-1}}=1=\gamma_{g^{-1}}\circ\gamma_g$.
      
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   Define the *center* of a group $G$, denoted by $Z(G)$, to be
   \begin{equation*}
   Z(G)=\{z\in G:zg=gz\text{ for all }g\in G\}
   \end{equation*}
   #+END_examplle

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   If $G$ is a group, then an *automorphism* of $G$ is an isomorphism $f:G\to G$.
   For example, every conjugation $\gamma_g$ is an automorphism of $G$ (it is
   called an *inner automorphism*), for its inverse is conjugation by $g^{-1}$.
   The set $\aut(G)$ of all the automorphism of $G$ is itself a group.
   \begin{equation*}
   \inn(G)=\{\gamma_g:g\in G\}
   \end{equation*}
   is a subgroup of $\aut(G)$
   #+END_examplle
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   1. If $H$ is a subgroup of index 2 in a group $G$, then $g^2\in H$ for every
      $g\in G$
   2. If $H$ is a subgroup of index 2 in a group $G$, then $H$ is a normal
      subgroup of $G$
   #+END_proposition


   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   The group of *quaternions* is the group $\bQ$ of order 8 consisting of the
   following matrices in $GL(2, \C)$
   \begin{equation*}
   \bQ=\{I,A,A^2,A^3,B,BA,BA^2,BA^3\}
   \end{equation*}
   where $I$ is the identity matrix
   \begin{equation*}
   A=
   \begin{pmatrix}
   0&1\\
   -1&0
   \end{pmatrix}, \text{ and }
   B=\begin{pmatrix}
   0&i\\
   i&0
     \end{pmatrix}
   \end{equation*}
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   $\bQ$ is normal. By Lagrange's theorem the only possible orders of subgroups
   are 1,2,4 or 8. The only subgroup of order 2 is $\la -I\ra$ since $-I$ is the
   only element of order 2
   #+END_examplle
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   The alternating group $A_4$ is a group of order 12 having no subgroup of
   order 6
   #+END_proposition

#+BEGIN_exercise
Show that if there is a bijection $f:X\to Y$, then there is an isomorphism
$\varphi:S_X\to S_Y$
#+END_exercise
#+BEGIN_proof
If $\alpha\in S_X$, define $\varphi(\alpha)=f\circ\alpha\circ f^{-1}$. Since
$f,\alpha,f^{-1}$ are bijections, $\varphi(\alpha)$ is an bijection. \varphi is a
homomorphism. $\forall \beta\in S_Y$, we have $\alpha=f^{-1}\circ\beta\circ f$
#+END_proof
** Quotient group
   $\cals(G)$ is the set of all nonempty subsets of a group $G$. If
   $X,Y\in\cals(G)$, define
   \begin{equation*}
   XY=\{xy:x\in X\text{ and } y\in Y\}
   \end{equation*}

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   $K\le G$ is normal if and only if
   \begin{equation*}
   gK=Kg
   \end{equation*}
   #+END_lemma

   A natural question is that whether $HK$ is a subgroup when $H$ and $K$ are
   subgroups. The answer is no. Let $G=S_3,H=\la(1\;2)\ra,K=\la(1\;3)\ra$


   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   1. If $H$ and $K$ are subgroups of a group $G$, and if one of them is normal,
      then $HK\le G$ and $HK=KH$
   2. If $H,K\tril G$, then $HK\tril G$
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   Let $G/K$ denote the family of all the left cosets of a subgroup $K$ of $G$.
   If $K\tril G$, then
   \begin{equation*}
   aKbK=abK
   \end{equation*}
   for all $a,b\in G$ and $G/K$ is a group under this operation
   #+END_theorem

   #+BEGIN_proof
   $aKbK=abKK=abK$
   #+END_proof

   $G/K$ is called the *quotient group* $G$ mod $K$

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Every $K\tril G$ is the kernel of some homomorphism
   #+END_corollary
   
   #+BEGIN_proof
   Define the *natural map* $\pi:G\to G/K$, $a\mapsto aK$
   #+END_proof

   #+ATTR_LATEX: :options [First Isomorphism Theorem]
   #+BEGIN_theorem
   If $f:G\to H$ is a homomorphism, then
   \begin{equation*}
   \ker f\tril G\quad\text{ and }\quad G/\ker f\cong\im f
   \end{equation*}
   If $\ker f=K$ and $\varphi:G/K\to\im f\le H,aK\mapsto f(a)$, then $\varphi$
   is an isomorphism
   #+END_theorem
   
   #+BEGIN_remark
   \begin{center}
   \begin{tikzcd}
   G \arrow[rr,"f"] \arrow[dr,"\pi"]& &
   H\\ 
   &G/K \arrow[ur,"\varphi"]&
   \end{tikzcd}
   \end{center}
   #+END_remark

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   What's the quotient group $\R/\Z$? Define $f:\R\to S^1$ where $S^1$ is the
   circle group by
   \begin{equation*}
   f:x\mapsto e^{2\pi ix}
   \end{equation*}
   $\R/\Z\cong S^1$
   #+END_examplle

   #+ATTR_LATEX: :options [Product Formula]
   #+BEGIN_proposition
   If $H$ and $K$ are subgroups of a finite group $G$, then
   \begin{equation*}
   \abs{HK}\abs{H\cap K}=\abs{H}\abs{K}
   \end{equation*}
   #+END_proposition

   #+BEGIN_proof
   Define a function $f:H\times K\to HK,(h,k)\mapsto hk$. Show that
   $\abs{f^{-1}(x)}=\abs{H\cap K}$. 

   Claim that if $x=hk$, then
   \begin{equation*}
   f^{-1}(x)=\{(hd,d^{-1}k):d\in H\cap K\}
   \end{equation*}
   #+END_proof

   #+ATTR_LATEX: :options [Second Isomorphism Theorem]
   #+BEGIN_theorem
   If $H\tril G, K\le G$, then $HK\le G,H\cap K\tril G$ and
   \begin{equation*}
   K/(H\cap K)\cong HK/H
   \end{equation*}
   #+END_theorem

   #+BEGIN_proof
   $hkH=kk^{-1}hkH=kh'H=kH$
   #+END_proof

   #+ATTR_LATEX: :options [Third Isomorphism Theorem]
   #+BEGIN_theorem
   If $H,K\tril G$ with $K\le H$, then $H/K\tril G/K$ and
   \begin{equation*}
   (G/K)/(H/K)\cong G/H
   \end{equation*}
   #+END_theorem

   #+ATTR_LATEX: :options [Correspondence Theorem]
   #+BEGIN_theorem
   If $K\tril G, \pi:G\to G/K$ is the natural map, then
   \begin{equation*}
   S\mapsto \pi(S)=S/K
   \end{equation*}
   is a bijection between $Sub(G;K)$, the family of all those subgroups $S$ of
   $G$ that contain $K$, and $Sub(G/K)$, the family of all the subgroups of
   $G/K$. If we denote $S/K$ by $S^*$, then
   1. $T\le S\le G$ if and only if $T^*\le S^*$, in which case $[S:T]=[S^*:T^*]$
   2. $T\tril S$ if and only if $T^*\tril S^*$, in which case $S/T\cong S^*/T^*$
   #+END_theorem

   \begin{center}
   \begin{tikzcd}
   G \arrow[d,dash] \arrow[rd]&\\
   S \arrow[d,dash] \arrow[rd] & G/K \arrow[d,dash]\\
   T \arrow[d,dash] \arrow[rd] & S/K=S^* \arrow[d,dash]\\
   K  \arrow[rd] & T/K=T^* \arrow[d,dash]\\
   & \{1\}
   \end{tikzcd}
   \end{center}

   #+BEGIN_proof
   Use $\pi^{-1}\pi=1$ and $\pi\pi^{-1}=1$ to prove injectivity and surjectivity
   respectively. 

   For $[S:T]=[S^*:T^*]$, show there is a bijection between the family of all
   cosets of the form $sT$ and the family of all the cosets of the form
   $s^*T^*$.

   injective:
   \begin{align*}
   \pi(m)T^*=\pi(n)T^*&\Leftrightarrow \pi(m)\pi(n)^{-1}\in T^*\\
   &\Leftrightarrow mn^{-1}K\in T/K\\
   &\Rightarrow mn^{-1}t^{-1}\in K\\
   &\Rightarrow mn^{-1}=tk\in T\\
   &\Leftrightarrow mT=nT\\
   \end{align*}
   
   surjective:
   
   
   If $G$ is finite, then
   \begin{align*}
   [S^*:T^*]&=\abs{S^*}/\abs{T^*}\\
   &=\abs{S/K}/\abs{T/K}\\
   &=(\abs{S}/\abs{K})/(\abs{T}/\abs{K})\\
   &=\abs{S}/\abs{T}\\
   &=[S:T]
   \end{align*}

   If $T\tril S$, by third isomorphism theorem, $T/S\cong (T/K)/(S/K)=T^*/S^*$

   If $T^*\tril S^*$, 
   \begin{equation*}
   \pi(sts^{-1})\in \pi(s)T^*\pi(s)^{-1}=T^*
   \end{equation*}
   so that $sts^{-1}\in \pi^{-1}(T^*)=T$
   #+END_proof
   

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop2.78
   If $G$ is a finite abelian group and $d$ is a divisor of $\abs{G}$, then $G$
   contains a subgroup of order $d$
   #+END_proposition

   #+BEGIN_proof
   Abelian group's subgroup is normal and hence we can build quotient groups.
   p90 for proof. Use the correspondence theorem
   #+END_proof
   
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $H$ and $K$ are grops, then their *direct product*, denoted by 
   $H\times K$ 
   , is the set of all ordered pairs $(h,k)$ with the operation
   \begin{equation*}
   (h,k)(h',k')=(hh',kk')
   \end{equation*}
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let $G$ and $G'$ be groups and $K\tril G, K'\tril G'$. Then $K\times K'\tril
   G\times G'$ and
   \begin{equation*}
   (G\times G')/(K\times K')\cong (G/K)\times(G'/K')
   \end{equation*}
   #+END_proposition

   #+BEGIN_proof
   
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $G$ is a group containing normal subgroups $H$ and $K$ and $H\cap K=\{1\}$
   and $HK=G$, then $G\cong H\times K$
   #+END_proposition

   #+BEGIN_proof
   Note $\abs{HK}\abs{H\cap K}=\abs{H}\abs{K}$. Consider $\varphi:G\to H\times
   K$. Show it's homo and bijective.
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm2.81
   If $m,n$ are relatively prime, then
   \begin{equation*}
   \I_{mn}\cong\I_m\times\I_n
   \end{equation*}
   #+END_theorem

   #+BEGIN_proof
   \begin{align*}
   f:&\Z\to\I_m\times\I_n\\
   &a\mapsto([a]_m,[a]_n)
   \end{align*}
   is a homo.
   $\Z/\la mn\ra\cong\I_m\times\I_n$
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let $G$ be a group, and $a,b\in G$ be commuting elements of orders $m,n$. If
   $(m,n)=1$, then $ab$ has order $mn$
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   If $(m,n)=1$, then $\phi(mn)=\phi(m)\phi(n)$
   #+END_corollary

   #+BEGIN_proof
   Theorem ref:thm2.81 shows that $f:\I_{mn}\cong\I_m\times\I_n$. The result will
   follow if we prove that $f(U(\I_{mn}))=U(\I_m)\times U(\I_n)$, for then
   \begin{align*}
   \phi(mn)&=\abs{U(\I_{mn})}=\abs{f(U(\I_{mn}))}\\
   &=\abs{U(\I_m)\times U(\I_n)}=\abs{U(\I_m)}\cdot\abs{U(\I_n)}
   \end{align*}
   If $[a]\in U(\I_{mn})$, then $[a][b]=[1]$ for some $[b]\in\I_{mn}$ and
   \begin{align*}
   f([ab])&=([ab]_m,[ab]_n)=([a]_m[b]_m,[a]_n[b]_n)\\
   &=([a]_m,[a]_n)([b]_m,[b]_n)=([1]_m,[1]_n)
   \end{align*}
   Hence $f([a])=([a]_m,[a]_n)\in U(\I_m)\times U(\I_n)$

   For the reverse inclusion, if $f([c])=([c]_m,[c]_n)\in U(\I_m)\times
   U(\I_n)$, then we must show that $[c]\in U(\I_{mn})$. There is $[d]_m\in\I_m$
   with $[c]_m[d]_m=[1]_m$, and there is $[e]_n\I_n$ with $[c]_n[e]_n=[1]_n$.
   Since $f$ is surjective, there is $b\in\Z$ with
   $([b]_m,[b]_n)=([d]_m,[e]_n)$, so that
   \begin{equation*}
   f([1])=([1]_m,[1]_n)=([c]_m[b]_m,[c]_n[b]_n)=f([c][b])
   \end{equation*}
   Since $f$ is an injection, $[1]=[c][b]$ and $[c]\in U(\I_{mn})$
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   1. If $p$ is a prime, then $\phi(p^e)=p^e-p^{e-1}=p^e(1-\frac{1}{p})$
   2. If $n=p_1^{e_1}\dots p_t^{e_t}$, then
      \begin{equation*}
      \phi(n)=n(1-\frac{1}{p_1})\dots(1-\frac{1}{p_t})
      \end{equation*}
   #+END_corollary

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   A cyclic group of order $n$ has a unique subgroup of order $d$, for each
   divisor $d$ of $n$, and this subgroup is cyclic.
   #+END_lemma

   Define an equivalence relation on a group $G$ by $x\equiv y$ if $\la x\ra=\la
   y\ra$. Denote the equivalence class containing $x$ by $\gen(C)$, where $C=\la
   x\ra$. Equivalence classes form a partition and we get
   \begin{equation*}
   G=\displaystyle\prod_{C}\gen(C)
   \end{equation*}
   where $C$ ranges over all cyclic subgroups of $G$. Note $\abs{\gen(C)}=\phi(n)$

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm2.86
   A group $G$ of order $n$ is cyclic if and only if for each divisor $d$ of
   $n$, there is at most one cyclic subgroup of order $d$
   #+END_theorem

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   If $G$ is an abelian group of order $n$ having at most one cyclic subgroup of
   order $p$ for each prime divisor $p$ of $n$, then $G$ is cyclic
   #+END_theorem

   Exercise:
   * 2.71 Suppose $H\le G, \abs{H}=\abs{K}$. Since $\abs{H}=[H:K]\abs{K}$,
     $[H:K]=1$. Hence $H=K$
   * 2.67 1. $\inn(S_3)\cong S_3/Z(S_3)\cong S_3$ and $\abs{\aut(S_3)}\le 6$.
     Hence $\aut(S_3)=\inn(S_3)$

     
#+BEGIN_exercise
label:ex2.69
Prove that if $G$ is a group for which $G/Z(G)$ is cyclic, then $G$ is abelian
#+END_exercise
#+BEGIN_proof
Suppose $G/Z(G)=\la a\ra$, let $g=a^kz^{-1},g'=a^{k'}z'^{-1}$, then 
$gg'=a^kz^{-1}z^{k'}z'^{-1}=a^{k+k'}z'^{-1}z^{-1}=g'g$. Hence $G$ is abelian.
#+END_proof
** Group Actions
   #+ATTR_LATEX: :options [Cayley]
   #+BEGIN_theorem
   Every group $G$ is isomorphic to a subgroup of the symmetric group $S_G$. In
   particular, if $\abs{G}=n$, then $G$ is isomorphic to a subgroup of $S_n$
   #+END_theorem

   #+BEGIN_proof
   For each $a\in G$, define $\tau_a(x)=ax$ for every $x\in G$. $\tau_a$ is a
   bijection for its inverse is $\tau_{a^{-1}}$
   \begin{equation*}
   \tau_a\tau_{a^{-1}}=\tau_1=\tau_{a^{-1}}\tau_a
   \end{equation*}
   #+END_proof

   #+ATTR_LATEX: :options [Representation on Cosets]
   #+BEGIN_theorem
   Let $G$ be a group and $H\le G$ having finite index $n$. Then there exists a
   homomorphism $\varphi:G\to S_n$ with $\ker\varphi\le H$
   #+END_theorem
  
#+BEGIN_proof
We still denote the family of all the cosets of $H$ in $G$ by $G/H$

For each $a\in G$, define "translation" $\tau_a:G/H\to G/H$ by $\tau_a(xH)=axH$
for every $x\in G$. For $a,b\in G$
\begin{equation*}
(\tau_a\circ\tau_b)(xH)=a(bxH)=(ab)xH
\end{equation*}
so that 
\begin{equation*}
\tau_a\tau_b=\tau_{ab}
\end{equation*}
It follows that each $\tau_a$ is a bijection and so $\tau_a\in S_{G/H}$. Define
$\varphi:G\to S_{G/H}$ by $\varphi(a)=\tau_a$. Rewriting
\begin{equation*}
\varphi(a)\varphi(b)=\tau_a\tau_b=\tau_{ab}=\varphi(ab)
\end{equation*}
so that \varphi is a homomorphism. Finally if $a\in\ker\varphi$, then
$\varphi(a)=1_{G/H}$, so that $\tau_a(xH)=xH$, in particular, when $x=1$, this
gives $aH=H$ and $a\in H$. And $S_{G/H}\cong S_n$
#+END_proof

   When $H=\{1\}$, this is the Cayley theorem.

   Four-group $\bV=\{1,(1 2)(3 4),(1 3)(2 4), (1 4)(2 3)\}$
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Every group $G$ of order 4 is isomorphic to either $\I_4$ or the four-group
   $\bV$. And $\I_4\not\cong\bV$
   #+END_proposition

   #+BEGIN_proof
   By lagrange's theorem, every element in $G$ other than 1 has order 2 or 4. If
   4, then $G$ is cyclic.

   Suppose $x,y\neq 1$, then $xy\neq x,y$. Hence $G=\{1,x,y,xy\}$.
   #+END_proof
   

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $G$ is a group of order 6, then $G$ is isomorphic to either $\I_6$ or
   $S_3$. Moreover $\I_6\not\cong S_3$
   #+END_proposition

   #+BEGIN_proof

   If $G$ is not cyclic, since $\abs{G}$ is even, it has some elements having
   order 2, say $t$ by exercise ref:ex2.27

   If $G$ is abelian. Suppose it has another different element $a$ with order 2.
   Then $H=\{1,a,t,at\}$ is a subgroup which contradict. Hence it must contain
   an element $b$ of order 3. Then $bt$ has order 6 and $G$ is cyclic.

   If $G$ is not abelian. If $G$ doesn't have elements of order 3, then it's
   abelian. Hence $G$ has an element $s$ of order 3.

   Now $\abs{\la s\ra}=3$, so $[G:\la s\ra]=\abs{G}/\abs{\la s\ra}=2$ and $\la
   s\ra$ is normal. 
   Since $t=t^{-1}$, $tst\in\la s\ra$. If $tst=s^0=1$, $s=1$.
   If $tst=s$, $\abs{\la st\ra}=6$. Therefore $tst=s^2=s^{-1}$.

   Let $H=\la t\ra$, $\varphi:G\to S_{G/\la t\ra}$ given by
   \begin{equation*}
   \varphi(g):x\la t\ra\mapsto gx\la t\ra
   \end{equation*}
   By representation on cosets, $\ker\varphi\le\la t\ra$. Hence
   $\ker\varphi=\{1\}$ or $\ker\varphi=\la t\ra$. Since
   \begin{equation*}
   \varphi(t)=
   \begin{pmatrix}
   \la t\ra&s\la t\ra&s^2\la t\ra\\
   t\la t\ra&ts\la t\ra&ts^2\la t \ra
   \end{pmatrix}
   \end{equation*}
   If $\varphi(t)$ is the identity permutation, then $ts\la t\ra=s\la t\ra$, so
   that $s^{-1}ts\in\la t\ra=\{1,t\}$. But now $s^{-1}ts=t$. Therefore
   $t\not\in\ker\varphi$ and $\ker\varphi=\{1\}$. Therefore $\varphi$ is
   injective. Because $\abs{G}=\abs{S_3}$, $G\cong S_3$
   #+END_proof


   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $X$ is a set and $G$ is a group, then $G$ *acts* on $X$ if there is a
   function $G\times X\to X$, denoted by $(g,x)\to gx$ s.t.
   1. (gh)x=g(hx) for all $g,h\in G$ and $x\in X$
   2. $1x=x$ for all $x\in X$

      
   $X$ is a *\(G\)-set* if $G$ acts on $X$
   #+END_definition

   If a group $G$ acts on a set $X$, then fixing the first variable, say $g$,
   gives a function $\alpha_g:X\to X$, namely, $\alpha_g:x\mapsto gx$. This
   function is a permutation of $X$, for its inverse is $\alpha_{g^{-1}}$
   \begin{equation*}
   \alpha_g\alpha_{g^{-1}}=1=\alpha_{g^{-1}}\alpha_g
   \end{equation*}
   If's easy to see that $\alpha:G\to S_X$ defined by $\alpha:g\mapsto\alpha_g$
   is a homomorphism. Conversely, given any homomorphism $\varphi:G\to S_X$,
   define $gx=\varphi(g)(x)$. Thus an action of a group $G$ on a set $X$ is
   another way of viewing a homomorphism.

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $G$ acts on $X$ and $x\in X$, then the *orbit* of $x$, denoted by
   $\calo(x)$, is the subset of $X$
   \begin{equation*}
   \calo(x)=\{gx:g\in G\}\subseteq X
   \end{equation*}
   the *stabilizer* of $x$, denoted by $G_x$, is the subgroup
   \begin{equation*}
   G_x=\{g\in G:gx=x\}\le G
   \end{equation*}
   #+END_definition
   
   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   1. Caylay's theorem says that $G$ acts on itself by translation:
      $\tau_g:a\mapsto ga$.  We say $G$ acts *transitively* on $X$ if there is
      only one orbit. 
   2. When $G$ acts on $G/H$ by translation $\tau_g:aH\mapsto gaH$, then the
      orbit $\calo(aH)=G/H$
   3. When a group $G$ acts on itself by conjugation, then the orbit  $\calo(x)$
      is 
      \begin{equation*}
      \{y\in G:y=axa^{-1}\text{ for some }a\in G\}
      \end{equation*}
      in this case, $\calo(x)$ is called the *conjugacy class* of $x$, and it is
      commonly denoted by $x^G$.

      *centralizer* $C_G(x)=\{g\in G:gxg^{-1}=x\}$
   4. Let $X=\{1,2,\dots,n\}$, let $\alpha\in S_n$ and regard the cyclic group
      $G=\la\alpha\ra$ as acting on $X$. If $i\in X$, then
      \begin{equation*}
      \calo(i)=\{\alpha^k(i):k\in\Z\}
      \end{equation*}
      Let the complete factorization of \alpha be $\alpha=\beta_1\dots\beta_{t(\alpha)}$,
      and let $i=i_1$ be moved by \alpha. If the cycle involving $i_1$ is
      $\beta_j=(i_1 i_2 \dots i_r)$,
      \begin{equation*}
      \calo(i)=\{i_1,\dots,i_r\}
      \end{equation*}
      where $i=i_1$. It follows that $\abs{\calo(i)}=r$. The stabilizer $G_l$ of
      a number $l$ is $G$ if \alpha fixes $l$
   #+END_examplle
   

   *Normalizer*
   \begin{equation*}
   N_G(H)=\{g\in G:gHg^{-1}=H\}
   \end{equation*}

   
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $G$ acts on a set $X$, then $X$ is the disjoint union of the orbits. If
   $X$ is finite, then
   \begin{equation*}
   \abs{X}=\displaystyle\sum_i\abs{\calo(x_i)}
   \end{equation*}
   where $x_i$ is chosen from each orbit
   #+END_proposition

   #+BEGIN_proof
   $x\equiv y\Leftrightarrow$ there exists $g\in G$ with $y=gx$ is an
   equivalence relation
   #+END_proof


   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm2.98
   If $G$ acts on a set $X$ and $x\in X$ then
   \begin{equation*}
   \abs{\calo(x)}=[G:G_x]
   \end{equation*}
   #+END_theorem

   #+BEGIN_proof
   Let $G/G_x$ denote the family of cosets. Construct a bijection
   $\varphi:G/G_x\to \calo(x)$
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
label:cor2.99
   If a finite group $G$ acts on a set X, then the number of elements in any
   orbit is a divisor of $\abs{G}$. 
   #+END_corollary

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   label:cor2.100
   If $x$ lies in a finite group $G$, then the number of conjugates of $x$ is
   the index of its centralizer
   \begin{equation*}
   \abs{x^G}=[G:C_G(x)]
   \end{equation*}
   and hence it's a divisor of $G$
   #+END_corollary

   #+BEGIN_proof
   $x^G$ is the orbit, $C_G(x)$ is the stabilizer
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $H$ is a subgroup of a finite group $G$, then the number of conjugates of
   $H$ in $G$ is $[G:N_G(H)]$
   #+END_proposition

   #+BEGIN_proof
   Similar to theorem ref:thm2.98
   #+END_proof

   #+ATTR_LATEX: :options [Cauchy]
   #+BEGIN_theorem
   label:thmCauchy
   If $G$ is a finite group whose order is divisible by a prime $p$, then $G$
   contains an element of order $p$
   #+END_theorem

   #+BEGIN_proof
   Prove by induction on $m\ge 1$, where $\abs{G}=mp$. If $m=1$, it's obvious.

   If $x\in G$ , then $\abs{x^G}=[G:C_G(x)]$. If $x\not\in Z(G)$, then $x^G$
   has more than one element, so $\abs{C_G(x)}<\abs{G}$. If $p\mid \abs{C_G(x)}$, by
   inductive hypothesis, we are done. Else if $p\nmid \abs{C_G(x)}$ for all
   noncentral $x$ and $\abs{G}=[G:C_G(x)]\abs{C_G(x)}$, we have
   \begin{equation*}
   p\mid[G:C_G(x)]
   \end{equation*}
   $Z(G)$ consists of all those elements with $\abs{X^G}=1$, we have
   \begin{equation*}
   \abs{G}=\abs{Z(G)}+\displaystyle\sum_i[G:C_G(x_i)]
   \end{equation*}
   Hence $p\mid\abs{Z(G)}$ and by proposition ref:prop2.78
   #+END_proof
   
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   The *class equation* of a finite group $G$ is
   \begin{equation*}
   \abs{G}=\abs{Z(G)}+\displaystyle\sum_i[G:C_G(x_i)]
   \end{equation*}
   where each $x_i$ is selected from each conjugacy class having more than one element
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $p$ is a prime, then a finite group $G$ is called a *p-group* if
   $\abs{G}=p^n$ for some $n\ge 0$
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm2.103
   If $p$ is a prime and $G$ is a p-group, then $Z(G)\neq\{1\}$
   #+END_theorem

   #+BEGIN_proof
   Consider 
   \begin{equation*}
   \abs{G}=\abs{Z(G)}+\displaystyle\sum_i[G:C_G(x_i)]
   \end{equation*}
   #+END_proof

#+ATTR_LATEX: :options []
#+BEGIN_corollary
If $p$ is a prime, then every group $G$ of order $p^2$ is abelian
#+END_corollary
#+BEGIN_proof
If $G$ is not abelian, then $Z(G)$ has order $p$. The center is always normal, and
so $G/Z(G)$ is defined; it has order $p$ and is cyclic by Lagrange's theorem. 
This contradicts Exercise ref:ex2.69
#+END_proof

#+ATTR_LATEX: :options []
#+BEGIN_examplle
Cauchy's theorem and Fermat's theorem are special cases of some common theorem.

If $G$ is a finite group and $p$ is a prime, define
\begin{equation*}
X=\{(a_0,a_1,\dots,a_{p-1})\in G^p:a_0a_1\dots a_{p-1}=1\}
\end{equation*}
Note that $\abs{X}=\abs{G}^{p-1}$, for having chosen the last $p-1$ entries
arbitrarily, the 0th entry must equal $(a_1a_2\dots a_{p-1})^{-1}$. Introduce an
action of $\I_p$ on $X$ by defining, for $0\le i\le p-1$,
\begin{equation*}
[i](a_0,\dots,a_{p-1})=(a_{i+1},\dots,a_{p-1},a_0,\dots,a_i)
\end{equation*}
The product of the new \(p\)-tuple is a conjugate of $a_0a_1\dots a_{p-1}$
\begin{equation*}
a_{i+1}\dots a_{p-1}a_0\dots a_{i}=(a_0\dots a_i)^{-1}(a_0\dots a_{p-1})
(a_0\dots a_i)
\end{equation*}
This conjugate is $1$ for $g^{-1}1g=1$, and so $[i](a_0,\dots,a_{p-1})\in X$. By
Corollary ref:cor2.99, the size of every orbit of $X$ is a divisor of
$\abs{\I_p}=p$. Now orbits with just one element consists of a \(p\)-tuple all
of whose entries $a_i$ are equal, for all cyclic permutations of the \(p\)-tuple
are the same. In other words, such an orbit corresponds to an element $a\in G$
with $a^p=1$. Clearly $(1,1,\dots,1)$ is such an orbit; if it were the only such
, then we would have
\begin{equation*}
\abs{G}^{p-1}=\abs{X}=1+kp
\end{equation*}
That is, $\abs{G}^{p-1}\equiv 1\mod p$. If $p$ is a divisor of $\abs{G}$, then
we have a contradiction and thus proved Cauchy's theorem.
#+END_examplle

#+ATTR_LATEX: :options []
#+BEGIN_proposition
If $G$ is a group of order $\abs{G}=p^e$ then $G$ has a normal subgroup of order
$p^k$ for every $k\le e$
#+END_proposition

#+BEGIN_proof
We prove the result by induction on $e\ge 0$.

By Theorem ref:thm2.103, $Z(G)\neq\{1\}$. Let $Z\le Z(G)$ be a subgroup of order
$p$ and $Z$ is normal. If $k\le e$, then $p^{k-1}\le p^{e-1}=\abs{G/Z}$. By
induction, $G/Z$ has a normal subgroup $H^*$ of order $p^{k-1}$. The
correspondence theorem says there is a subgroup $H$ of $G$ containing $Z$ with
$H^*=H/Z$; moreover $H^*\triangleleft G/Z$  implies $H\triangleleft G$. But
$\abs{H/Z}=p^{k-1}$ implies $\abs{H}=p^k$ as desired.
#+END_proof

#+ATTR_LATEX: :options []
#+BEGIN_definition
A group $G\neq\{1\}$ is called *simple* if $G$ has no normal subgroups other than
$\{1\}$ and  $G$ itself.
#+END_definition

#+ATTR_LATEX: :options []
#+BEGIN_proposition
An abelian group $G$ is simple if and only if it is finite and of prime order
#+END_proposition
#+BEGIN_proof
Assume $G$ is simple. Since $G$ is abelian, every subgroup is normal, and so $G$
has no subgroups otherthan $\{1\}$ and $G$. Choose $x\in G$ with $x\neq 1$.
Since $\la x\ra\le G$, we have $\la x\ra =G$. If $x$ has infinite order, then
all the powers of $x$ are distinct, and so $\la x^2\ra<\la x\ra$ is a forbidden
subgroup of $\la x\ra$, a contradiction. Therefore every $x\in G$ has finite
order. If $x$ has order $m$ and if $m$ is composite, say $m=kl$, then 
$\la x^k\ra$ is a proper subgroup of $\la x\ra$, a contradiction. Therefore
$G=\la x\ra$ has prime order.
#+END_proof

Suppose that an element $x\in G$ has $k$ conjugates, that is 
\begin{equation*}
\abs{x^G}=\abs{\{gxg^{-1}:g\in G\}}=k
\end{equation*}
If there is a subgroup $H\le G$ with $x\in H\le G$, how many conjugates does $x
$ have in $H$?

Since
\begin{equation*}
x^H=\{hxh^{-1}:h\in H\}\subseteq x^G
\end{equation*}
we have $\abs{x^H}\le\abs{x^G}$. It is possible that there is a strict
inequality $\abs{x^H}<\abs{x^G}$. For example, take $G=S_3,x=(1\; 2)$, and
$H=\la x\ra$. Now let us consider this question, in particular, for
$G=S_5,x=(1\;2\;3), H=A_5$

#+ATTR_LATEX: :options []
#+BEGIN_lemma
All 3-cycles are conjugate in $A_5$
#+END_lemma

#+BEGIN_proof
Let $G=S_5,\alpha=(1\; 2\;3), H=A_5$. We know that $\abs{\alpha^{S_5}}=20$, for there
are 20 3-cycles in $S_5$. Therefore, $20=\abs{S_5}/\abs{C_{S_5}(\alpha)}$ by
Corollary ref:cor2.100 , so that $\abs{C_{S_5}(\alpha)}=6$. Here they are
\begin{equation*}
(1),\;(1\;2\;3),\;(1\;3\;2),\;(4\;5),\;(4\;5)(1\;2\;3),\;(4\;5\;)(1\;3\;2)
\end{equation*}
The last there of these are odd permutations, so that $\abs{C_{A_5}(\alpha)}=3$. We
conclude that
\begin{equation*}
\abs{\alpha^{A_5}}=\abs{A_5}/\abs{C_{A_5}(\alpha)}=20
\end{equation*}
that is all 3-cycles are conjugate to \alpha in $A_5$
#+END_proof

#+ATTR_LATEX: :options []
#+BEGIN_lemma
label:lemma2.109
If $n\ge 3$, every element in $A_n$ is a 3-cycle or a product of 3-cycles
#+END_lemma
#+BEGIN_proof
Since each $\beta$ equals $\tau_1\dots\tau_{2q}$
#+END_proof

#+ATTR_LATEX: :options []
#+BEGIN_theorem
$A_5$ is a simple group
#+END_theorem
#+BEGIN_proof
If $H\triangleleft A_5$ and $H\neq\{(1)\}$. Now if $H$ contains a 3-cycle, then
normality forces $H$ to contain all its conjugates. Therefore it suffices to
prove that $H$ contains 3-cycle.

Since $\sigma\in H$, we may assume, after a harmless relabeling, that either
$\sigma=(1\;\;2\;3),\sigma=(1\;2)(3\;4)$ or $\sigma=(1\;2\;3\;4\;5x)$

If $\sigma=(1\;2)(3\;4)$, define $\tau=(1\;2)(3\;5)$. Now
$(3\;5\;4)=(\tau\sigma\tau^{-1})\sigma^{-1}\in H$. If $\sigma=(1\;2\;3\;4\;5)$,
define $\rho=(1\;3\;2)$ and $(1\;3\;4)=\rho\sigma\rho^{-1}\sigma^{-1}\in H$
#+END_proof

$A_4$ is not simple for $\bV\triangleleft A_4$.

#+ATTR_LATEX: :options []
#+BEGIN_lemma
$A_6$ is a simple group
#+END_lemma

#+BEGIN_proof
Let $\{1\}\neq H\triangleleft A_6$; we must show that $H=A_6$. Assume that there
is some $\alpha\in H$ with $\alpha\neq (1)$ that fixes some $i$, where $1\le
i\le 6$. Define
\begin{equation*}
F=\{\sigma\in A_6:\sigma(i)=i\}
\end{equation*}
Note that $\alpha\in H\cap F$, so that $H\cap F\neq\{(1)\}$. The second
isomorphism theorem gives $H\cap F\triangleleft F$. But $F$ is simple for
$F\cong A_5$, we have $H\cap F=F$: that is $F\le H$. It follows that $H$
contains a 3-cycle, and so $H=A_6$ by Exercise ref:ex2.91.

If there is no $\alpha\in H$ with $\alpha\neq\{1\}$ that fixes some $i$ with
$1\le i\le 6$. If we consider the cycle structures of permutations in $A_6$,
however, any such \alpha must have cycle structure $(1\;2)(3\;4\;5\;6)$ or
$(1\;2\;3)(4\;5\;6)$. In the first case $\alpha^2\in H$, $\alpha^2\in H$
fixes 1. In the second case $\alpha(\beta\alpha^{-1}\beta^{-1})$ where
$\beta=(2\;3\;4)$ fixes 1.
#+END_proof

#+ATTR_LATEX: :options []
#+BEGIN_theorem
$A_n$ is a simple group for all $n\ge 5$
#+END_theorem

#+BEGIN_proof
If $H$ is a nontrivial normal subgroup of $A_n$, then we must show that $H=A_n$.
By Exercise ref:ex2.91 it suffices to prove that $H$ contains a 3-cycle. If
$\beta\in H$ is nontrivial, then there exists some $i$ that $\beta$ moves: say,
$\beta(i)=j\neq i$. Choose a 3-cycle $\alpha$ that fixes $i$ and moves $j$. The
permutations \alpha and \beta do not commute. It follows that
$\gamma=(\alpha\beta\alpha^{-1})\beta^{-1}$ is a nontrivial element of $H$. But
$\beta\alpha^{-1}\beta^{-1}$ is a 3-cycle, and so
$\gamma=\alpha(\beta\alpha^{-1}\beta^{-1})$ is a product of two 3-cycles. Hence
\gamma moves at most 6 symbols, say $i_1,\dots,i_6$. Define
\begin{equation*}
F=\{\sigma\in A_n:\sigma\text{ fixes all }i\neq i_1,\dots,i_6\}
\end{equation*}
Now $F\cong A_6$ and $\gamma\in H\cap F$. Hence $H\cap F\triangleleft F$. But
$F$ is simple, and so $H\cap F=F$; that is $F\le H$. Therefore $H$ contains a
3-cycle 
#+END_proof

#+ATTR_LATEX: :options [Burnside's Lemma]
#+BEGIN_theorem
Let $G$ act on a finite set $X$. If $N$ is the number of orbits, then
\begin{equation*}
N=\frac{1}{\abs{G}}\displaystyle\sum_{\tau\in G}Fix(\tau)
\end{equation*}
where $Fix(\tau)$ is the number of $x\in X$ fixed by \tau
#+END_theorem
#+BEGIN_proof
List the elements of $X$ as follows: Choose $x_1\in X$ and then list all the
elements $x_1,\dots,x_r$ in the orbit $\calo(x_1)$; then choose
$x_{r+1}\not\in\calo(x_1)$, and so on until all the elements of $X$ are listed.
Now list the elements $\tau_1,\dots,\tau_n$ of $G$ and form the following array,
where
\begin{equation*}
f_{i,j}=
\begin{cases}
1&\text{ if }\tau_i\text{ fixes }x_j\\
0&\text{ if }\tau_i\text{ moves }x_j
\end{cases}
\end{equation*}
#+ATTR_LATEX: :mode math :align c|cccccc
|          | $x_1$     | $x_2$     | $\dots$ | $x_{r+1}$   | $x_{r+2}$   | $\dots$ |
|----------+-----------+-----------+---------+-------------+-------------+---------|
| $\tau_1$ | $f_{1,1}$ | $f_{1,2}$ | $\dots$ | $f_{1,r+1}$ | $f_{1,r+2}$ | $\dots$ |
| $\vdots$ |           |           |         |             |             |         |
| $\tau_n$ | $f_{n,1}$ | $f_{n,2}$ | $\dots$ | $f_{n,r+1}$ | $f_{n,r+2}$ | $\dots$ |
Now $Fix(\tau_i)$ is the number of 1's in the \(i\)th row. therefore 
$\sum_{\tau\in G}Fix(\tau)$ is the total number of 1's in the array. The number
of 1's in column 1 is $\abs{G_{x_1}}$. By Exercise ref:ex2.99
$\abs{G_{x_1}}=\abs{G_{x_2}}$. By Theorem ref:thm2.98 the number of 1's in the
$r$ columns labels by the $x_i\in\calo(x_i)$ is thus
\begin{equation*}
r\abs{G_{x_1}}=\abs{\calo(x_1)}\cdot\abs{G_{x_1}}=(\abs{G}/
\abs{G_{x_1}})\abs{G_{x_1}}=\abs{G}
\end{equation*}
Therefore
\begin{equation*}
\displaystyle\sum_{\tau\in G}Fix(\tau)=N\abs{G}
\end{equation*}
#+END_proof

We are going to use Burnside's lemma to solve problems of the following sort.
How many striped flags are there having six stripes each of which can be colored
red, white or blue?
#+ATTR_LATEX: :align |c|c|c|c|c|c|
|---+---+---+---+---+---|
| r | w | b | r | w | b |
|---+---+---+---+---+---|

#+ATTR_LATEX: :align |c|c|c|c|c|c|
|---+---+---+---+---+---|
| b | w | r | b | w | r |
|---+---+---+---+---+---|

Let $X$ be the set of all 6-tuples of colors: if $x\in X$, then
\begin{equation*}
x=(c_1,c_2,c_3,c_4,c_5,c_6)
\end{equation*}

Let \tau be the permutation that reserves all the indices:
#+ATTR_LATEX: :mode math :environment pmatrix :math-prefix \tau= :math-suffix =(1\;6)(2\;5)(3\;4)
| 1 | 2 | 3 | 4 | 5 | 6 |
| 6 | 5 | 4 | 3 | 2 | 1 |

(thus $\tau$ turns over each 6-tuple $x$ of colored stripes). The cyclic group
$G=\la\tau\ra$ acts on $X$; since $\abs{G}=2$, the orbit of any 6-tuple $x$
consists of either 1 or 2 elements. Since a flag is unchanged by turning it
over, it is reasonable to identify a flag with an orbit of 6-tuple. For example,
the orbit consisting of the 6-tuples
\begin{equation*}
(r,w,b,r,w,b)\text{ and }(b,w,r,b,w,r)
\end{equation*}
above. The number of flags is thus the number $N$ of orbits; by Burnside's
lemma, $N=\frac{1}{2}[Fix((1))+Fix(\tau)]$. The identity permutation $(1)$ fixes
every $x\in X$, and so $Fix((1))=3^6$. Now \tau fixes a 6-tuple $x$ if it's a
"palindrome". It follows that $Fix(x)=3^3$. The number of flags is thus
\begin{equation*}
N=\frac{1}{2}(3^6+3^3)=378
\end{equation*}

#+ATTR_LATEX: :options []
#+BEGIN_definition
If a group $G$ acts on $X=\{1,\dots,n\}$ and if $\calc$ is a set of $q$ colors,
then $G$ acts on the set $\calc^n$ of all \(n\)-tuples of colors by
\begin{equation*}
\tau(c_1,\dots,c_n)=(c_{\tau1},\dots,c_{\tau n})\text{ for all }\tau\in G
\end{equation*}
An orbit of $(c_1,\dots,c_n)\in\calc^n$ is called a *\((q,G)\)-coloring* of $X$.
#+END_definition

#+ATTR_LATEX: :options []
#+BEGIN_examplle
Color each square in a $4\times 4$ grid red or black.

If $X$ consists of the 16 squares in the grid and if $\calc$ consists of the two
colors red and black, then the cyclic group $G=\la R\ra$ or order 4 acts on $X$,
where $R$ is a clockwise rotation by $\ang{90}$; 

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{|c|c|c|c|}
\hline
1 & 2 & 3 & 4\\
\hline
5 & 6 & 7 & 8\\
\hline
9 & 10 & 11 & 12\\
\hline
13 & 14 & 15 & 16\\
\hline
\end{tabular}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
\begin{tabular}{|c|c|c|c|}
\hline
13 & 9 & 5 & 1\\
\hline
14 & 10 & 6 & 2\\
\hline
15 & 11 & 7 & 3\\
\hline
16 & 12 & 8 & 4\\
\hline
\end{tabular}
\end{subfigure}
\label{fig2.10}
\end{figure}

Figure shows how $R$ acts: the right square is \(R)\)'s action on the left
square. In cycle notation
\begin{align*}
&R=(1,\;4,\;16,\;13)(2,\;8,\;15,\;9)(3,\;12,\;14,\;5)(6,\;7,\;11,\;10)\\
&R^2=(1,\;16)(4,\;13)(2,\;15)(8,\;9)(3,\;14)(12,\;5)(6,\;11)(7,\;10)\\
&R^3=(1,\;13,\;16,\;4)(2,\;9,\;15,\;8)(3,\;5,\;14,\;12)(6,\;10,\;11,\;7)
\end{align*}

By Burnside's lemma, the number of chessboards is
\begin{equation*}
\frac{1}{4}[Fix((1))+Fix(R)+Fix(R^2)+Fix(R^3)]
\end{equation*}

#+END_examplle



#+BEGIN_exercise
Prove that if $p$ is a prime and $G$ is a finite group in which every element
has order a power of $p$, then $G$ is a \(p\)-group. (A possibly infinite group
$G$) is called a *\(p\)-group* if every element in $G$ has order a power of $p$
#+END_exercise
#+BEGIN_proof
By Cauchy's theorem ref:thmCauchy
#+END_proof

#+BEGIN_exercise
label:ex2.91
1. For all $n\ge 5$, prove that all 3-cycles are conjugate in $A_n$
2. Prove that if a normal subgroup $H\triangleleft A_n$ contains a 3-cycle,
   where $n\ge 5$, then $H=A_n$
#+END_exercise
#+BEGIN_proof
1. If $(1\;2\;3)$ and $(i\; j\; k)$ are not disjoint. As Example ref:example2.8
   illustrated, $\alpha\in S_5$ 

   If they are disjoint, simple
2. By lemma ref:lemma2.109
#+END_proof

#+BEGIN_exercise
label:ex2.99
1. Let a group $G$ act on a set $X$, and suppose that $x,y\in X$ lie in the same
   orbit: y=gx for some $g\in G$. Prove that $G_y=gG_xg^{-1}$
2. Let $G$ be a finite group acting on a set $X$; prove that if $$x,y\in X lie
   in the same orbit, then $\abs{G_x}=\abs{G_y}$
#+END_exercise

#+BEGIN_proof
1. If $f\in G_x$, then $gfg^{-1}(y)=gfg^{-1}gx=gx=y$
2. There is a bijection.
#+END_proof
* Commuctative Rings \rom{1}
** First Properties
   [[index:commutative ring]]
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A *commutative ring* \(R\) is a set with two binary operations, addition and
   multiplication s.t.
   1. $R$ is an abelian group under addition
   2. (*commutativity*) \(ab=ba\) for all \(a,b\in R\)
   3. (*associativity*) \(a(bc)=(ab)c\) for every \(a,b,c\in R\)
   4. there is an element \(1\in R\) with \(1a=a\) for every \(a\in R\)
   5. (*distributivity*) \(a(b+c)=ab+ac\) for every \(a,b,c\in R\)
   #+END_definition

   The element 1 in a ring $R$ has several names: it is called *one*, the *unit* of
   \(R\), or the *identity* in \(R\)

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   1. \(\Z,\Q,\R\) and \(\C\) are commutative rings with the usual addition and
      multiplication
   2. Consider the set \(R\) of all real numbers $x$ of the form
      \begin{equation*}
      x=a+b\omega
      \end{equation*}
      where \(a,b\in\Q\) and \(\omega=\sqrt[3]{2}\). \(R\) is closed under ordinary
      addition. However, if \(R\) is closed under multiplication, then
      \(\omega^2\in R\) and there are rationals \(a\) and \(b\) with
      \begin{align*}
      &\omega^2=a+b\omega\\
      &2=a\omega+b\omega^2\\
      &b\omega^2=ab+b^2\omega
      \end{align*}
      Hence \(2-a\omega=ab+b^2\omega\) and so
      \begin{equation*}
      2-ab=(b^2+a)\omega
      \end{equation*}
      A contradiction.
   #+END_examplle

   
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let $R$ be a commutative ring.
   1. \(0\cdot a=0\) for every \(a\in R\)
   2. If \(1=0\) then \(R\) consists of the single element 0. In this case \(R\)
      is called the *zero ring*
   3. If \(-a\) is the additive inverse of \(a\), then \((-1)(-a)=a\)
   4. \((-1)a=-a\) for every \(a\in R\)
   5. If $n\in\N$ and \(n1=0\), then \(na=0\) for all \(a\in R\)
   6. The binomial theorem holds: if \(a,b\in R\), then 
      \begin{equation*}
      (a+b)^n=\displaystyle\sum_{r=0}^n\binom{n}{r}a^rb^{n-r}
      \end{equation*}
   #+END_proposition

   #+BEGIN_proof
   6. [@6] \(\binom{n+1}{r}=\binom{n}{r-1}+\binom{n}{r}\)
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A subset $S$ of a commutative ring $R$ is a *subring* of \(R\) if
   1. \(1\in S\)
   2. if \(a,b\in S\) then \(a-b\in S\)
   3. if \(a,b\in S\), then \(ab\in S\)
   #+END_definition

   *Notation*. The tradition in ring theory is to write \(S\subseteq R\) for a
   subring

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   A subring \(S\) of a commutative ring \(R\) is itself a commutative ring.
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A *domain* (often called an *integral domain*) is a commutative ring \(R\) that
   satisfies two extra axioms: first
   \begin{equation*}
   1\neq 0
   \end{equation*}
   second, the *cancellation law* for multiplication: for all \(a,b,c\in R\)
   \begin{equation*}
   \text{ if } ca=cb\text{ and }c\neq 0,\text{ then }a=b
   \end{equation*}
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   A nonzero commutative ring \(R\) is a domain if and only if the product of
   any two nonzero elements of \(R\) is nonzero
   #+END_proposition
   #+BEGIN_proof
   \(ab=ac\) if and only if \(a(b-c)=0\)
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop3.6
   The commutative ring \(\I_m\) is a domain if and only if \(m\) is a prime
   #+END_proposition
   #+BEGIN_proof
   If \(m=ab\), where \(1<a,b<m\), then \([a],[b]\neq[0]\) yet
   \([a][b]=[m]=[0]\)

   Conversely, if \(m\) is a prime and \([a][b]=[ab]=[0]\), then \(m\mid ab\)
   #+END_proof
   
   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   1. Let \(\calf(\R)\) be the set of all the function \(\R\to \R\) equipped
      with the operations of *point-wise addition* and *point-wise multiplication*:
      Given \(f,g\in\calf(\R)\), define functions \(f+g\) and \(fg\) by
      \begin{equation*}
      f+g:a\mapsto f(a)+f(b)\quad\text{ and }\quad fg:a\mapsto f(a)g(a)
      \end{equation*}
      We claim that \(\calf(\R)\) with these operations is a commutative ring.
      The zero element is the constant function \(z\) with value 0.
      \(\calf(\R)\) is not a domain by
      \begin{equation*}
      f(a)=
      \begin{cases}
      a&\text{ if }a\le 0\\
      0\\
      \end{cases}\e g(a)=
      \begin{cases}
      0&\text{ if }a\le 0\\
      a
      \end{cases}
      \end{equation*}
   #+END_examplle

   [[index:division]]
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Let \(a\) and $b$ be elements of a commutative ring \(R\). Then \(a\) *divides*
   \(b\) *in* \(R\) (or \(a\) is a *divisor* of \(b\) or $b$ is a *multiple* of $a$),
   denoted by $a\mid b$, if there exists an element \(c\in R\) with \(b=ca\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   An element $u$ in a commutative ring $R$ is called a *unit* if \(u\mid 1\) in $R$.
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(R\) be a domain, and let \(a,b\in R\) be nonzero. Then \(a\mid b\) and
   \(b\mid a\) if and only if \(b=ua\) for some unit \(u\in R\)
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If \(a\) is an integer, then \([a]\) is a unit in \(\I_m\) if and only if
   \(a\) and \(m\) are relatively prime. 
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   If $p$ is a prime, then every nonzero $[a]$ in $\I_p$ is a unit.
   #+END_corollary

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $R$ is a commutative ring, then the *group of units* of $R$ is
   \begin{equation*}
   U(R)=\{\text{all units in }R\}
   \end{equation*}
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A *field* $F$ is a commutative ring in which $1\neq0$ and every nonzero element
   $a$ is a unit; that is, there is \(a^{-1}\in F\) with \(a^{-1}a=1\)
   #+END_definition
   A commutative ring $R$ is a field if and only if $U(R)=R^{\times}$, the
   nonzero elements of $R$.

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Every field $F$ is a domain
   #+END_proposition
   #+BEGIN_proof
   $ab=ac,b=a^{-1}ab=a^{-1}(ac)=c$
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   The commutative ring $\I_m$ is a field if and only if $m$ is prime
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm3.13
   If $R$ is a domain then there is a field $F$ containing $R$ as a subring.
   Moreover, $F$ can be chosen so that for each $f\in  F$, there are \(a,b\in
   R\) with 
   \(b\neq 0\) and \(f=ab^{-1}\) 
   #+END_theorem

   #+BEGIN_proof
   Let \(X=\{(a,b)\in R\times R:b\neq 0\}\) and define a relation $\equiv$ on
   $X$ by $(a,b)\equiv(c,d)$ if $ad=bc$. We claim that $\equiv$ is an
   equivalence relation. If $(a,b)\equiv(c,d)$ and $(c,d)\equiv(e,f)$, then
   $ad=bc,cf=de$ and \(adf=b(cf)=bde\), gives $af=be$

   Denote the equivalence class of (a,b) by $[a,b]$, define $F$ as the set of
   all equivalence classes $[a,b]$ and equip $F$ with the following addition and
   multiplication 
   \begin{align*}
   &[a,b]+[c,d]=[ad+bc,bd]\\
   &[a,b][c,d]=[ac,bd]
   \end{align*}
   Show addition and multiplication are well-defined.
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   The field $F$ constructed from $R$ in Theorem ref:thm3.13 is called the
   *fraction field* of $R$, denoted by $\Frac(R)$, and we denote
   $[a,b]\in\Frac(R)$ by $a/b$
   #+END_definition

   Note that $\Frac(\Z)=\Q$
** Polynomials
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $R$ is a commutative ring, then a *sequence* \sigma in $R$ is
   \begin{equation*}
   \sigma=(s_0,s_1,\dots,s_i,\dots)
   \end{equation*}
   the entries \(s_i\in R\) for all \(i\ge 0\) are called the *coefficients* of \sigma
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A sequence \(\sigma=(s_0,\dots,s_i,\dots)\) in a commutative ring $R$ is
   called a *polynomial* if there is some integer \(m\ge 0\) with $s_i=0$ for all
   $i>m$; that is 
   \begin{equation*}
   \sigma=(s_0,\dots,s_m,0,\dots)
   \end{equation*}
   A polynomial has only finitely many nonzero coefficients. The *zero
   polynomial*, denoted by $\sigma=0$
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $\sigma(s_0,\dots,s_n,0,\dots)\neq0$ is a polynomial, we call $s_n$ the
   *leading coefficient* of $\sigma$, we call $n$ the *degree* of \sigma, an we
   denote $n$ by $\deg(\sigma)$ 
   #+END_definition

   *Notation*. If $R$ is a commutative ring, then the set of all polynomials with
   coefficients in $R$ is denoted by $R[x]$


   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop3.14
   If $R$ is a commutative ring, then $R[x]$ is a commutative ring that contains
   $R$ as a subring
   #+END_proposition

   #+BEGIN_proof
   \(\sigma=(s_0,s_1,\dots),\tau=(t_0,t_1,\dots)\)
   \begin{align*}
   &\sigma+\tau=(s_0+t_0,s_1+t_1,\dots)\\
   &\sigma\tau=(c_0,c_1,\dots)
   \end{align*}
   where \(c_k=\sum_{i+j=k}s_it_j=\sum_{i=0}^ks_it_{k-i}\).
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   label:lemma3.15
   Let $R$ be a commutative ring and let \(\sigma,\tau\in R[x]\) be nonzero
   polynomials.
   1. Either \(\sigma\tau=0\) or \(\deg(\sigma\tau)\le\deg(\sigma)+\deg(\tau)\)
   2. If $R$ is a domain, then $\sigma\tau\neq0$ and 
      \begin{equation*}
      \deg(\sigma\tau)=\deg(\sigma)+\deg(\tau)
      \end{equation*}
   3. If $R$ is a domain, then $R[x]$ is a domain
   #+END_lemma

   #+BEGIN_proof
   \(\sigma=(s_0,s_1,\dots),\tau=(t_0,t_1,\dots)\) have degrees $m$ and $n$ respectively.
   1. if $k>m+n$, then each term in \(\sum_is_it_{k-i}\) is 0
   2. Each term in \(\sum_is_it_{m+n-i}\) is 0 with the possible exception of
      $s_mt_n$. Since $R$ is a domain, $s_m\neq 0$ and $t_n\neq0$ imply $s_mt_n\neq0$.
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $R$ is a commutative ring, then $R[x]$ is called the *ring of polynomials
   over $R$*
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Define the element $x\in R[x]$ by
   \begin{equation*}
   x=(0,1,0,0,\dots)
   \end{equation*}
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   1. IF $\sigma=(s_0,\dots)$, then
      \begin{equation*}
      x\sigma=(0,s_0,s_1,\dots)
      \end{equation*}
   2. If $n\ge 1$, then $x^n$ is the polynomial having 0 everywhere except for 1
      in the \(n\)th coordinate
   3. If \(r\in R\), then
      \begin{equation*}
      (r,0,\dots)(s_0,s_1,\dots,s_j,\dots)=(rs_0,rs_1,\dots,rs_j,\dots)
      \end{equation*}
   #+END_lemma

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $\sigma=(s_0,\dots,s_n,0,\dots)$, then
   \begin{equation*}
   \sigma=s_0+s_1x+s_2x^2+\dots+s_nx^n
   \end{equation*}
   where each element \(s\in R\) is identified with the polynomial \((s,0,\dots)\)
   #+END_proposition

   As a customary, we shall write 
   \begin{equation*}
   f(x)=s_0+s_1x+\dots+s_nx^n
   \end{equation*}
   instead of \sigma. $s_0$ is called its *constant term*. If $s_n=1$ , then
   $f(x)$ is called *monic*.

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Polynomials $f(x)=s_0+\dots+s_nx^n$ and $g(x)=t_0+\dots+t_mx^m$ are equal if
   and only if $n=m$ and $s_i=t_i$ for all $i$.
   #+END_corollary

   If $R$ is a commutative ring, each polynomial $f(x)=s_0+\dots+s_nx^n$
   defines a *polynomial function* $f:R\to R$ by evaluation: If \(a\in R\), define
   \(f(a)=s_0+\dots+s_na^n\in R\).


   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Let $k$ be a field. The fraction field of $k[x]$, denoted by $k(x)$, is
   called the *field of rational function* over $k$
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $k$ is a field, then the elements of $k(x)$ have the form $f(x)/g(x)$
   where $f(x),g(x)\in k[x]$ and $g(x)\neq 0$
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $p$ is a prime, then the field of rational functions \(\I_p(x)\) is a n
   infinite field containing \(\I_p\) as a subfield.
   #+END_proposition

   #+BEGIN_proof
   By Lemma ref:lemma3.15 (3), \(\I_p[x]\) is an infinite domain for the powers
   $x^n$ for $n\in\N$ are distinct. Thus its fraction filed \(\I_p(x)\) is an
   infinite field containing \(\I_p[x]\) as a subring. But \(\I_p[x]\) contains
   \(\I_p\) as a subring, by Proposition ref:prop3.14.
   #+END_proof


   $R[x]$ is often called the ring of all *polynomials over $R$ in one variable*.
   If we write $A=R[x]$, then $A[y]$ is called the ring of all 
   *polynomials over $R$ in two variables $x$ and $y$*, and it is denoted by $R[x,y]$.

   #+BEGIN_exercise
   Show that if $R$ is a commutative ring, then $R[x]$ is never a field
   #+END_exercise

   #+BEGIN_proof
   If $R[x]$ is a field, then $x^{-1}\in R[x]$ and $x^{-1}=\sum_ic_ix^i$.
   However
   \begin{equation*}
   \deg(xx^{-1})=\deg(1)=1=\deg(x)+\deg(x^{-1})
   \end{equation*}
   A contradiction.
   #+END_proof

   #+BEGIN_exercise
   label:ex3.22
   Show that the polynomial function defined by $f(x)=x^p-x\in\I_p[x]$ is
   identically zero.
   #+END_exercise

   #+BEGIN_proof
   By Fermat's theorem ref:Fermat, \(a^p\equiv a\mod p\)
   #+END_proof
** Greatest Common Divisors
   #+ATTR_LATEX: :options [Division Algorithm]
   #+BEGIN_theorem
   Assume that $k$ is a field and that \(f(x),g(x)\in k[x]\) with $f(x)\neq 0$.
   Then there are unique polynomials \(q(x),r(x)\in k[x]\) with
   \begin{equation*}
   g(x)=q(x)f(x)+r(x)
   \end{equation*}
   and either $r(x)=0$ or $\deg(r)<\deg(f)$
   #+END_theorem

   #+BEGIN_proof
   We first prove the existence of such $q$ and $r$. If \(f\mid g\), then
   \(g=qf\) for some $q$; define the remainder $r=0$. If \(f\nmid g\), then
   consider all polynomials of the form \(g-qf\) as $q$ varies over $k[x]$. The
   least integer axiom provides a polynomial \(r=g-qf\) having least degree
   among all such polynomials. Since \(g=qf+r\), it suffices to show that
   \(\deg(r)<\deg(f)\). Write \(f(x)=s_nx^n+\dots+s_1x+s_0\) and
   \(r(x)=t^mx^m+\dots t_0\). Now $s_n\neq 0$ implies that $s_n$ is a unit
   because $k$ is a field and so $s_n^{-1}\in k$. If $\deg(r)\ge\deg(f)$, define
   \begin{equation*}
   h(x)=r(x)-t_ms_n^{-1}x^{m-n}f(x)
   \end{equation*}
   that is, if \(\LT(f)=s_nx^n\), where LT abbreviates *leading term*, then
   \begin{equation*}
   h=r-\frac{\LT(r)}{\LT(f)}f
   \end{equation*}
   note that $h=0$ or \(\deg(h)<\deg(r)\). If $h=0$, then \(r=[\LT(r)/\LT(f)]f\)
   and
   \begin{align*}
   g&=qf+r=qf+\frac{\LT(r)}{\LT(f)}f\\
   &=\left[q+\frac{\LT(r)}{\LT(f)}\right]f
   \end{align*}
   contradicting \(f\nmid g\). If \(h\neq0\), then $\deg(h)<\deg(r)$ and
   \begin{equation*}
   g-qf=r=h+\frac{\LT(r)}{\LT(f)}f
   \end{equation*}
   Thus $g-[q+\LT(r)/\LT(f)]f=h$, contradicting $r$ being a polynomial of least
   degree having this form. Therefore $\deg(r)<\deg(f)$

   To prove uniqueness of $q(x)$ and $r(x)$ assume that $g=q'f+r'$, where
   $\deg(r')<\deg(f)$. Then
   \begin{equation*}
   (q-q')f=r'-r
   \end{equation*}
   If $r'\neq r$, then each side has a degree. But
   \(\deg((q-q')f)=\deg(q-q')+\deg(f)\ge\deg(f)\), while
   \(\deg(r'-r)\le\max\{\deg(r'),\deg(r)\}<deg(f)\), a contradiction. Hence
   $r'=r$ and $(q-q')f=0$. As $k[x]$ is a domain and $f\neq 0$, it follows that
   $q-q'=0$ and $q=q'$
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $f(x)$ and $g(x)$ are polynomials in $k[x]$, where $k$ is a field, then
   the polynomials $q(x)$ and $r(x)$ occurring in the division algorithm are
   called the *quotient* and the *remainder* after dividing $g(x)$ by $f(x)$
   #+END_definition
   
   The hypothesis that $k$ is a filed is much too strong: long division can be
   carried out in $R[x]$ for every commutative ring $R$ as long as the leading
   coefficient of $f(x)$ is a unit in $R$; in particular, long division is
   always possible when $f(x)$ is monic.

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let $R$ be a commutative ring and let $f(x)\in R[x]$ be a monic polynomial.
   If $g(x)\in R[x]$, then there exists $q(x),r(x)\in R[x]$ with
   \begin{equation*}
   g(x)=q(x)f(x)+r(x)
   \end{equation*}
   where either $r(x)=0$ or \(\deg(r)<\deg(f)\)
   #+END_corollary

   #+BEGIN_proof
   Note that \(\LT(r)/\LT(f)\in R\) because $f(x)$ is monic
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $f(x)\in k[x]$, where $k$ is a field, then a *root* of $f(x)$ *in $k$* is an
   element $a\in k$ with $f(a)=0$
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   Let $f(x)\in k[x]$, where $k$ is a field, and let $u\in k$. Then there is
   \(q(x)\in k[x]\) with
   \begin{equation*}
   f(x)=q(x)(x-u)+f(u)
   \end{equation*}
   #+END_lemma
   #+BEGIN_proof
   The division algorithm gives
   \begin{equation*}
   f(x)=q(x)(x-u)+r
   \end{equation*}
   Now evaluate
   \begin{equation*}
   f(u)=q(u)(u-u)+r
   \end{equation*}
   and so $r=f(u)$
   #+END_proof
   
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop3.24
   If $f(x)\in k[x]$, where $k$ is a field, then $a$ is a root of $f(x)$ in $k$
   if and only if $x-a$ divides $f(x)$ in $k[x]$
   #+END_proposition

   #+BEGIN_proof
   If $a$ is a root of $f(x)$ in $k$, then $f(a)=0$ and the lemma gives
   $f(x)=q(x)(x-a)$. 
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm3.25
   Let $k$ be a field and let $f(x)\in k[x]$. If $f(x)$ has degree $n$, then
   $f(x)$ has at most $n$ roots in $k$
   #+END_theorem

   #+BEGIN_proof
   We prove the statement by induction on $n\ge 0$. If $n=0$, then $f(x)$ is a
   nonzero constant, and so the number of its roots in $k$ is zero. Now let
   $n>0$. If $f(x)$ has no roots in $k$, then we are done. Otherwise we may
   assume that there is $a\in k$ with $a$ a root of $f(x)$; hence by Proposition
   ref:prop3.24
   \begin{equation*}
   f(x)=q(x)(x-a)
   \end{equation*}
   moreover, \(q(x)\in k[x]\) has degree $n-1$. 
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   Theorem ref:thm3.25 is not true for polynomials with coefficients in an
   arbitrary commutative ring $R$. For example, if $R=\I_8$, then the quadratic
   polynomial $x^2-1$ has 4 roots: $[1],[3],[5],[7]$
   #+END_examplle
   

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Every \(n\)th root of unity in $\C$  is equal to
   \begin{equation*}
   e^{2\pi ik/n}=\cos\left(\frac{2\pi k}{n}\right)+i\sin\left(\frac{2\pi k}{n}\right)
   \end{equation*}
   where $k=0,1,\dots,n-1$
   #+END_corollary

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let $k$ be an infinite field and let $f(x)$ and $g(x)$ be polynomials in
   $k[x]$. If $f(x)$ and $g(x)$ determine the same polynomial function, then
   $f(x)=g(x)$ 
   #+END_corollary
   
   #+BEGIN_proof
   If $f(x)\neq g(x)$, then the polynomial $h(x)=f(x)-g(x)$ is nonzero, so that
   it has some degree, say $n$. Now every element of $k$ is a root of $h(x)$;
   since $k$ is infinite, $h(x)$ has more than $n$ roots, a contradiction.
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   If $k$ is a field and $G$ is a finite subgroup of the multiplicative group
   $k^\times$., then $G$ is cyclic. In particular, if $k$ itself is finite, then
   $k^\times$ is cyclic.
   #+END_theorem

   #+BEGIN_proof
   Let $d$ be a divisor of $\abs{G}$. If there are two subgroups of $G$ of order
   $d$, say $S$ and $T$, then $\abs{S\cup T}>d$. But each $a\in S\cup T$
   satisfies $a^d=1$ and hence it's a root of $x^d-1$, a contradiction. Thus $G$
   is cyclic, by Theorem ref:thm2.86.
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $k$ is a finite field, a generator of the cyclic group $k^\times$ is
   called a *primitive element* of $k$
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $f(x)$ and $g(x)$ are polynomials in $k[x]$, where $k$ is a field, then a
   *common divisor* is a polynomial \(c(x)\in k[x]\) with \(c(x)\mid f(x)\) and
   \(c(x)\mid g(x)\). If $f(x)$ and $g(x)$ in $k[x]$ are not both 0, define their
   *greatest common divisor*, abbreviated gcd, to be the monic common divisor
   having largest degree. If $f(x)=0=g(x)$, define their $\gcd=0$. The gcd of
   $f(x)$ and $g(x)$ is often denoted by $(f,g)$
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   If $k$ is a field and $f(x),g(x)\in k[x]$, then their \(\gcd d(x)\) is a
   nonlinear combination of $f(x)$ and $g(x)$; that is there are \(s(x),t(x)\in
   k[x]\) with
   \begin{equation*}
   d(x)=s(x)f(x)+t(x)g(x)
   \end{equation*}
   #+END_theorem

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let $k$ be a field and let \(f(x),g(x)\in k[x]\). A monic common divisor
   $d(x)$ is the gcd if and only if $d(x)$ is divisible by every common divisor
   #+END_corollary

   [[index:irreducible]]
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   An element $p$ in a domain $R$ is *irreducible* if $p$ is neither 0 nor a unit
   and in any factorization $p=uv$ in $R$, either $u$ or $v$ is a unit. Elements
   \(a,b\in R\) are *associates* if there is a unit \(u\in R\) with $b=ua$
   #+END_definition

   For example, a prime $p$ is irreducible in $\Z$

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $k$ is a field, then a polynomial \(p(x)\in k[x]\) is irreducible if and
   only if \(\deg(p)=n\ge 1\) and there is no factorization in $k[x]$ of the
   form $p(x)=g(x)h(x)$ in which both factors have degree smaller than $n$
   #+END_proposition

   #+BEGIN_proof
   We show fist that $h(x)\in k[x]$ is a unit if and only if \(\deg(h)=0\). If
   $h(x)u(x)=1$, then $\deg(h)+\deg(u)=\deg(1)=0$, we have \(\deg(h)=0\).
   Conversely if $\deg(h)=0$, then $h(x)$ is a nonzero constant; that is, 
   $h\in k$; since $k$ is a field, $h$ has an inverse

   If $p(x)$ is irreducible, then its only factorization are of the form
   $p(x)=g(x)h(x)$ where $g(x)$ or $h(x)$ is a unit; that is, either $\deg(g)=0$
   or $\deg(h)=0$. 

   Conversely, if $p(x)$ is reducible, then it has factorization $p(x)=g(x)h(x)$
   where neither $g(x)$ nor $h(x)$ is a unit;
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   label:cor3.34
   Let $k$ be a field and let $f(x)\in k[x]$ be a quadratic or cubic polynomial.
   Then $f(x)$ is irreducible in $k[x]$ if and only if $f(x)$ does not have a root
   in $k$
   #+END_corollary

   #+BEGIN_proof
   If $f(x)=g(x)h(x)$, then $\deg(f)=\deg(g)+\deg(h)$
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   1. We determine the irreducible polynomials in $\I_2[x]$ of small degree.

      As always, the linear polynomials $x$ and $x+1$ are irreducible

      There are four quadratics: $x^2,x^2+x,x^2+1,x^2+x+1$ 
   #+END_examplle

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   Let $k$ be a field, let $p(x),f(x)\in k[x]$, and let $d(x)=(p,f)$. If $p(x)$
   is a monic irreducible polynomial, then
   \begin{equation*}
   d(x)=
   \begin{cases}
   1&\text{if }p(x)\nmid f(x)\\
   p(x)&\text{if }p(x)\mid f(x)
   \end{cases}
   \end{equation*}
   #+END_lemma

   #+ATTR_LATEX: :options [Euclid's Lemma]
   #+BEGIN_theorem
   Let $k$ be a field and let \(f(x),g(x)\in k[x]\). If $p(x)$ is an irreducible
   polynomial in $k[x]$, and \(p(x)\mid f(x)g(x)\), then either
   \begin{equation*}
   p(x)\mid f(x)\hspace{0.5cm}\text{or}\hspace{0.5cm}p(x)\mid g(x)
   \end{equation*}
   More generally, if \(p(x)\mid f_1(x)\dots f_n(x))\), then \(p(x)\mid f_i(x)\)
   for some $i$
   #+END_theorem

   #+BEGIN_proof
   Assume $p\mid fg$ but that \(p\nmid f\). Since $p$ is irreducible, $(p,f)=1$,
   and so $1=sp+tf$ for some polynomials $s$ and $t$. Therefore
   \begin{equation*}
   g=spg+tfg
   \end{equation*}
   and so \(p\mid g\)
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   Two polynomials \(f(x),g(x)\in k[x]\) where $k$ is a field, are called
   *relatively prime* if their gcd is 1
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let \(f(x),g(x),h(x)\in k[x]\), where $k$ is a field and let $h(x)$ and
   $f(x)$ be relatively prime. If \(h(x)\mid f(x)g(x)\), then \(h(x)\mid g(x)\)
   #+END_corollary

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $k$ is a field, then a rational function \(f(x)/g(x)\in k(x)\) is in
   *lowest terms* if $f(x)$ and $g(x)$ are relatively prime
   #+END_definition


   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $k$ is a field, every nonzero \(f(x)/g(x)\in k(x)\) can be put in lowest terms
   #+END_proposition


   #+ATTR_LATEX: :options [Euclidean Algorithm]
   #+BEGIN_theorem
   If $k$ is a field and \(f(x),g(x)\in k[x]\), then there are algorithms for
   computing \(\gcd(f,g)\) as well as for finding a pair of polynomials $s(x)$
   and $t(x)$ with 
   \begin{equation*}
   (f,g)=s(x)f(x)+t(x)g(x)
   \end{equation*}
   #+END_theorem

   #+BEGIN_proof
   \begin{gather*}
   g=q_1f+r_1\\
   f=q_2r_1+r_2\\
   r_1=q_3r_2+r_3\\
   \vdots\\
   r_{n-4}=q_{n-2}r_{n-3}+r_{n-2}\\
   r_{n-3}=q_{n-1}r_{n-2}+r_{n-1}\\
   r_{n-2}=q_nr_{n-1}+r_n\\
   r_{n-1}=q_{n+1}r_n
   \end{gather*}
   Since the degrees of the remainders are strictly decreasing, this procedure
   must stop after a finite number of steps. The claim is that $d=r_n$ is the
   gcd. If $c$ is any common divisor of $f$ and $g$, then $c\mid r_i$  for every
   $i$. Also
   \begin{align*}
   r_n&=r_{n-2}-q_nr_{n-1}\\
   &=r_{n-2}-q_n(r_{n-3}-q_{n-1}r_{n-2})\\
   &=(1+q_{n-1})r_{n-2}-q_nr_{n-3}\\
   &=(1+q_{n-1})(r_{n-4}-q_{n-2}r_{n-3})-q_nr_{n-3}\\
   &=(1+q_{n-1})r_{n-4}-[(1+q_{n-1})q_{n-2}+q_n]r_{n-3}\\
   &\vdots\\
   &=sf+tg
   \end{align*}
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   Let $k$ be a subfield of a field $K$, so that $k[x]$ is a subring of $K[x]$.
   If \(f(x),g(x)\in k[x]\), then their gcd in $k[x]$ is equal to their gcd in $K[x]$
   #+END_corollary

   #+BEGIN_proof
   The division algorithm in $K[x]$ gives
   \begin{equation*}
   g(x)=Q(x)f(x)+R(x)
   \end{equation*}
   $k[x]$ gives
   \begin{equation*}
   g(x)=q(x)f(x)+r(x)
   \end{equation*}
   and this also holds in $K[x]$. So that uniqueness of quotient and remainder
   gives $Q(x)=q(x),R(x)=r(x)$.
   #+END_proof

   #+ATTR_LATEX: :options [Unique Factorization]
   #+BEGIN_theorem
   If $k$ is a field, then every polynomial \(f(x)\in k[x]\) of degree $\ge1$ is
   a product of a nonzero constant and monic irreducibles. Moreover, if $f(x)$
   has two such factorizations
   \begin{equation*}
   f(x)=ap_1(x)\dots p_m(x)\hspace{0.5cm}\text{and}\hspace{0.5cm}
   f(x)=bq_1(x)\dots q_n(x)
   \end{equation*}
   then \(a=b,m=n\) and the \(q\)'s may be reindexed so that \(q_i=p_i\) for all $i$
   #+END_theorem

   #+BEGIN_proof
   We prove the existence of a factorization for a polynomial $f(x)$ by
   induction on $\deg(f)\ge1$. If \(\deg(f)=1=\), then $f(x)=ax+c=a(x+a^{-1}c)$.
   As every linear polynomial, $x+a^{-1}c$ is irreducible.

   Assume now that $\deg(f)\ge1$. If $f(x)$ is irreducible and its leading
   coefficient is $a$, write $f(x)=a(a^{-1}f(x))$; we are done. If $f(x)$ is not
   irreducible, then $f(x)=g(x)h(x)$, where \(\deg(g)<\deg(f)\) and
   \(\deg(h)<\deg(f)\). By the inductive hypothesis, 
   \(g(x)=bp_1(x)\dots p_m(x)\) and \(h(x)=cq_1(x)\dots q_n(x)\). It follows
   that 
   \begin{equation*}
   f(x)=(bc)p_1(x)\dots p_m(x)q_x(x)\dots q_n(x)
   \end{equation*}

   We now prove by induction on \(M=\max\{m,n\}\ge1\) if there is an
   equation
   \begin{equation*}
   ap_1(x)\dots p_m(x)=bq_1(x)\dots q_n(x)
   \end{equation*}
   where $a$ and $b$ are nonzero constants and the \(p\)'s and \(q\)'s are monic
   irreducibles. For the inductive step, \(p_m(x)\mid q_1(x)\dots q_n(x)\). By
   Euclid's lemma, there is $i$ with \(p_m(x)\mid q_i(x)\). But $q_i(x)$ are
   monic irreducible, so that $q_i(x)=p_m(x)$. Canceling this factor we will use
   inductive hypothesis
   #+END_proof

   Let $k$ be a field and assume that there are $a,r_1,\dots,r_n\in k$ with
   \begin{equation*}
   f(x)=a \displaystyle\prod_{i=1}^n(x-r_i)
   \end{equation*}
   
   If \(r_1,\dots,r_s\) where \(s\le n\) are the distinct roots of $f(x)$, then
   collecting terms gives
   \begin{equation*}
   f(x)=a(x-r_1)^{e_1}\dots (x-r_s)^{e_s}
   \end{equation*}
   where $r_j$ are distinct and $e_j\ge1$. We call $e_j$ the *multiplicity* of the
   root $r_j$. 

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm3.43
   Let \(f(x)=a_0+a_1x+\dots+a_nx^n\in\Z[x]\subseteq\Q[x]\). Every rational root
   $r$ of $f(x)$ has the form $b/c$, where \(b\mid a_0\) and \(c\mid a_n\)
   #+END_theorem

   #+BEGIN_proof
   We may assume that $r=b/c$ is in lowest form. 
   \begin{gather*}
   0=f(b/c)=a_0+a_1(b/c)+\dots+a_n(b/c)^n\\
   0=a_0c^n+a_1bc^{n-1}+\dots+a_nb^n
   \end{gather*}
   Hence \(a_0c^n=b(-a_1c^{n-1}-\dots-a_nb^{n-1})\), that is \(b\mid a_0c^n\).
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A complex number \alpha is called an *algebraic integer* if \alpha is a root of a monic
   \(f(x)\in\Z[x]\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   label:cor3.44
   A rational number $z$ that is an algebraic integer must lie in $\Z$. More
   precisely, if \(f(x)\in\Z[x]\subseteq\Q[x]\) is a monic polynomial, then
   every rational root of $f(x)$ is an integer that divides the constant term
   #+END_corollary

   #+BEGIN_proof
   $a_n=1$ in Theorem ref:thm3.43
   #+END_proof

   For example, consider \(f(x)=x^3+4x^2-2x-1\in\Q[x]\). By Corollary
   ref:cor3.34, this cubic is irreducible if and only if it has no rational
   root. As $f(x)$ is monic, the candidates for rational roots are \(\pm 1\),
   for these are the only divisor of -1 in $\Z$. Thus $f(x)$ has no roots in
   $\Q$ and hence $f(x)$ is irreducible in $\Q[x]$
** Homomorphisms
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $A$ and $R$ are (commutative) rings, a *(ring) homomorphism* is a function
   \(f:A\to R\) s.t.
   1. \(f(1)=1\)
   2. \(f(a+a')=f(a)+f(a')\)
   3. \(f(aa')=f(a)f(a')\)
   #+END_definition


   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   1. Let $R$ be a domain and let \(F=\Frac(R)\). 
      \(R'=\{[a,1]:a\in R\}\subseteq F\), then the function \(f:R\to R'\) given
      by \(f(a)=[a,1]\), is an isomorphism
   2. Complex conjugation \(z=a+ib\mapsto\overline{z}=a-ib\) is an isomorphism
      \(\C\to \C\).
   3. Let $R$ be a commutative ring, and let \(a\in R\). Define the *evaluation
      homomorphism* \(e_a:R[x]\to R\) by \(e_a(f(x))=f(a)\).
   #+END_examplle

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   If \(f:A\to R\) is a ring homomorphism, then for all \(a\in A\)
   1. \(f(a^n)=f(a)^n\)
   2. if \(a\) is a unit, then \(f(a)\) is a unit and \(f(a^{-1})=f(a)^{-1}\)
   3. if \(f:A\to R\) is a ring homomorphism, then
      \begin{equation*}
      f(U(A))\le U(R)
      \end{equation*}
      where \(U(A)\) is the group of units of $A$; if $f$ is an isomorphism,
      then
      \begin{equation*}
      U(A)\cong U(R)
      \end{equation*}
   #+END_lemma

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $R$ and $S$ are commutative rings and \(\varphi:R\to S\) is a ring
   homomorphism, then there is a ring homomorphism \(\varphi^*:R[x]\to S[x]\)
   given by
   \begin{equation*}
   \varphi^*:r_0+r_1x+r_2x^2+\dots\mapsto\varphi(r_0)+\varphi(r_1)x+
   \varphi(r_2)x^2+\dots
   \end{equation*}
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If \(f:A\to R\) is a ring homomorphism, then its *kernel* is
   \begin{equation*}
   \ker f=\{a\in A:f(a)=0\}
   \end{equation*}
   and its *image* is 
   \begin{equation*}
   \im f=\{r\in R:\exists a\in R\e r=f(a)\}
   \end{equation*}
   #+END_definition

   The kernel of a group homomorphism is not merely a subgroup; it is a *normal*
   subgroup. Similarly, the kernel of a ring homomorphism is almost a subring
   (\(1\not\in\ker f\))
   and is closed under multiplication.

   [[index:ideal]]
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   An *ideal* in a commutative ring $R$ is a subset $I$ of $R$ s.t. 
   1. \(0\in I\)
   2. if \(a,b\in I\), then \(a+b\in I\)
   3. if \(a\in I\) and \(r\in R\), then \(ra\in I\)
   #+END_definition

   An ideal \(I\neq R\) is called a *proper ideal*

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   If \(b_1,\dots,b_n\in R\), then the set of all linear combinations
   \begin{equation*}
   I=\{r_1b_1+\dots+r_nb_n:r_i\in R\}
   \end{equation*}
   is an ideal in $R$. We write \(I=(b_1,\dots,b_n)\) in this case and we call
   $I$ the *ideal generated by* \(b_1,\dots,b_n\). In particular, if $n=1$, then
   \begin{equation*}
   I=(b)=\{rb:r\in R\}
   \end{equation*}
   is an ideal in $R$; $(b)$ consists of all the multiplies of $b$ and it is
   called the *principal ideal* generated by $b$. Notice that $R$ and $\{0\}$ are
   always principal ideals: \(R=(1),\{0\}=(0)\)
   #+END_examplle


   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If \(f:A\to R\) is a ring homomorphism, then \(\ker f\) is an ideal in $A$
   and \(\im f\) is a subring of $R$. Moreover, if $A$ and $R$ are not zero rings,
   then \(\ker f\) is a proper ideal.
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   1. If an ideal $I$ in a commutative ring $R$ contains 1, then $I=R$
   2. it follows from 1 that if $R$ is a field, then the only ideals are $\{0\}$
      and $R$
   #+END_examplle

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   A ring homomorphism \(f:A \to R\) is an injection if and only if \(\ker f=\{0\}\)
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   If \(f:k\to R\) is a ring homomorphism, where $k$ is a field and $R$ is not the
   zero ring, then $f$ is an injection
   #+END_corollary

   #+BEGIN_proof
   the only proper ideal in $k$ is $\{0\}$
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   If $k$ is a field, then every ideal $I$ in $k[x]$ is a principal ideal.
   Moreover, if \(I\neq\{0\}\), there is a monic polynomial that generates $I$
   #+END_theorem

   #+BEGIN_proof
   If $k$ is a field, then $k[x]$ is an example of a *euclidean ring*. Follows
   Theorem ref:thm3.60
   #+END_proof

   [[index:principal ideal domain]]
   
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A domain $R$ is a *principal ideal domain* (PID) if every ideal in $R$ is a principal
   ideal. 
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   1. The ring of integers is a PID
   2. Every field is a PID
   3. If $k$ is a field, then the polynomial ring $k[x]$ is a PID
   4. There are rings other than \(\Z\) and \(k[x]\) where $k$ is a field that
      have a division algorithm; they are called *euclidean rings*.
   #+END_examplle

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   Let \(R=\Z[x]\). The set of all polynomials with even constant term is an
   ideal in \(\Z[x]\). We show that \(I\) is not a principal ideal.

   Suppose there is \(d(x)\in\Z[x]\) with \(I=(d(x))\). The constant \(2\in I\),
   so that there is \(f(x)\in\Z[x]\) with \(2=d(x)f(x)\). We have
   \(0=\deg(2)=\deg(d)+\deg(f)\). The candidates for $d(x)$ are $\pm1$ and
   $\pm2$. Suppose $d(x)=\pm2$; since \(x\in I\), there is \(g(x)\in\Z[x]\) with
   \(x=d(x)g(x)=\pm2g(x)\). But every coefficients on the right side is even.
   This contradiction gives \(d(x)=\pm1\). Hence $I=\Z[x]$, another
   contradiction. Therefore $I$ is not a principal ideal.
   #+END_examplle

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   An element \delta in a commutative ring $R$ is a *greatest commmon divisor*, gcd, of
   elements \(\alpha,\beta \in R\) if
   1. \delta is a common divisor of \alpha and \beta 
   2. if \gamma is any common divisor of \alpha and \beta, then \(\gamma\mid\delta\)
   #+END_definition

   #+BEGIN_remark
   Let $R$ be a PID and let \(\pi,\alpha\in R\) with \pi irreducible. A gcd \delta of
   \pi and \alpha is a divisor of \pi. Hence \(\pi=\delta\epsilon\). And
   irreducibility of \pi forces either \delta or \epsilon to be a unit. Now
   \(\alpha=\delta\beta\). If \delta is not a unit, then \epsilon is a unit and so 
   \begin{equation*}
   \alpha=\delta\beta=\pi\epsilon^{-1}\beta
   \end{equation*}
   that is \(\pi\mid\alpha\). We conclude that if \(\pi\nmid\alpha\) then \delta is a
   unit; that is 1 is a gcd of \pi and \alpha
   #+END_remark

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm3.57
   Let $R$ be a PID
   1. Every \(\alpha,\beta\in R\) has a gcd, \delta, which is a linear combination of
      \alpha and \beta
      \begin{equation*}
      \delta=\sigma\alpha+\tau\beta
      \end{equation*}
   2. If an irreducible element \(\pi\in R\) divides a product \(\alpha\beta\), then
      either \(\pi\mid\alpha\) or \(\pi\mid\beta\)
   #+END_theorem
   #+BEGIN_proof
   1. We may assume that at least one of \alpha and \beta is not zero. Consider the set $I$
      of all the linear combinations
      \begin{equation*}
      I=\{\sigma\alpha+\tau\beta:\sigma,\tau\in R\}
      \end{equation*}
      $I$ is an ideal and so there is \(\delta\in I\) with \(I=(\delta)\); we claim
      that \delta is gcd of \alpha and \beta
   2. If \(\pi\nmid\alpha\), then the remark says that 1 is a gcd of \pi and \alpha.
      Thus \(1=\sigma\pi+\tau\alpha\) and so
      \begin{equation*}
      \beta=\sigma\pi\beta+\tau\alpha\beta
      \end{equation*}
      Since \(\pi\mid\alpha\beta\), it follows that \(\pi\mid\beta\)
   #+END_proof
   
   [[index:least common multiple]]
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $f$ and $g$ are elements in a commutative ring $R$, then a *common multiple*
   is an element \(m\in R\) with \(f\mid m\) and \(g\mid m\). If $f$ and $g$ in
   $R$ are not both 0, define their *least common multiple*, abbreviated lcm.
   #+END_definition

   #+BEGIN_exercise
   label:ex3.47
   1. If $A$ and $R$ are domains and \(\varphi:A\to R\) is a ring homomorphism,
      prove that 
      \begin{equation*}
      [a,b]\to [\varphi(a),\varphi(b)]
      \end{equation*}
      is a ring homomorphism \(\Frac(A)\to\Frac(B)\)
   2. Prove that if a field $k$ contains an isomorphic copy of \(\Z\) as a
      subring, then $k$ must contain an isomorphic copy of \(\Q\)
   3. Let $R$ be a domain and let \(\varphi:R\to k\) be an injective ring
      homomorphism, where $k$ is a field. Prove that there exists a unique ring
      homomorphism \(\Phi:\Frac(R)\to k\) extending \varphi ; that is, \(\Phi|R=\varphi\)
   #+END_exercise

   #+BEGIN_proof
   1. 
      \begin{align*}
      f([1,1])&=[1,1]\\
      f([a,b]+[c,d])&=f([ad+bc,bd])=[\varphi(ad+bc),\varphi(bd)]\\
      &=[\varphi(a)\varphi(d)+\varphi(b)\varphi(c),\varphi(b)\varphi(d)]\\
      &=[\varphi(a),\varphi(b)]+[\varphi(c),\varphi(d)]\\
      &=f([a,b])+f([c,d])\\
      f([a,b][c,d])&=f([ac,bd])=[\varphi(ac),\varphi(bd)]=[\varphi(a)\varphi(c),
      \varphi(b)\varphi(d)]\\&=f([a,b])f([c,d])
      \end{align*}
   2. Suppose \(k'\le k\) and \(k'\cong\Z\), then \(\Frac(k')\cong\Frac(\Z)\).
      Obviously.
   3. $k$ is a field and has inverse.
   #+END_proof
   
** Euclidean Rings
   [[index:degree function]] [[index:euclidean ring]]
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A *euclidean ring* is a domain that is equipped with a function
   \begin{equation*}
   \partial:R-\{0\}\to\N
   \end{equation*}
   called a *degree function*, s.t.
   1. \(\partial(f)\le\partial(fg)\) for all \(f,g\in R\) with \(f,g\neq0\)
   2. for all \(f,g\in R\) with \(f\neq0\), there exists \(q,r\in R\) with
      \begin{equation*}
      g=qf+r
      \end{equation*}
      where either \(r=0\) or \(\partial(r)<\partial(f)\)
   #+END_definition

   #+ATTR_LATEX: :options []
   #+BEGIN_examplle
   1. The integers \(\Z\) is a euclidean ring with the degree function
      \(\partial(m)=\abs{m}\). In \(\Z\) we have
      \begin{equation*}
      \partial(mn)=\abs{mn}=\abs{m}\abs{n}=\partial(m)\partial(n)
      \end{equation*}
   2. when $k$ is a field, the domain $k[x]$ is a euclidean ring with degree
      function the usual degree of a nonzero polynomial. In $k[x]$, we have
      \begin{align*}
      \partial(fg)=\deg(fg)=\deg(f)+\deg(g)=\partial(f)+\partial(g)
      \end{align*}
      If a degree function is multiplicative, then \partial is called a *norm*
   3. The Gaussian integers \(\Z[i]\) form a euclidean ring whose degree
      function 
      \begin{equation*}
      \partial(a+bi)=a^2+b^2
      \end{equation*}
      is a norm. One reason to show that \(\Z[i]\) is a euclidean ring is that
      it is a PID, and hence it has unique factorization of its elements of into
      products of irreducibles.

      \partial is a multiplicative degree function for 
      \begin{equation*}
      \partial(\alpha\beta)=\alpha\beta\overline{\alpha\beta}=\alpha\beta\overline{\alpha}
      \overline{\beta}=\alpha\overline{\alpha}\beta\overline{\beta}= \partial(\alpha)\partial(\beta)
      \end{equation*}

      Let us show that \partial satisfies the second desired property. Given
      \(\alpha,\beta\in\Z[i]\) with \(\beta\neq0\), regard \(\alpha/\beta\) as an
      element of \(\C\). Rationalizing the denominator gives
      \(\alpha/\beta=\alpha\overline{\beta}/\beta\overline{\beta}=\alpha
      \overline{\beta}/\partial{\beta}\), so that
      \begin{equation*}
      a/\beta=x+yi
      \end{equation*}
      where \(x,y\in\Q\). Write \(x=a+u\) and \(y=b+v\), where \(a,b\in\Z\)are
      integers closest to $x$ and $y$, respectively; thus
      \(\abs{u},\abs{v}\le1/2\). It follows that
      \begin{equation*}
      \alpha=\beta(a+bi)+\beta(u+vi)
      \end{equation*}
      Notice that \(\beta(u+vi)\in\Z[i]\). Finally we have
      \begin{equation*}
      \partial(\beta(u+vi))=\partial(\beta)\partial(u+vi)<\partial(\beta)
      \end{equation*}
      And so \(\Z[i]\) is a euclidean ring whose degree function is a norm

      Note that quotients and remainders are not unique because of the choice
   #+END_examplle

   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm3.60
   Every euclidean ring $R$ is a PID
   #+END_theorem

   #+BEGIN_proof
   Let $I$ be an ideal in $R$. If $I\neq\{0\}$, by the least integer axiom, the
   set of all degrees of nonzero elements in $I$ has a smallest element, say
   $n$; choose $d\in I$ with \(\partial(d)=n\). Clearly \((d)\subseteq I\). For any
   \(a\in I\), then there are \(q,r\in R\) with \(a=qd+r\), where either $r=0$
   or \(\partial(r)<\partial(a)\). But \(r=a-qd\in I\) and so $d$ having the least degree
   implies that $r=0$. Hence $a=qd\in(d)$.
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   The ring of Gaussian integers $\Z[i]$ is a PID
   #+END_corollary

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   An element $u$ in a domain $R$ is a *universal side divisor* if $u$ is not a unit
   and for every $x\in R$, either \(u\mid x\) or there is a unit \(z\in R\) with \(u\mid(x+z)\)
   #+END_definition


   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $R$ is a euclidean ring but not a field, then $R$ has a universal side divisor
   #+END_proposition

   #+BEGIN_proof
   Define 
   \begin{equation*}
   S=\{\partial(v):v\neq 0\text{ and }v\text{ is not a unit}\}
   \end{equation*}
   where \partial is the degree function on $R$. Since $R$ is not a field, $S$ is a
   nonempty subset of the natural number. By the least integer axiom, $S$ has a
   smallest element, say, $\partial(u)$. We claim that $u$ is a universal side
   divisor. If \(x\in R\), then there are \(q,r\) with \(x=qu+r\).
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   label:prop3.64
   1. Let $R$ be a euclidean ring $R$ that is not a field. If the degree function
      \partial is a norm, then \alpha is a unit if and only if \(\partial(\alpha)=1\)
   2. Let $R$ be a euclidean ring $R$ that is not a field. If the degree function
      \partial is a norm and if \(\partial(a)=p\), where $p$ is a prime, then \alpha is not
      irreducible
   3. The only units in the ring \(\Z[i]\) of Gaussian integers are \(\pm1\) and
      \(\pm i\)
   #+END_proposition

   #+BEGIN_proof
   1. Since \(1^2=1\), we have \(\partial(1)^2=\partial(1)\), so that \(\partial(1)=0\) or
      \(\partial(1)=1\). If \(\partial(1)=0\), then \(\partial(a)=\partial(1a)=0\). But $R$ is not a
      field, and so \partial is not identically zero. We conclude that \(\partial(1)=1\)

      If \(a\in R\) is a unit, then there is \(\beta\in R\) with
      \(\alpha\beta=1\). Therefore \(\partial(\alpha)\partial(\beta)=1\) and hence \(\partial(\alpha)=1\)

      For the converse, we begin by showing that there is no element \(\beta\in
      R\) with 
      \(\partial(\beta)=0\). If such an element exists, the division algorithms gives 
      \(1=q\beta+r\) and so \(\partial(r)=0\). That is \beta is a unit, then \(\partial(\beta)=1\), a
      contradiction

      Assume now that \(\partial(\alpha)=1\). The division algorithm gives
      \begin{equation*}
      \alpha=q\alpha^2+r
      \end{equation*}
      As \(\partial(\alpha^2)=\partial(\alpha)^2=1\), \(r=0\) or \(\partial(r)=0\), which would not occur.
      Hence $r=0$ and \(\alpha=q\alpha^2\). It follows that \(1=q\alpha\), and
      so \alpha is a unit
   2. If on the contrary, \(\alpha=\beta\gamma\), where neither \(\beta\) or
      \gamma is a unit, then \(p=\partial(\alpha)=\partial(\beta)\partial(\gamma)\). 
   3. If \(\alpha=a+bi\in\Z[i]\) is a unit, then \(1=\partial(\alpha)=a^2+b^2\).
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   If $p$ is a prime and \(p\equiv 1\mod 4\), then there is an integer $m$ with
   \begin{equation*}
    m^2\equiv -1\mod p
   \end{equation*}
   #+END_lemma
   #+BEGIN_proof
   If \(G=(\I_p)^\times\) is the multiplicative group of nonzero elements in
   \(\I_p\), then \(\abs{G}=p-1\equiv 0\mod 4\). By Proposition ref:prop2.78,
   $G$ contains a subgroup $S$ of order $4$. By Exercise ref:ex2.36 either $S$
   is cyclic or \(a^2=1\) for all \(a\in S\). Since \(\I_p\) is a field,
   however, it cannot contain four roots of the quadratic \(x^2-1\). Therefore,
   $S$ is cyclic, say \(S=\la[m]\ra\) where \([m]\) is the congruence class of
   $m$ mod $p$. Since \([m]\) has order 4, we have \([m^4]=[1],[m^2]\neq1\), and
   so \([m^2]=[-1]\) for \([-1]\)  is the unique element in \(S\) of order 2.
   Therefore, \(m^2\equiv -1\mod p\)
   #+END_proof

   #+ATTR_LATEX: :options [Fermat's Two-Squares Theorem]
   #+BEGIN_theorem
   label:thm3.66
   An odd prime $p$ is a sum of two squares,
   \begin{equation*}
   p=a^2+b^2
   \end{equation*}
   where $a$ and $b$ are integers if and only if \(p\equiv 1\mod 4\)
   #+END_theorem

   #+BEGIN_proof
   Assume that \(p=a^2+b^2\). Since $p$ is odd, $a$ and $b$ have different
   parity; say, $a$ is even and $b$ is odd. Hence \(a=2m\) and \(b=2n+1\) and
   \begin{equation*}
   p=a^2+b^2=4m^2+4n^2+4n+1\equiv1\mod 4
   \end{equation*}
   Conversely, assume that \(p\equiv1\mod4\). By the lemma, there is an integer
   $m$ s.t.
   \begin{equation*}
   p\mid(m^2+1)
   \end{equation*}
   In \(\Z[i]\), there is a factorization \(m^2+1=(m+i)(m-i)\) and so 
   \begin{equation*}
   p\mid(m+i)(m-i) \text{ in }\Z[i]
   \end{equation*}
   If \(p\mid(m\pm i)\) in \(\Z[i]\), then there are integers $u$ and $v$ with
   \(m\pm i=p(u+iv)\). Comparing the imaginary parts gives \(pv=1\), a
   contradiction. We conclude that $p$ does not satisfy the analog of Euclid's
   lemma in Theorem ref:thm3.57; it follows from Exercise ref:ex3.62 that  $p$
   is not irreducible. Hence there is a factorization
   \begin{equation*}
   p=\alpha\beta\in\Z[i]
   \end{equation*}
   Therefore, taking norms gives an equation in \(\Z\)
   \begin{align*}
   p^2&=\partial(p)=\partial(\alpha\beta)\\
   &=\partial(\alpha)\partial(\beta)=(a^2+b^2)(c^2+d^2)
   \end{align*}
   By Proposition ref:prop3.64, the only units in \(\Z[i]\) are \(\pm1\) and
   \(\pm i\), so that any nonzero Gaussian integers that is not a unit has a norm
   $>1$; therefore \(a^2+b^2\neq1\) and \(c^2+d^2\neq1\). Euclid's lemma now
   gives \(p\mid a^2+b^2\)  or \(p\mid c^2+d^2\); then fundamental theorem of arithmetic
   gives \(p=a^2+b^2\).
   
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_lemma
   [[label:lemma3.67]]
   If \(\alpha\in\Z[i]\) is irreducible, then there is a unique prime number $p$
   with \(a\mid p\) in \(\Z[i]\)
   #+END_lemma

   #+BEGIN_proof
   Since \(\partial(\alpha)=\alpha\overline{\alpha}\), we have \(\alpha\mid\partial(\alpha)\). Now
   \(\partial(\alpha)=p_1\dots p_n\). If \(\alpha\mid q\) for some prime \(q\neq p_i\), then 
   \(\alpha\mid(q,p_i)=1\), forcing \alpha to be unit. A contradiction
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   Let \(\alpha=a+bi\in\Z[i]\) be neither 0 nor a unit. Then \alpha is irreducible if
   and only if
   1. \alpha is an associate of a prime $p$ in \(\Z\) of the form \(p=4m+3\); or
   2. \alpha is an associate of $1+i$ or its conjugate; or
   3. \(\partial(\alpha)=a^2+b^2\) is a prime in \(\Z\) of the form \(4m+1\)
   #+END_proposition

   #+BEGIN_proof
   By Lemma ref:lemma3.67 there is a unique prime number $p$ divides by \alpha in
   \(\Z[i]\). Since \(\alpha\mid p\), we have \(\partial(\alpha)\mid\partial(p)=p^2\) in
   \(\Z\), so that \(\partial(\alpha)=p\) or \(\partial(\alpha)=p^2\).
   1. \(p\equiv 3\mod4\)

      By Theorem ref:thm3.66 \(p^2=a^2+b^2\). We have \(\alpha\beta=p\) and
      \(\partial(\alpha)\partial(\beta)=\partial(p)\). Therefore, \(p^2\partial(\beta)=p^2\) and
      \(\partial(\beta)=1\). Thus \beta is a unit by Proposition ref:prop3.64 and $p$ is
      irreducible.
   2. \(p\equiv 2\mod 4\)

      \(a^2+b^2=2\)
   3. \(p\equiv1\mod4\)
      If \(\partial(\alpha)=p^2\), \beta is a unit as case 1. Now 
      \(\alpha\overline{\alpha}=p^2=(\alpha\beta)^2\), so that
      \(\overline{\alpha}=\alpha\beta^2\) but \(\beta^2=\pm1\) by Proposition ref:prop3.64

   
   #+END_proof

   #+BEGIN_exercise
   label:ex3.62
   If $R$ is a euclidean ring and \(\pi\in R\) is irreducible, prove that 
   \(\pi\mid\alpha\beta\) implies \(\pi\mid\alpha\) or \(\pi\mid\beta\)
   #+END_exercise

   #+BEGIN_proof
   $R$ is PID and follow Theorem ref:thm3.57.
   #+END_proof
** Linear Algebra
*** Vector Spaces
    [[index:vector space]] [[index:vector]]
    
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    If $k$ is a field, then a *vector space over* $k$ is an (additive) abelian
    group $V$ equipped with a *scalar multiplication*; there is a function 
    \(k\times V\to V\), denoted by \((a,v)\mapsto av\) s.t. for all \(a,b,1\in
    k\) and all 
    \(u,v\in V\)
    1. \(a(u+v)=au+av\)
    2. \((a+b)v=av+bv\)
    3. \((abv)=a(bv)\)
    4. \(1v=v\)


    The elements of \(V\) are called *vectors* and the elements of $k$ are called *scalars*
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_examplle
    1. Euclidean space \(V=\R^n\) is a vector space over $\R$
    2. If $R$ is a commutative ring and $k$ is a subring that is a field, then
       $R$ is a vector space over $k$

       For example, if $k$ is a field, then the polynomial ring \(R=k[x]\) is a
       vector space over $k$.
    #+END_examplle

    [[index:subspace]]
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    If $V$ is a vector space over a field $k$, then a *subspace* of $V$ is a
    subset $U$ of $V$ s.t.
    1. \(0\in U\)
    2. \(u,u'\in U\) imply \(u+u'\in U\)
    3. \(u\in U\) and \(a\in k\) imply \(au\in U\)
    #+END_definition

    
    [[index:k-linear combination]] [[index:span]]
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Let $V$ be a vector space over a field $k$. A *\(k\)-linear combination* of a
    list \(v_1,\dots,v_n\) in $V$ is a vector of $v$ of the form
    \begin{equation*}
    v=a_1v_1+\dots+a_nv_n
    \end{equation*}
    where \(a_i\in k\) for all \(i\)
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    If \(X=v_1,\dots,v_m\) is a list in a vector space $V$, then
    \begin{equation*}
    \la v_1,\dots,v_m\ra
    \end{equation*}
    the set of all the \(k\)-linear combinations of \(v_1,\dots,v_m\) is called
    the *subspace spanned by \(X\)*. We also say that \(v_1,\dots,v_m\) *spans*
    \(\la v_1,\dots,v_m\ra\)
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_lemma
    Let $V$ be a vector space over a field $k$
    1. Every intersection of subspaces of $V$ is itself a subspace
    2. If \(X=v_1,\dots,v_m\) is a list in $V$, then the intersection of all the
       subspaces of $V$ containing $X$ is \(\la v_1,\dots,v_m\ra\), and so 
       \(\la v_1,\dots,v_m\ra\) is the *smallest subspace*
    #+END_lemma

    #+ATTR_LATEX: :options []
    #+BEGIN_examplle
    Let \(V=\R^2\), let \(e_1=(1,0)\) and let \(e_2=(0,1)\). then \(V=\la
       e_1,e_2\ra\) 
    #+END_examplle

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    A vector space $V$ is called *finite-dimensional* if it is spanned by a finite
    list; otherwise $V$ is called *infinite-dimensional*
    #+END_definition

    *Notation*. If \(v_1,\dots,v_m\) is a list, then
    \(v_1,\dots,\what{v_i},\dots,v_m\) is the shorter list with \(v_i\) deleted

    #+ATTR_LATEX: :options []
    #+BEGIN_proposition
    label:prop3.73
    If $V$ is a vector space, then the following conditions on a list
    \(X=v_1,\dots,v_m\) spanning $V$ are equivalent
    1. $X$ is not a shortest spanning list
    2. some \(v_i\) is in the subspace spanned by the others; that is
       \begin{equation*}
       v_i\in\la v_1,\dots,\what{v_i},\dots,v_m\ra
       \end{equation*}
    3. there are scalars \(a_1,\dots,a_m\) not all zero with
       \begin{equation*}
       \displaystyle\sum_{l=1}^ma_lv_l=0
       \end{equation*}
    #+END_proposition
    

    [[index:linearly dependent]]
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    A list \(X=v_1,\dots,v_m\) in a vector space $V$ is *linearly dependent* if
    there are scalars \(a_1,\dots,a_m\) not all zero, with
    \(\sum_{l=1}^ma_lv_l=0\); otherwise $X$ is called *linearly independent*
    #+END_definition


    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    If \(X=v_1,\dots,v_m\) is a list spanning a vector space $V$, then $X$ is a
    shortest spanning list if and only if $X$ is linearly independent
    #+END_corollary


    [[index:basis]]
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    A *basis* of a vector space $V$ is a linearly independent list that spans $V$
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_proposition
    Let \(X=v_1,\dots,v_n\) be a list in a vector space $V$ over a field $k$.
    Then $X$ is a basis if and only if each vector in $V$ has a unique
    expression as a \(k\)-linear combination of vectors in $X$
    #+END_proposition

    #+BEGIN_proof
    If a vector \(v=\sum a_iv_i=\sum b_iv_i\), then \(\sum(a_i-b_i)=0\)
    #+END_proof

    [[index:coordinate set]]
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    If \(X=v_1,\dots,v_n\) is a basis of a vector space $V$ and if \(v\in V\),
    then there are unique scalars \(a_1,\dots,a_n\) with
    \(v=\sum_{i=1}^na_iv_i\). The \(n\)-tuple \((a_1,\dots,a_n)\) is called the
    *coordinate set* of a vector \(v\in V\) relative to the basis $X$
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    Every finite-dimensional vector space $V$ has a basis
    #+END_theorem

    #+BEGIN_proof
    A finite spanning list \(X\) exists, since $V$ is finite-dimensional. If it
    is  linearly independent, it is a basis; if not, $X$ can be shortened to a
    spanning list \(X'\) by Proposition ref:prop3.73
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_lemma
    Let \(u_1,\dots,u_n\) be elements in a vector space $V$, and let
    \(v_1,\dots,v_m\in\la u_1,\dots,u_n\ra\). If \(m>n\), then \(v_1,\dots,v_m\)
    is a linearly dependent list
    #+END_lemma

    #+BEGIN_proof
    Induction on \(n\ge 1\)
    
    /Base step/. If \(n=1\)
    
    /Inductive step/. For \(i=1,\dots,m\)
    \begin{equation*}
    v_i=a_{i1}u_1+\dots+a_{in}u_n
    \end{equation*}
    We may assume that some \(a_{i1}\neq0\) otherwise 
    \(v_1,\dots,v_m\in\la u_2,\dots,u_n\ra\) , and the inductive hypothesis
    applies. Changing notation 
    if necessary we may assume \(a_{11}\neq 0\). For each \(i\ge 2\), define
    \begin{equation*}
    v_i'=v_i-a_{i1}a_{11}^{-1}v_1\in\la u_2,\dots,u_n\ra
    \end{equation*}

    Since \(m-1>n-1\)
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    A homogeneous system of linear equations, over a field $k$, with more
    unknowns than equations has a nontrivial solution.
    #+END_corollary
    
    #+BEGIN_proof
    An \(n\)-tuple \((\beta_1,\dots,\beta_n)\) is a solution of a system
    \begin{gather*}
    \alpha_{11}x_1+\dots+\alpha_{1n}x_n=0\\
    \vdots\quad\vdots\quad\vdots\\
    \alpha_{m1}x_1+\dots+\alpha_{mn}x_n=0
    \end{gather*}
    if \(\alpha_{i1}\beta_1+\dots+\alpha_{in}\beta_n=0\) for all $i$. In other
    words, if \(c_1,\dots,c_n\) are the columns of the \(m\times n\) coefficient
    matrix \(A=[\alpha_{ij}]\), then 
    \begin{equation*}
    \beta_1c_1+\dots+\beta_nc_n=0
    \end{equation*}
    Note that \(c_i\in k^m\). Now \(k^m\) can be spanned by \(m\) vectors. Since
    \(n>m\), \(c_1,\dots,c_n\) is linearly dependent
    #+END_proof

    #+ATTR_LATEX: :options [Invariance of Dimension]
    #+BEGIN_theorem
    If \(X=x_1,\dots,x_n\) and \(Y=y_1,\dots,y_m\) are bases of a vector space
    $V$, then \(m=n\)
    #+END_theorem

    #+BEGIN_proof
    Otherwise \(n<m\) or \(m<n\)
    #+END_proof

    [[index:dimension]]
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    If $V$ is a finite-dimensional vector space over a field $k$, then its
    *dimension* denoted by \(\dim_k(V)\) or \(\dim(V)\), is the number of elements
    in a basis of $V$
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_examplle
    Let \(X=\{x_1,\dots,x_n\}\) be a finite set. Define
    \begin{equation*}
    k^X=\{\text{functions }f:X\to k\}
    \end{equation*}
    Now \(k^X\) is a vector space if we define addition
    \begin{equation*}
    f+f':x\mapsto f(x)+f'(x)
    \end{equation*}
    and scalar multiplication for \(a\in k\)
    \begin{equation*}
    af:x\mapsto af(x)
    \end{equation*}

    It's easy to check that the set of \(n\) functions of the form \(f_x\),
    where \(x\in X\) defined by
    \begin{equation*}
    f_x(y)=
    \begin{cases}
    1&\text{if }y=x\\
    0
    \end{cases}
    \end{equation*}
    form a basis.

    An \(n\)-tuple \((a_1,\dots,a_n)\) is really a function \(f:\{1,\dots,n\}\to
    k\) with \(f(i)=a_i\)
    #+END_examplle

    #+ATTR_LATEX: :options []
    #+BEGIN_lemma
    label:lemma3.73
    If \(X=v_1,\dots,v_n\) is a linearly dependent list of vectors in a vector
    space $V$, then there exists \(v_r\) with \(r\ge1\) with \(v_r\in\la v_1,\dots,v_{r-1}\ra\)
    #+END_lemma

    #+ATTR_LATEX: :options [Exchange Lemma]
    #+BEGIN_lemma
    label:lemma3.74
    If \(X=x_1,\dots,x_m\) is a basis of a vector space $V$ and
    \(y_1,\dots,y_n\) is a linearly independent subset of $V$, then \(n\le m\)
    #+END_lemma

    #+BEGIN_proof
    We begin by showing that one of the \(x\)'s in \(X\) can be replaced by
    \(y_n\) so that the new list still spans \(V\). Now \(y_n\in\la X\ra\), so
    that the list
    \begin{equation*}
    y_n,x_1,\dots,x_m
    \end{equation*}
    is linearly dependent. By Lemma ref:lemma3.73 there is some \(i\) with 
    \(x_i=ay_n+\sum_{j<i}a_jx_j\). Throwing out \(x_i\) and replacing it by
    \(y_n\) gives a spanning list 
    \begin{equation*}
    X'=y_n,x_1,\dots,\what{x_i},\dots,x_m
    \end{equation*}

    Now repeat this argument for the spanning list 
    \(y_{n-1},y_n,x_1,\dots,\what{x_i},\dots,x_m\). It follows that the
    disposable vector must be one of the remaining \(x\)'s, say \(x_l\). After
    throwing out \(x_l\), we have a new spanning list \(X''\). If \(n>m\), then
    this procedure ends with a spanning list consisting of \(m\) \(y\)'s and no
    \(x\)'. Thus a proper sublist of \(Y=y_1,\dots,y_n\) spans $V$, a contradiction
    #+END_proof

    #+ATTR_LATEX: :options [Invariance of Dimension]
    #+BEGIN_theorem
    If \(X=x_1,\dots,x_n\) and \(Y=y_1,\dots,y_m\) are bases of a vector space
    $V$, then \(m=n\)
    #+END_theorem

    #+BEGIN_proof
    By Lemma ref:lemma3.74, \(n\le m\) and \(m\le n\)
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    A *longest* (or a *maximal*) linearly independent list \(u_1,\dots,u_m\) is a
    linearly independent list for which there is no vector \(v\in V\) s.t. 
    \(u_1,\dots,u_m,v\) is linearly independent
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_lemma
    If $V$ is a finite-dimensional vector space, then a longest linearly
    independent list \(v_1,\dots,v_n\) is a basis of $V$
    #+END_lemma

    #+ATTR_LATEX: :options []
    #+BEGIN_proposition
    Let \(Z=u_1,\dots,u_m\) be a linearly independent list in an
    \(n\)-dimensional vector space $V$. Then \(Z\) can be extended to a basis
    #+END_proposition

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    If \(\dim(V)=n\), then any list of \(n+1\) or more vectors is linearly dependent
    #+END_corollary

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    Let $V$ be a vector space with \(\dim(V)=n\)
    1. A list of \(n\) vectors that spans $V$ must be linearly independent
    2. Any linearly independent list of \(n\) vectors must span $V$
    #+END_corollary

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    Let $U$ be a subspace of a vector space $V$ of dimension $n$
    1. $U$ is finite-dimensional and \(\dim(U)\le\dim(V)\)
    2. If \(\dim(U)=\dim(V)\), then \(U=V\)
    #+END_corollary
*** Linear Tranformations
    [[index:linear transformation]]
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    If \(V\) and $W$ are vector spaces over a field $k$, then a function
    \(T:V\to W\) is a *linear transformation* if for all vectors \(u,v\in V\), and
    all scalars \(a\in k\)
    1. \(T(u+v)=T(u)+T(v)\)
    2. \(T(av)=aT(v)\)
    #+END_definition

    [[index:singular]]
    We say that a linear transformation $T$ is *nonsingular* (or is an
    *isomorphism*) if $T$ is a bijection.
    
    #+ATTR_LATEX: :options []
    #+BEGIN_examplle
    1. If \theta is an angle, then the rotation about the origin by \theta is a linear
       transformation \(R_\theta:\R^2\to \R^2\)
    2. If \(V\) and $W$ are vector spaces over a field $k$, write
       \(\Hom_k(V,W)\) for the set of all linear transformations \(V\to W\).
       It's a vector space
    
    #+END_examplle

    [[index:general linear group]]
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    If $V$ is a vector space over a field $k$, then the *general linear group*,
    denoted by \(\gl(V)\), is the set of all nonsingular linear transformations 
    \(V\to V\)
    #+END_definition

    A composite $ST$ of linear transformation $S$ and $T$ is again a linear
    transformation 

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    label:thm3.92
    Let \(v_1,\dots,v_n\) be a basis of a vector space $V$ over a field $k$. If
    $W$ is a vector space over $k$ and \(u_1,\dots,u_n\) is a list in $W$, then
    there exists a unique linear transformation \(T:V\to W\) with \(T(v_i)=u_i\)
    for all $i$
    #+END_theorem

    #+BEGIN_proof
    Each \(v\in V\) has a unique expression of the form \(v=\sum_ia_iv_i\) and
    so \(T:V\to W\) given by \(T(v)=\sum a_iu_i\) is a well-defined function

    To prove the uniqueness of $T$, assume that \(S:V\to W\) is a linear
    transformation with 
    \begin{equation*}
    S(v_i)=u_i=T(v_i)
    \end{equation*}
    Then 
    \begin{align*}
    S(v)&=S(\sum a_iv_i)=\sum S(a_iv_i)\\
    &=\sum a_iS(v_i)=\sum a_iT(v_i)=T(v)
    \end{align*}
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    If two linear transformations \(S,T:V\to W\) agree on a basis, then \(S=T\)
    #+END_corollary

    #+ATTR_LATEX: :options []
    #+BEGIN_proposition
    If \(T:k^n\to k^m\) is a linear transformation, then there exists an
    \(m\times n\) matrix $A$ s.t.
    \begin{equation*}
    T(y)=Ay
    \end{equation*}
    for all \(y\in k^n\) (here \(y\) is an \(n\times 1\) column matrix) 
    #+END_proposition

    #+BEGIN_proof
    If \(e_1,\dots,e_n\) is the standard basis of \(k^n\) and
    \(e_1',\dots,e_m'\) is the standard basis of \(k^m\), define \(A=[a_{ij}]\)
    to be the matrix whose \(j\)th column is the coordinate set of \(T(e_j)\).
    If \(S:k^n\to k^m\) is defined by \(S(y)=Ay\), then \(S=T\) since they agree
    on a basis: \(T(e_j)=\sum_ia_{ij}e_i'=Ae_j\)
    #+END_proof

    [[index:matrix]]
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Let \(X=v_1,\dots,v_n\) be a basis of $V$ and let \(Y=w_1,\dots,w_m\) be a
    basis of $W$. If \(T:V\to W\) is a linear transformation, then the *matrix
    of $T$* is the \(m\times n\) matrix \(A=[a_{ij}]\), whose \(j\)th column
    \(a_{1j},a_{2j},\dots,a_{mj}\) is the coordinate set of \(T(v_j)\)
    determined by \(w\)'s: \(T(v_j)=\sum_{i=1}^ma_{ij}w_j\). The matrix $A$ does
    depend on the choice of bases $X$ and $Y$: we will write
    \begin{equation*}
    A={}_Y[T]_X
    \end{equation*}

    In case \(V=W\), we often let the basis \(X=v_1,\dots,v_n\) and
    \(w_1,\dots,w_m\) coincide. If \(1_V:V\to V\), given by \(v\mapsto v\) is
    the identity linear transformation, then \({}_X[1_V]_X\) is the
    \(n\times n\)  *identity matrix $I_n$*, defined by
    \begin{equation*}
    I=[\delta_{ij}]
    \end{equation*}
    where \(\delta_{ij}\) is the Kronecker delta. A matrix is *nonsingular* if it
    has inverse.
    #+END_definition
    
    #+ATTR_LATEX: :options []
    #+BEGIN_examplle
    Let \(T:V\to W\) be a linear transformation, and let \(X=v_1,\dots,v_n\) and
    \(Y=w_1,\dots,w_n\) be bases of $V$ and $W$ ,respectively. The matrix for
    $T$ is set up from the equation
    \begin{equation*}
    T(v_j)=a_{1j}w_1+\dots+a_{mj}w_m
    \end{equation*}
    #+END_examplle

    #+ATTR_LATEX: :options []
    #+BEGIN_examplle
    1. Let \(T:\R^2\to \R^2\) be rotation by \(\ang{90}\). The matrix of $T$
       related to the standard basis \(X=(1,0),(0,1)\) is 
       #+ATTR_LATEX: :mode math :environment bmatrix :math-prefix {}_X[T]_X=
       | 0 | -1 |
       | 1 | 0  |

       However if \(Y=(0,1)(1,0)\), then
       #+ATTR_LATEX: :mode math :environment bmatrix :math-prefix {}_Y[T]_Y=
       |  0 | 1 |
       | -1 | 0 |
    2. Let $k$ be a field, let \(T:V\to V\) be a linear transformation on a
       two-dimensional vector space, and assume that there is some vector
       \(v\in V\) with \(T(v)\) not a scalar multiple of \(v\). The assumption on
       $v$ says that the list \(X=v,T(v)\) is linearly independent, and hence
       it's a basis of $V$. Write \(v_1=v,v_2=Tv\).

       We compute \({}_X[T]_X\)
       \begin{equation*}
       T(v_1)=v_2\quad\text{ and }\quad T(v_2)=av_1+bv_2
       \end{equation*}
       for some \(a,b\in k\). We conclude that
       #+ATTR_LATEX: :mode math :environment bmatrix :math-prefix {}_X[T]_X=
       | 0 | a |
       | 1 | b |
    #+END_examplle

    #+ATTR_LATEX: :options []
    #+BEGIN_proposition
    label:prop3.97
    Let \(V\) and \(W\) be a vector spaces over a field $k$, and let
    \(X=v_1,\dots,v_n\) and \(Y=w_1,\dots,w_m\) be bases of \(V\) and \(W\),
    respectively. If \(\Hom_k(V,W)\) denotes the set of all linear
    transformations \(T:V\to W\) and \(\Mat_{m\times n}k\) denotes the set of
    all \(m\times n\) matrices with entries in $k$, then the function 
    \(T\mapsto{}_Y[T]_X\) is a bijection \(\Hom_k(V,W)\to\Mat_{m\times n}(k)\) 
    #+END_proposition

    #+BEGIN_proof
    Given a matrix \(A\), its columns define vectors in \(W\); in more detail,
    if the \(j\)th column of $A$ is \(a_{1j},\dots,a_{mj}\), define
    \(z_j=\sum_{i=1}^ma_{ij}w_i\). By Theorem ref:thm3.92, there exists a linear
    transformation \(T:V\to W\) with \(T(v_j)=z_j\) and \({}_Y[T]_X=A\).
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_proposition
    label:prop3.98
    Let \(T:V\to W\) and \(S:W\to U\) be linear transformations. Choose bases 
    \(X=x_1,\dots,x_n\) of \(V\), \(Y=y_1,\dots,y_m\) of \(W\), and
    \(Z=z_1,\dots,z_l\) of $U$, then
    \begin{equation*}
    {}_Z[S\circ T]_X=({}_Z[S]_Y)({}_Y[T]_X)
    \end{equation*}
    #+END_proposition

    #+BEGIN_proof
    Let \({}_Y[T]_X=[a_{ij}]\), so that \(T(x_j)=\sum_pa_{pj}y_p\), and let
    \({}_Z[S]_Y=[b_{qp}]\), so that \(S(y_p)=\sum_qb_{qp}z_q\). Then
    \begin{align*}
    ST(x_j)=S(T(x_j))&=S(\displaystyle\sum_{p}a_{pj}y_p)\\
    &=\displaystyle\sum_{p}a_{pj}S(y_p)=\displaystyle\sum_{p}
    \displaystyle\sum_qa_{pj}b_{qp}z_q=\displaystyle\sum_{q}c_{qj}z_q
    \end{align*}
    where \(c_{qj}=\sum_{p}b_{qp}a_{pj}\). Therefore
    \begin{equation*}
    {}_Z[ST]_X=[c_{qj}]={}_Z[S]_Y{}_Y[T]_X
    \end{equation*}
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    Matrix multiplication is associative
    #+END_corollary

    #+BEGIN_proof
    Let $A$ be an \(m\times n\) matrix, let \(B\) be an \(n\times p\) matrix,
    and let \(C\) be a \(p\times q\) matrix. By Theorem ref:thm3.92, there are
    linear transformations
    \begin{equation*}
    k^q\xrightarrow{T}k^p\xrightarrow{S}k^n\xrightarrow{R}k^m
    \end{equation*}
    with \(C=[T],B=[S],A=[R]\)
    
    Then 
    \begin{equation*}
    [R\circ(S\circ T)]=[R][S\circ T]=[R]([S][T])=A(BC)
    \end{equation*}
    On the other hand
    \begin{equation*}
    [(R\circ S)\circ T]=[R\circ S][T]=([R][S])[T]=(AB)C
    \end{equation*}
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    label:cor3.100
    Let \(T:V\to W\) be a linear transformation of vector space \(V\) over a
    field $k$, and let \(X\) and \(Y\) be bases of \(V\) and \(W\),
    respectively. If \(T\) is nonsingular, then the matrix of \(T^{-1}\) is the
    inverse of the matrix of $T$
    \begin{equation*}
    {}_X[T^{-1}]_Y=({}_Y[T]_X)^{-1}
    \end{equation*}
    #+END_corollary
    
    #+BEGIN_proof
    \(I={}_Y[1_W]_Y={}_Y[T]_{XX}[T^{-1}]_Y\) and \(I={}_X[1_V]_X={}_X[T^{-1}]_{YY}[T]_X\)
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    label:cor3.101
    Let \(T:V\to V\) be a linear transformation on a vector space $V$ over a
    field $k$. If \(X\) and \(Y\) are bases of $V$, then there is a nonsingular
    matrix $P$ with entries in $k$ so that
    \begin{equation*}
    {}_Y[T]_Y=P(_X[T]_X)P^{-1}
    \end{equation*}
    Conversely, if \(B=PAP^{-1}\), where \(B,A,P\) are \(n\times n\) matrices
    with entries in $k$ and $P$ is nonsingular, then there is a linear
    transformation \(T:k^n\to k^n\) and bases $X$ and $Y$ of \(k^n\) s.t.
    \(B=_Y[T]_Y,A=_X[T]_X\)
    #+END_corollary

    #+BEGIN_proof
    The first statement follows from Proposition ref:prop3.98 and associativity
    \begin{equation*}
    {}_Y[T]_Y=_Y[1_VT1_V]_Y=(_Y[1_V]_X)(_X[T]_X)(_X[1_V]_Y)
    \end{equation*}
    Set \(P=_Y[1_V]_X\)

    For the converse, let \(E=e_1,\dots,e_n\) be the standard basis of \(k^n\),
    and define \(T:k^n\to k^n\) be \(T(e_j)=Ae_j\). If follows that
    \(A=_E[T]_E\). Now define a basis \(Y=y_1,\dots,y_n\) by \(y_j=P^{-1}e_j\).
    $Y$ is a basis because $P^{-1}$ is nonsingular. It suffices to prove that 
    \(B=_Y[T]_Y\); that is \(T(y_j)=\sum_ib_{ij}y_i\), where \(B=[b_{ij}]\)
    \begin{align*}
    T(y_j)&=Ay_y=AP^{-1}e_j=P^{-1}Be_j\\
    &=P^{-1}\displaystyle\sum_{i}b_{ij}e_i
    =\displaystyle\sum_{i}b_{ij}P^{-1}e_i\\
    &=\displaystyle\sum_{i}b_{ij}y_i
    \end{align*}
    #+END_proof

    [[index:similar]]
    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    Two \(n\times n\) matrices $B$  and \(A\) with entries in field $k$ are
    *similar* if there is a nonsingular matrix \(P\) with entries in $k$ with
    \(B=PAP^{-1}\) 
    #+END_definition



    Corollary ref:cor3.101 says the two matrices arise from the same linear
    transformation on a vector space $V$ if and only if they are similar

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    If \(T:V\to W\) is a linear transformation, then the *kernel* (or the *null
    space*) of $T$ is
    \begin{equation*}
    \ker T=\{v\in V:T(v)=0\}
    \end{equation*}
    and the *image* of $T$ is
    \begin{equation*}
    \im T=\{w\in W:w=T(v)\text{ for some }v\in V\}
    \end{equation*}
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_proposition
    Let \(T:V\to W\) be a linear transformation
    1. \(\ker T\) is a subspace of $V$ and \(\im T\) is a subspace of $W$
    2. $T$ is injective if and only if \(\ker T=\{0\}\)
    #+END_proposition

    #+ATTR_LATEX: :options []
    #+BEGIN_lemma
    label:lemma3.103
    Let \(T:V\to W\) be a linear transformation
    1. If $T$ is nonsingular, then for every basis \(X=v_1,\dots,v_n\) of $V$,
       we have \(T(X)=T(v_1),\dots,T(v_n)\) a basis of $W$
    2. Conversely, if there exists some basis \(X=v_1,\dots,v_n\) of $V$ for
       which \(T(X)\) is a basis of $W$, then $T$ is nonsingular
    #+END_lemma
    #+BEGIN_proof
    1. If \(\sum c_iT(v_i)=0\), then \(T(\sum c_iv_i)=0\) and so 
       \(\sum c_iv_i\in\ker T=\{0\}\). Hence each \(c_i=0\) because $X$ is linearly
       independent. If \(w\in W\), then the surjectivity of $T$ provides \(v\in V\)
       with \(w=T(v)\). But \(v=\sum a_iv_i\), and so 
       \(w=T(v)=T(\sum a_iv_i)=\sum a_iT(v_i)\). Therefore $T(X)$ is a basis of
       $W$
    2. Let \(w\in W\). Since \(T(X)\) is a basis of $W$, we have 
       \(w=\sum c_iT(v_i)=T(\sum c_iv_i)\). Add so $T$ is surjective. If 
       \(\sum c_iv_i\in\ker T\), then \(\sum c_iT(v_i)=0\) and so linear
       independence gives all \(c_i=0\); hence \(\ker T=\{0\}\). Therefore $T$
       is nonsingular
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_theorem
    If $V$ is an \(n\)-dimensional vector space over a field $k$, then \(V\) is
    isomorphic to \(k^n\)
    #+END_theorem

    #+BEGIN_proof
    Choose a basis \(v_1,\dots,v_n\) of $V$. If \(e_1,\dots,e_n\) is the
    standard basis of \(k^n\), then Theorem ref:thm3.92 says that there is a
    linear transformation \(T:V\to k^n\) with \(T(v_i)=e_i\); by Lemma
    ref:lemma3.103 $T$ is nonsingular
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    Two finite-dimensional vector space $V$ and $W$ over a field $k$ are
    isomorphic if and only if \(\dim(V)=\dim(W)\)
    #+END_corollary

    #+ATTR_LATEX: :options []
    #+BEGIN_proposition
    label:prop3.106
    Let $V$ be a finite-dimensional vector space with \(\dim(V)=n\), and let 
    \(T:V\to V\) be a linear transformation. The following statements are
    equivalent
    1. $T$ is an isomorphism
    2. $T$ is surjective
    3. $T$ is injective
    #+END_proposition

    #+BEGIN_proof
    \(2\to 3\). Let \(v_1,\dots,v_n\) be the basis of $V$. Since $T$ is
    surjective, there are vectors \(u_1,\dots,u_n\) with \(Tu_i=v_i\). We claim
    that \(u_1,\dots,u_n\) are linearly independent. To show that $T$ is
    injective, it suffices to show that \(\ker T=\{0\}\)

    \(3\to 1\). Let \(v_1,\dots,v_n\) be a basis of $V$. If \(c_1,\dots,c_n\)
    are scalars not all 0, then \(\sum c_iv_i\neq0\). Since $T$ is injective, it
    follows that \(\sum c_iT(v_i)\neq0\) and so \(Tv_1,\dots,Tv_n\) are linearly
    independent. Therefore Lemma ref:lemma3.103 shows that $T$ is an isomorphism

    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    If \(A\) and $B$ are \(n\times n\) matrices with \(AB=I\), then \(BA=I\).
    Therefore $A$ is nonsingular with inverse $B$
    #+END_corollary
    #+BEGIN_proof
    There are linear transformations \(T,S:k^n\to k^n\) with \([T]=A,[S]=B\),
    and \(AB=I\) gives
    \begin{equation*}
    [TS]=[T][S]=[1_{k^n}]
    \end{equation*}
    Since \(T\mapsto[T]\) is a bijection, by Proposition ref:prop3.97, it
    follows that \(TS=1_{k^n}\). Hence $T$ is a surjection and \(S\) is an
    injection by Proposition ref:prop1.. But Proposition ref:prop3.106 says taht $T,S$ are both
    isomorphism, so that \(S=T^{-1}\) and \(TS=1_{k^n}=ST\)
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    The set of all nonsingular \(n\times n\) matrices with entries in $k$ is
    denoted by \(\gl(n,k)\)
    #+END_definition

    It's easy to prove that \(\gl(n,k)\) is a group

    #+ATTR_LATEX: :options []
    #+BEGIN_proposition
    Let $V$ be an \(n\)-dimensional vector space over a field $k$, and let
    \(X=v_1,\dots,v_n\) be a basis of $V$. Then \(\mu:\gl(V)\to\gl(n,k)\) defined
    by \(T\mapsto[T]={}_X[T]_X\) is an isomorphism
    #+END_proposition

    #+BEGIN_proof
    By Proposition ref:prop3.97 the function \(\mu':T\mapsto[T]\) is a bijection
    \begin{equation*}
    \Hom_k(V,V)\to\Mat_n(k)
    \end{equation*}

    If \(T\in\gl(V)\), then \([T]\) is a nonsingular matrix by Corollary
    ref:cor3.100; that is, if \mu is the restriction of \(\mu'\), then 
    \(\mu:\gl(V)\to\gl(n,k)\) is an injective homomorphism.

    If \(A\in\gl(n,k)\), then \(A=[T]\) for some \(T:V\to V\). It suffices to
    show that \(T\) is an isomorphism; that is, \(T\in\gl(V)\). Since \([T]\) is
    a nonsingular matrix, there is a matrix $B$ with \([T]B=I\). Now \(B=[S]\)
    for some \(S:V\to V\) and 
    \begin{equation*}
    [TS]=[T][S]=I=[1_V]
    \end{equation*}
    #+END_proof

    #+ATTR_LATEX: :options []
    #+BEGIN_definition
    A linear transformation $T:V\to V$ is a *scalar transformation* if there is 
    \(c\in k\) with \(T(v)=cv\) for all \(v\in V\); that is \(T=c1_V\). A
    *scalar matrix* is a matrix of the form \(cI\)
    #+END_definition

    #+ATTR_LATEX: :options []
    #+BEGIN_corollary
    1. The center of the group \(\gl(V)\) consists of all the nonsingular scalar
       transformations
    2. The center of the group \(\gl(n,k)\) consists of all the nonsingular
       scalar matrices
    #+END_corollary
** Quotient Rings and Finite Fields
   #+ATTR_LATEX: :options []
   #+BEGIN_theorem
   label:thm3.110
   If $I$ is an ideal in a commutative ring $R$, then the additive abelian group
   \(R/I\) can be made into a commutative ring in such a way that the natural
   map \(\pi:R\to R/I\) is a surjective ring homomorphism
   #+END_theorem

   #+BEGIN_proof
   Define multiplication on the additive abelian group \(R/I\) by
   \begin{equation*}
   (a+I)(b+I)=ab+I
   \end{equation*}
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   The commutative ring \(R/I\) constructed in Theorem ref:thm3.110 is called
   the *quotient ring* of $R$ modulo $I$
   #+END_definition


   #+ATTR_LATEX: :options []
   #+BEGIN_corollary
   If $I$ is an ideal in a commutative ring $R$, then there are a commutative
   ring $A$ and a ring homomorphism \(\pi:R\to A\) with \(I=\ker\pi\)
   #+END_corollary

   #+BEGIN_proof
   Natural map \(\pi:R\to R/I\)
   #+END_proof

   #+ATTR_LATEX: :options [First Isomorphism Theorem]
   #+BEGIN_theorem
   If \(f:R\to A\) is a homomorphism of rings, then \(\ker f\) is an ideal in
   $R$, \(\im f\) is a subring of $A$, and
   \begin{equation*}
   R/\ker f\cong\im f
   \end{equation*}
   #+END_theorem

   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   If $k$ is a field, the intersection of all the subfields of $k$ is called the
   *prime field* of $k$
   #+END_definition

   Every subfield of \(\C\) contains $\Q$ and so the prime field of \(\C\) and
   of $\R$ is $\Q$.

   *Notation*. From now on, we will denote \(\I_p\) by \(\F_p\) when we are
   regarding it as a field.

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $k$ is a field, then its prime field is isomorphic to $\Q$ or to $\F_p$
   for some prime $p$
   #+END_proposition

   #+BEGIN_proof
   Consider the ring homomorphism \(\chi:\Z\to k\) defined by \(\chi(n)=n\epsilon\),
   where we denote the *one* in $k$ by \epsilon. Since every ideal in \(\Z\) is
   principal, there is an integer $m$ with \(\ker\chi=(m)\). If \(m=0\), then
   \chi is an injection, and so there is an isomorphism copy of \(\Z\) that is a
   subring of $k$. By Exercise ref:ex3.47 , there is a field
   \(Q\cong\Frac(\Z)=\Q\). with \(\im\chi\subseteq Q\subseteq k\). Now \(Q\)
   is the prime ideal of $k$, for every subfield of $k$ contains 1. hence
   contains \(\im\chi\), and hence it contains \(Q\), for \(Q\cong\Q\) has no
   proper subfields. If \(m\neq0\), the first isomorphism theorem gives
   \(\I_m=\Z/(m)\cong\im\chi\subseteq k\). Since $k$ is a field, \(\im\chi\) is
   a domain, and so Proposition ref:prop3.6 gives $m$ prime. If we now write $p$
   instead of $m$, then \(\im\chi=\{0,\epsilon,2\epsilon,\dots,(p-1)\epsilon\}\) is a
   subfield of $k$ isomorphic to \(\F_p\)
   #+END_proof

   [[index:characteristic]]
   #+ATTR_LATEX: :options []
   #+BEGIN_definition
   A field $k$ has *characteristic 0* if its prime field is isomorphic to \(\Q\);
   a field $k$ has *characteristic $p$* if its prime field is isomorphic to
   \(\F_p\) for some prime $p$
   #+END_definition

   The fields \(\Q,\R,\C\) have characteristic 0

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $k$ is a field of characteristic \(p>0\), then \(pa=0\) for all \(a\in k\)
   #+END_proposition

   #+BEGIN_proof
   \(p\cdot 1=0\)
   #+END_proof
   
   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $k$ is a field of characteristic $p>0$ then \(pa=0\) for all \(a\in k\)
   #+END_proposition

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If \(k\) is a finite field, then \(\abs{k}=p^n\) for some prime $p$ and some \(n\ge1\)
   #+END_proposition

   #+BEGIN_proof
   The prime field \(P\) of $k$ cannot be the infinite field $\Q$, and so
   \(P\cong\F_p\) for some $p$. Now $k$ is a  vector space over $P$, and so it is
   a vector space over \(\F_p\). Clearly, $k$ is finite-dimensional, and if
   \(\dim_{\F_p}(k)=n\), then \(\abs{k}=p^n\)
   #+END_proof

   #+ATTR_LATEX: :options []
   #+BEGIN_proposition
   If $k$ is a field and \(I=(p(x))\), where \(p(x)\) is a nonzero polynomial in
   $k[x]$, then the following are equivalent: \(p(x)\) is irreducible;
   \(k[x]/I\) is a field; \(k[x]/I\) is a domain
   #+END_proposition

   #+BEGIN_proof
   Assume \(p(x)\) is irreducible. Note that \(I=(p(x))\) is a proper ideal so
   that the /one/ in \(k[x]/I\), namely, \(1+I\) is not zero.
   #+END_proof

   





   
* Index  
  printindex:nil

